%%big poster size with big fonts
\documentclass[extrafontsizes, 30pt]{memoir}
\usepackage[paperheight=31in,paperwidth=47in,margin=1in,heightrounded,showframe]{geometry}

%%background from pptx
\usepackage{background}
\backgroundsetup{
scale=1,
angle=0,
opacity=1,  %% adjust
contents={\includegraphics[width=\paperwidth,height=\paperheight]{images/coda2016_poster_background.pdf}}
}

%%put words where I want
\usepackage[absolute, overlay]{textpos}
\setlength{\TPHorizModule}{1in}
\setlength{\TPVertModule}{\TPHorizModule}

%%tikz setup
\usepackage{tikz, subfig, amsthm}
\usepackage{tikz-3dplot}
\usetikzlibrary{arrows, shapes, positioning, backgrounds, decorations.pathreplacing}


%%other options
\definecolor{isublue}{RGB}{58,117,196}
\definecolor{isured}{RGB}{206,17,38}
\definecolor{isugreen}{RGB}{7,109,84}
\counterwithout{figure}{chapter}
\counterwithout{table}{chapter}
\usepackage{amsmath}
\setlength{\parindent}{0cm}
\def\vdotswide{\vbox{\baselineskip=20pt \lineskiplimit=0pt \kern6pt \hbox{.}\hbox{.}\hbox{.}}} 

%%big sums
\usepackage{graphicx}
\usepackage{calc}
\newlength{\depthofsumsign}
\setlength{\depthofsumsign}{\depthof{$\sum$}}
\newcommand{\nsum}[1][1.4]{% only for \displaystyle
    \mathop{%
        \raisebox
            {-#1\depthofsumsign+1\depthofsumsign}
            {\scalebox
                {#1}
                {$\displaystyle\sum$}%
            }
    }
}

\newcommand{\Nsum}[1][1.8]{% only for \displaystyle
    \mathop{%
        \raisebox
            {-#1\depthofsumsign+1\depthofsumsign}
            {\scalebox
                {#1}
                {$\displaystyle\sum$}%
            }
    }
}

%%bibliography
\usepackage[backend=bibtex]{biblatex}
\addbibresource{refs.bib}
\renewcommand*{\bibfont}{\tiny}

\begin{document}
%% For every picture that defines or uses external nodes, you'll have to
%% apply the 'remember picture' style. To avoid some typing, we'll apply
%% the style to all pictures.
%%http://www.texample.net/tikz/examples/connecting-text-and-graphics/
\tikzstyle{every picture}+=[remember picture]
\tikzstyle{na} = [baseline=-.5ex]


%%-----------------------------------------------
%%
%%Middle big box
%%
\begin{textblock}{16}(15.5, 6.71)
{\large \bfseries Restricted Boltzmann machine (RBM)}

\begin{figure}[ht]
  \centering
  \resizebox{\linewidth}{!}{\input{images/rbm.tikz}}
  \caption{An example restricted Boltzmann machine (RBM), which consists of two layers, a hidden ($\mathcal{H}$) and a visible layer ($\mathcal{V}$), with no connections within a layer. Hidden nodes indicated by white circles and the visible nodes indicated by blue circles \cite{friedman2001elements}.}
  \label{fig:rbm}
\end{figure}

\vspace{1cm}
\begin{figure}[ht]
  \centering
  \input{images/visibles.tikz}
  \caption{Image classification using a RBM. On the left, each image pixel comprises a node in the visible layer, $\mathcal{V}$. On the right, the output of the RBM is used to create features which are then passed to a supervised learning algorithm.} 
  \label{fig:visibles}
\end{figure}

~\\[-1cm]
{\bfseries Joint distribution} \\[.25cm]
Let $\boldsymbol x = \{h_1, \dots, h_H, v_1,\dots,v_V\}$ represent the states of the visible and hidden nodes in an RBM. Then the probability each node taking the the value corresponding to $\boldsymbol x$ is:
\begin{align}
\label{eqn:pmf}
f_{\boldsymbol \theta} (\boldsymbol x) = \frac{\exp\left(\nsum\limits_{i = 1}^V \nsum\limits_{j=1}^H \theta_{ij} v_i h_j + \nsum\limits_{i = 1}^V\theta_{v_i} v_i + \nsum\limits_{j = 1}^H\theta_{h_j} h_j\right)}{\Nsum\limits_{\boldsymbol x \in \mathcal{X}}\exp\left(\nsum\limits_{i = 1}^V \nsum\limits_{j=1}^H \theta_{ij} v_i h_j + \nsum\limits_{i = 1}^V\theta_{v_i} v_i + \nsum\limits_{j = 1}^H\theta_{h_j} h_j\right)} 
\end{align}

\end{textblock}

%%-----------------------------------------------
%%
%%Top left box
%%
\begin{textblock}{14}(.65, 5.71)
{\large \bfseries Deep learning} \\[.25cm]
\begin{minipage}{.62\textwidth}
Three layer deep Boltzmann machine, with visible-to-hidden and hidden-to-hidden connections but no within-layer connections. This can be considered as multiple single layer restricted Boltzmann machines with the lower stack hidden layer acting as the visible layer for the higher stacked model. Claimed ability to learn "internal representations that become increasingly complex" \cite{salakhutdinov2009deep}, used in classification problems.
\end{minipage}
\begin{minipage}{.38\textwidth}
\begin{figure}
  \centering
  \input{images/deep_rbm.tikz}
  \caption{Deep RBM example.}
  \label{fig:deep_rbm}
\end{figure}
\end{minipage}


\end{textblock}

%%-----------------------------------------------
%%
%%Middle left box
%%
\begin{textblock}{14}(.65, 11.33)
{\large \bfseries Degeneracy, instability, and uninterpretability... Oh my!}\\[-1cm]

The highly flexible nature of the RBM ($H + V + HV$ parameters) makes the following characteristics of model impropriety of particular concern. \\[-.5cm]

\begin{table}
\centering
\begin{tabular}{| p{.48\linewidth} | p{.5\linewidth} |}
\hline
Characteristic & Detection \\
\hline
\hline
{\bfseries Near-degeneracy.} Occurs when there is a disproportionate amount of probability placed on only a few elements of the sample space by the model \cite{handcock2003assessing}. & If the mean parametrization on the model parameters, $\mu(\boldsymbol \theta)$, is close to the boundary of the convex hull of the set of statistics in the neg-potential function $Q(\boldsymbol x)$. \\
\hline
{\bfseries Instability.} Small changes in natural parameters result in large changes of the pmf, excessive sensitivity \cite{schweinberger2011instability}. & If for any $C > 0$ there exists $N_C > 0$ such that 
$\max\limits_{\boldsymbol x_N \in \mathcal{X}_N}[Q(\boldsymbol x_N)] > CN$ for all $N > N_C$. Where $Q(\cdot)$ is the neg-potential function of the model. \\
\hline
{\bfseries Uninterpretability.} Due to the existence of dependence, marginal mean-structure no longer maintained \cite{kaiser2007statistical}. & If the magnitude of the difference between model expectations and expectations under independence, $\left|\text{E}(\boldsymbol X | \boldsymbol \theta) - \text{E}(\boldsymbol X | \emptyset ) \right|$, is large.\\
\hline
\end{tabular}
\label{tab:degen}
\caption{Table of ``improper model'' characteristics.}
\end{table}
\end{textblock}

%%-----------------------------------------------
%%
%%Bottom left box
%%
\begin{textblock}{14}(.65, 21.5)
{\large \bfseries Avoiding degeneracy} \\[-.75cm]

For the $\{-1, 1 \}$ encoding of $\mathcal{V}$ and $\mathcal{H}$, the origin is the center of the parameter space. In particular, at $\boldsymbol \theta = \boldsymbol 0$, the RBM is equivalent to elements of $\boldsymbol X$ being distributed as iid Bernoulli$\left(\frac{1}{2}\right)$ r.v.s. $\Rightarrow$ No \emph{near-degeneracy}, \emph{instability}, or \emph{uninterpretability}!
\end{textblock}
\begin{textblock}{14.5}(.65, 23.9)
\begin{minipage}{.42\textwidth}
\begin{figure}[ht]
  \begin{minipage}{0.49\textwidth}
  \resizebox{\linewidth}{!}{
    \tdplotsetmaincoords{60}{-60}
    \input{images/toyhull_top.tikz}
    
    \draw(0,1,0) -- (1,0,0) -- (0,0,0) -- (0,1,0); 
     \draw(1,1,1) -- (1,0,0) -- (0,0,0) -- (1,1,1); 
     \draw(1,1,1) -- (0,1,0) -- (0,0,0) -- (1,1,1); 
     \draw(1,1,1) -- (0,1,0) -- (1,0,0) -- (1,1,1); 
    \end{tikzpicture}
  }
  \end{minipage}
  \begin{minipage}{0.49\textwidth}
  \resizebox{\linewidth}{!}{
    \tdplotsetmaincoords{60}{-60}
    \input{images/toyhull_top_2.tikz}
    
    \draw(-1,1,-1) -- (1,-1,-1) -- (-1,-1,-1) -- (-1,1,-1); 
     \draw(1,1,1) -- (1,-1,-1) -- (-1,-1,-1) -- (1,1,1); 
     \draw(1,1,1) -- (-1,1,-1) -- (-1,-1,-1) -- (1,1,1); 
     \draw(1,1,1) -- (-1,1,-1) -- (1,-1,-1) -- (1,1,1); 
    \end{tikzpicture}
  }
  \end{minipage}
  \caption{The convex hulls of the statistic space in three dimensions for a toy RBM with $|\mathcal{V}| = |\mathcal{H}| = 1$ for $\{0,1\}$-encoding (left) and $\{-1,1\}$-encoding (right) enclosed by an unrestricted hull of 3-space.}
\label{fig:toyhull}
\end{figure}
\end{minipage}
\begin{minipage}{.02\textwidth}
\hfill
\end{minipage}
\begin{minipage}{.55\textwidth}
\begin{figure}
\centering
\includegraphics[scale=1]{images/frac_volume.pdf}
\label{fig:frac_volume}
\caption{Volume relationship for the convex hulls of statistics in $Q(\cdot)$ vs. unrestricted space.}
\end{figure}
\end{minipage}

\end{textblock}

%%-----------------------------------------------
%%
%%Top right box
%%
\begin{textblock}{14}(32.4, 5.71)
{\large \bfseries Manageable (a.k.a. small) examples}
\begin{figure}
\centering
\includegraphics{images/degeneracy.pdf}
\includegraphics{images/instability.pdf}
\includegraphics{images/uninterpretability.pdf}
\label{fig:three_ways}
\caption{As the magnitude of $\boldsymbol \theta$ grows, so does the occurence of near-degeneracy, instability, and uninterpretability for RBMs of varying sizes.}
\end{figure}
\end{textblock}

%%-----------------------------------------------
%%
%%Middle right box
%%
\begin{textblock}{14}(32.4, 14)
{\large \bfseries Model fitting} \\[.25cm]
Fitting is really hard to get right (Bayesian or ML).
If you are able to fit this model properly, the result is the empirical distribution of images.
You spent a lot of time just to get the empirical distribution back.
If you do not have an instance of every possible image in your training set, certain parameter must $\rightarrow \infty$, making fitting more complicated (shrink)
In reality it's practically impossible to have at least one of every possible image in your training set (large images, color scales, etc.)

\end{textblock}

%%-----------------------------------------------
%%
%%Bottom right box
%%
\begin{textblock}{14}(32.4, 23.85)
{\large \bfseries Discussion} \\[.25cm]
\end{textblock}

\begin{textblock}{14.54}(31.86, 24.8)
These models very easily are degenerate, unstable, and uninterpretable. Meaning, the space of possible fitted parameter values that leads to a proper model is highly restricted.

To further complicate things, the proper fitting these models is very intricate and easily leads to parameter values running off to $\infty$.

If you're thinking of using RBMs, or stacked RBMs in a deep architecture, don't. Instead, fit a less flexible and more sensible model.
\end{textblock}

%%-----------------------------------------------
%%
%%Bottom middle box
%%
\begin{textblock}{14.5}(16.17, 25.45)
{\large \bfseries References}
\printbibliography[heading=none]

\end{textblock}

\end{document}