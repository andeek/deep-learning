%%big poster size with big fonts
\documentclass[extrafontsizes, 30pt]{memoir}
\usepackage[paperheight=31in,paperwidth=47in,margin=1in,heightrounded,showframe]{geometry}

%%background from pptx
\usepackage{background}
\backgroundsetup{
scale=1,
angle=0,
opacity=1,  %% adjust
contents={\includegraphics[width=\paperwidth,height=\paperheight]{images/coda2016_poster_background.pdf}}
}

%%put words where I want
\usepackage[absolute, overlay]{textpos}
\setlength{\TPHorizModule}{1in}
\setlength{\TPVertModule}{\TPHorizModule}

%%tikz setup
\usepackage{tikz, subfig, amsthm}
\usepackage{tikz-3dplot}
\usetikzlibrary{arrows, shapes, positioning, backgrounds, decorations.pathreplacing}


%%other options
\definecolor{isublue}{RGB}{58,117,196}
\definecolor{isured}{RGB}{206,17,38}
\definecolor{isugreen}{RGB}{7,109,84}
\counterwithout{figure}{chapter}
\counterwithout{table}{chapter}
\usepackage{amsmath}
\setlength{\parindent}{0cm}
\def\vdotswide{\vbox{\baselineskip=20pt \lineskiplimit=0pt \kern6pt \hbox{.}\hbox{.}\hbox{.}}} 

%%big sums
\usepackage{graphicx}
\usepackage{calc}
\newlength{\depthofsumsign}
\setlength{\depthofsumsign}{\depthof{$\sum$}}
\newcommand{\nsum}[1][1.4]{% only for \displaystyle
    \mathop{%
        \raisebox
            {-#1\depthofsumsign+1\depthofsumsign}
            {\scalebox
                {#1}
                {$\displaystyle\sum$}%
            }
    }
}

\newcommand{\Nsum}[1][1.8]{% only for \displaystyle
    \mathop{%
        \raisebox
            {-#1\depthofsumsign+1\depthofsumsign}
            {\scalebox
                {#1}
                {$\displaystyle\sum$}%
            }
    }
}

%%bibliography
\usepackage[backend=bibtex]{biblatex}
\addbibresource{refs.bib}
\renewcommand*{\bibfont}{\tiny}

\begin{document}
%% For every picture that defines or uses external nodes, you'll have to
%% apply the 'remember picture' style. To avoid some typing, we'll apply
%% the style to all pictures.
%%http://www.texample.net/tikz/examples/connecting-text-and-graphics/
\tikzstyle{every picture}+=[remember picture]
\tikzstyle{na} = [baseline=-.5ex]


%%-----------------------------------------------
%%
%%Middle big box
%%
\begin{textblock}{16}(15.5, 6.71)
{\large \bfseries Restricted Boltzmann machine (RBM)}

\begin{figure}[ht]
  \centering
  \resizebox{\linewidth}{!}{\input{images/rbm.tikz}}
  \caption{An example restricted Boltzmann machine (RBM), which consists of two layers, a hidden ($\mathcal{H}$) and a visible layer ($\mathcal{V}$), with no connections within a layer. Hidden nodes indicated by white circles and the visible nodes indicated by blue circles \cite{friedman2001elements}.}
  \label{fig:rbm}
\end{figure}

\vspace{1cm}
\begin{figure}[ht]
  \centering
  \input{images/visibles.tikz}
  \caption{Image classification using a RBM. On the left, each image pixel comprises a node in the visible layer, $\mathcal{V}$. On the right, the output of the RBM is used to create features which are then passed to a supervised learning algorithm.} 
  \label{fig:visibles}
\end{figure}

~\\[-1cm]
{\bfseries Joint distribution} \\[.25cm]
Let $\boldsymbol x = \{h_1, \dots, h_H, v_1,\dots,v_V\}$ represent the states of the visible and hidden nodes in an RBM. Then the probability each node taking the the value corresponding to $\boldsymbol x$ is:
\begin{align}
\label{eqn:pmf}
f_{\boldsymbol \theta} (\boldsymbol x) = \frac{\exp\left(\nsum\limits_{i = 1}^V \nsum\limits_{j=1}^H \theta_{ij} v_i h_j + \nsum\limits_{i = 1}^V\theta_{v_i} v_i + \nsum\limits_{j = 1}^H\theta_{h_j} h_j\right)}{\Nsum\limits_{\boldsymbol x \in \mathcal{X}}\exp\left(\nsum\limits_{i = 1}^V \nsum\limits_{j=1}^H \theta_{ij} v_i h_j + \nsum\limits_{i = 1}^V\theta_{v_i} v_i + \nsum\limits_{j = 1}^H\theta_{h_j} h_j\right)} 
\end{align}

\end{textblock}

%%-----------------------------------------------
%%
%%Top left box
%%
\begin{textblock}{14}(.65, 5.71)
{\large \bfseries Deep learning} \\[.25cm]
\begin{minipage}{.62\textwidth}
Three layer deep Boltzmann machine, with visible-to-hidden and hidden-to-hidden connections but no within-layer connections. This can be considered as multiple single layer restricted Boltzmann machines with the lower stack hidden layer acting as the visible layer for the higher stacked model. Claimed ability to learn "internal representations that become increasingly complex" \cite{salakhutdinov2009deep}, used in classification problems.
\end{minipage}
\begin{minipage}{.38\textwidth}
\begin{figure}
  \centering
  \input{images/deep_rbm.tikz}
  \caption{Deep RBM example.}
  \label{fig:deep_rbm}
\end{figure}
\end{minipage}


\end{textblock}

%%-----------------------------------------------
%%
%%Middle left box
%%
\begin{textblock}{14}(.65, 11.33)
{\large \bfseries Degeneracy, instability, and uninterpretability... Oh my!}\\[-1cm]

The highly flexible nature of the RBM ($H + V + HV$ parameters) makes the following characteristics of model impropriety of particular concern. \\[-.5cm]

\begin{table}
\centering
\begin{tabular}{| p{.48\linewidth} | p{.5\linewidth} |}
\hline
Characteristic & Detection \\
\hline
\hline
{\bfseries Near-degeneracy.} Occurs when there is a disproportionate amount of probability placed on only a few elements of the sample space by the model \cite{handcock2003assessing}. & If the mean parametrization on the model parameters, $\mu(\boldsymbol \theta)$, is close to the boundary of the convex hull of the set of statistics in the neg-potential function $Q(\boldsymbol x)$. \\
\hline
{\bfseries Instability.} Small changes in natural parameters result in large changes of the pmf, excessive sensitivity \cite{schweinberger2011instability}. & If for any $C > 0$ there exists $N_C > 0$ such that 
$\max\limits_{\boldsymbol x_N \in \mathcal{X}_N}[Q(\boldsymbol x_N)] > CN$ for all $N > N_C$. Where $Q(\cdot)$ is the neg-potential function of the model. \\
\hline
{\bfseries Uninterpretability.} Due to the existence of dependence, marginal mean-structure no longer maintained \cite{kaiser2007statistical}. & If the magnitude of the difference between model expectations and expectations under independence, $\left|\text{E}(\boldsymbol X | \boldsymbol \theta) - \text{E}(\boldsymbol X | \emptyset ) \right|$, is large.\\
\hline
\end{tabular}
\label{tab:degen}
\caption{Table of ``improper model'' characteristics.}
\end{table}
\end{textblock}

%%-----------------------------------------------
%%
%%Bottom left box
%%
\begin{textblock}{14}(.65, 21.5)
{\large \bfseries Avoiding degeneracy} \\[-.75cm]

For the $\{-1, 1 \}$ encoding of $\mathcal{V}$ and $\mathcal{H}$, the origin is the center of the parameter space. In particular, at $\boldsymbol \theta = \boldsymbol 0$, the RBM is equivalent to elements of $\boldsymbol X$ being distributed as iid Bernoulli$\left(\frac{1}{2}\right)$ r.v.s. $\Rightarrow$ No \emph{near-degeneracy}, \emph{instability}, or \emph{uninterpretability}!
\end{textblock}
\begin{textblock}{14.5}(.65, 23.9)
\begin{minipage}{.42\textwidth}
\begin{figure}[ht]
  \begin{minipage}{0.49\textwidth}
  \resizebox{\linewidth}{!}{
    \tdplotsetmaincoords{60}{-60}
    \input{images/toyhull_top.tikz}
    
    \draw(0,1,0) -- (1,0,0) -- (0,0,0) -- (0,1,0); 
     \draw(1,1,1) -- (1,0,0) -- (0,0,0) -- (1,1,1); 
     \draw(1,1,1) -- (0,1,0) -- (0,0,0) -- (1,1,1); 
     \draw(1,1,1) -- (0,1,0) -- (1,0,0) -- (1,1,1); 
    \end{tikzpicture}
  }
  \end{minipage}
  \begin{minipage}{0.49\textwidth}
  \resizebox{\linewidth}{!}{
    \tdplotsetmaincoords{60}{-60}
    \input{images/toyhull_top_2.tikz}
    
    \draw(-1,1,-1) -- (1,-1,-1) -- (-1,-1,-1) -- (-1,1,-1); 
     \draw(1,1,1) -- (1,-1,-1) -- (-1,-1,-1) -- (1,1,1); 
     \draw(1,1,1) -- (-1,1,-1) -- (-1,-1,-1) -- (1,1,1); 
     \draw(1,1,1) -- (-1,1,-1) -- (1,-1,-1) -- (1,1,1); 
    \end{tikzpicture}
  }
  \end{minipage}
  \caption{The convex hulls of the statistic space in three dimensions for a toy RBM with $|\mathcal{V}| = |\mathcal{H}| = 1$ for $\{0,1\}$-encoding (left) and $\{-1,1\}$-encoding (right) enclosed by an unrestricted hull of 3-space.}
\label{fig:toyhull}
\end{figure}
\end{minipage}
\begin{minipage}{.02\textwidth}
\hfill
\end{minipage}
\begin{minipage}{.55\textwidth}
\begin{figure}
\centering
\includegraphics[scale=1]{images/frac_volume.pdf}
\label{fig:frac_volume}
\caption{Volume relationship for the convex hulls of statistics in $Q(\cdot)$ vs. unrestricted space.}
\end{figure}
\end{minipage}

\end{textblock}

%%-----------------------------------------------
%%
%%Top right box
%%
\begin{textblock}{14}(32.4, 5.71)
{\large \bfseries Manageable (a.k.a. small) examples}
\begin{figure}
\centering
\includegraphics{images/degeneracy.pdf}
\includegraphics{images/instability.pdf}
\includegraphics{images/uninterpretability.pdf}
\label{fig:three_ways}
\caption{As the magnitude of $\boldsymbol \theta$ grows, so does the occurence of near-degeneracy, instability, and uninterpretability for RBMs of varying sizes.}
\end{figure}
\end{textblock}

%%-----------------------------------------------
%%
%%Middle right box
%%
\begin{textblock}{14}(32.4, 13.8)
{\large \bfseries Bayesian model fitting} \\
{\bfseries Idea:} To avoid model impropriety, avoid parts of the parameter space that lead to \emph{near-degeneracy}, \emph{instability}, and \emph{uninterpretability} (i.e., shrink $\boldsymbol \theta$). \\[-.75cm]

Simulated $n = 5,000$ images (4 pixels) from RBM model with 4 hiddens then fit using Bayesian methods, 
\vspace{-.5cm}
\begin{itemize}
\setlength\itemsep{.05cm}
\item {\bfseries Trick prior.} Cancel out the normalizing term, resulting full conditionals are normally distributed. {\bfseries Conclusion:} Scalable solution, but requires tuning.
\vspace{-.75cm}
$$\pi(\boldsymbol \theta) \propto \gamma(\boldsymbol \theta)^n \exp\left(-\frac{1}{2C_{1}}\boldsymbol \theta_{main}'\boldsymbol \theta_{main} -\frac{1}{2C_{2}}\boldsymbol \theta_{interaction}'\boldsymbol \theta_{interaction}\right), \vspace{-.75cm}
$$ 
where $\gamma(\boldsymbol \theta) = \Nsum\limits_{\boldsymbol x \in \mathcal{X}}\exp\left(\nsum\limits_{i = 1}^V \nsum\limits_{j=1}^H \theta_{ij} v_i h_j + \nsum\limits_{i = 1}^V\theta_{v_i} v_i + \nsum\limits_{j = 1}^H\theta_{h_j} h_j\right)$ and $C_{2} < C_{1}$ \cite{li2014biclustering}.
\item{\bfseries Truncated Normal prior.} Use two independent truncated spherical normal distributions as priors for $\theta_{main}$ and $\theta_{interaction}$ with $\sigma_{interaction} < \sigma_{main}$. Full conditional distributions are not conjugate, requires a geometric adaptive MH step \cite{zhou2014some} and calculation of likelihood normalizing constant. \\{\bfseries Conclusion:} Computationally intensive and convergence issues.
\item{\bfseries Marginalized likelihood.} Marginalize out $\boldsymbol h$ in $f_{\boldsymbol \theta}(\boldsymbol x)$, and use the truncated Normal prior. {\bfseries Conclusion:} Least scalable, but removes need to gain MCMC convergence for $Hn$ sampled hidden nodes.
\vspace{-.75cm}
$$
g_{\boldsymbol \theta}(\boldsymbol v) = \Nsum\limits_{\boldsymbol h \in \{-1,1\}^H} \exp\left(\nsum\limits_{i = 1}^V \nsum\limits_{j=1}^H \theta_{ij} v_i h_j + \nsum\limits_{i = 1}^V\theta_{v_i} v_i + \nsum\limits_{j = 1}^H\theta_{h_j} h_j\right). \vspace{-.75cm}
$$
\end{itemize}

\end{textblock}

\begin{textblock}{14.54}(31.86, 25.5)
\begin{figure}
\centering
\includegraphics{images/image_prediction.pdf}
\label{fig:image_pred}
\vspace*{-15mm}
\caption{Posterior probability of each possible 4-pixel image using priors above.}
\end{figure}

{\bfseries Big takeaway:} RBMs very easily are degenerate, unstable, and uninterpretable. To further complicate things, a rigorous fitting method for these models is not scalable and replicates the nonparametric solution (empirical distribution).
\end{textblock}

%%-----------------------------------------------
%%
%%Bottom right box
%%
%%\begin{textblock}{14}(32.4, 23.85)
%%{\large \bfseries Discussion} \\[.25cm]
%%\end{textblock}

%%\begin{textblock}{14.54}(31.86, 24.8)
%%These models very easily are degenerate, unstable, and uninterpretable. Meaning, the space of possible fitted parameter values that leads to a proper model is highly restricted.

%%To further complicate things, the proper fitting these models is very intricate and easily leads to parameter values running off to $\infty$.

%%If you're thinking of using RBMs, or stacked RBMs in a deep architecture, don't. Instead, fit a less flexible and more sensible model.

%%If you do not have an instance of every possible image in your training set, certain parameter must $\rightarrow \infty$, making fitting more complicated (shrink)

%%In reality it's practically impossible to have at least one of every possible image in your training set (large images, color scales, etc.)
%%\end{textblock}

%%-----------------------------------------------
%%
%%Bottom middle box
%%
\begin{textblock}{14.5}(16.17, 25.45)
{\large \bfseries References}
\printbibliography[heading=none]

\end{textblock}

\end{document}