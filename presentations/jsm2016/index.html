<!DOCTYPE html>
<html>
<head>
  <title>An Exposition on the Propriety of Restricted Boltzmann Machines</title>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="index_files/rmdshower/node_modules/shower-ribbon/styles/screen-4x3.css">
  <link rel="stylesheet" href="index_files/rmdshower/style-common.css">
  <link rel="stylesheet" href="index_files/rmdshower/style-ribbon.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.js"></script>
  <script src="index_files/rmdshower/auto-render.min.js" type="text/javascript"></script>
  
  
  
            <link rel="stylesheet" href="style.css"/>
      
  
</head>

<body class="shower list">

  <header class="caption">
    <h1>An Exposition on the Propriety of Restricted Boltzmann Machines</h1>
    <p><a href="http://bit.ly/jsm2016-rbm" class="uri">http://bit.ly/jsm2016-rbm</a></p>
  </header>

    
  
<section id="an-exposition-on-the-propriety-of-restricted-boltzmann-machines" class="slide level2 white">
<h2>An exposition on the propriety of restricted Boltzmann machines</h2>
<p><img src="images/title_background.jpg" class="cover" style="height: 100%" alt="https://www.flickr.com/photos/kevin53/13427959404/in/photolist-mszNvs-9MQYEj-4yexiR-ks7nip-6o3CdZ-ngtetn-canv9W-bmdAKd-4vidoh-6PLNA8-5gvtyL-3MDEAo-8SsP23-ks6CS4-oJ44cc-93LLtu-jYhtWc-e4nSK4-pJrmi4-62mmJ-dANTW2-8zyj8N-ks7nFP-cSDsyd-pmTfZt-9dL7vS-nyf63n-8q76xZ-dzE85u-63vAS1-djhyF4-pZKfE7-dr9oRc-8i1GXY-jmsNrz-4wC15t-4wBZMp-8XmJa8-co9uys-63nQSn-a5tzkd-fqbZm4-au3EME-mszUiu-sbVvKY-ajAP7X-9kUypA-o4stRL-J5n3or-7icjfY"></p>
<p class="white">
Andee Kaplan, Daniel Nordman, and Stephen Vardeman<br/> Iowa State University<br/> July 31, 2016<br/> JSM - Chicago, IL<br/> </br></br></br></br> <a href="http://bit.ly/jsm2016-rbm">http://bit.ly/jsm2016-rbm</a>
</p>
</section>
<section id="restricted-boltzmann-machines" class="titleslide slide level1"><h2 class="shout">Restricted Boltzmann machines</h2></section><section id="what-is-this" class="slide level2">
<h2>What is this?</h2>
<div class="double">
<p class="double-flow">
Restricted Boltzmann machine (RBM) with two layers - hidden (<span class="math inline">\(\mathcal{H}\)</span>) and visible (<span class="math inline">\(\mathcal{V}\)</span>) [smolensky1986information].
</p>
<p class="double-flow">
<img src="images/rbm.png" width="100%"></br>
</p>
</div>
<p><img src="images/visibles_image.png" width="100%"></br> Used for image classification. Each image pixel is a node in the visible layer. The output creates features, passed to supervised learning.</p>
</section><section id="joint-distribution" class="slide level2">
<h2>Joint Distribution</h2>
<p>Let <span class="math inline">\(x = \{h_1, ..., h_H, v_1, ...,v_V\}\)</span> represent the states of the visible and hidden nodes in an RBM. Then the probability each node taking the the value corresponding to <span class="math inline">\(x\)</span> is:</p>
<p><span class="math display">\[
f_{\theta} (x) = \frac{\exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}{\sum\limits_{x \in \mathcal{X}}\exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}
\]</span></p>
</section><section id="deep-learning" class="slide level2">
<h2>Deep learning</h2>
<div class="double">
<p class="double=flow">
“Deep Boltzmann machine” - multiple single layer restricted Boltzmann machines with the lower stack hidden layer acting as the visible layer for the higher stacked model
</p>
<p class="double-flow">
<img src="images/deep_rbm.png" width="100%"></br>
</p>
</div>
<p><br/></p>
<p>Claimed ability to learn “internal representations that become increasingly complex” <span class="citation" data-cites="salakhutdinov2009deep">(Salakhutdinov and Hinton 2009)</span>, used in classification problems.</p>
</section><section id="why-do-i-care" class="slide level2">
<h2>Why do I care?</h2>
<p>Current heuristic fitting methods seem to work for classification. Beyond classification, RBMs are generative models:</p>
<blockquote>
<p>To generate data from an RBM, we can start with a random state in one of the layers and then perform alternating Gibbs sampling. <span class="citation" data-cites="hinton2006fast">(Hinton, Osindero, and Teh 2006)</span></p>
</blockquote>
<p>Can we fit a model that generates data that looks like data?</p>
</section>
<section id="degeneracy-instability-and-uninterpretability.-oh-my" class="titleslide slide level1"><h2 class="shout">Degeneracy, instability, and uninterpretability. Oh my!</h2></section><section id="near-degeneracy" class="slide level2">
<h2>Near-degeneracy</h2>
<p>The highly flexible nature of the RBM (<span class="math inline">\(H + V + HV\)</span> parameters) makes three characteristics of model impropriety of particular concern.</p>
<p>Let <span class="math inline">\(Q(\cdot)\)</span> denotes the neg-potentional function of the model, having support set <span class="math inline">\(\mathcal{S}\)</span>.</p>
<table>
<colgroup>
<col style="width: 53%" />
<col style="width: 46%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Characteristic</th>
<th style="text-align: left;">Detection</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Disproportionate amount of probability placed on only a few elements of the sample space by the model <span class="citation" data-cites="handcock2003assessing">(Handcock et al. 2003)</span></td>
<td style="text-align: left;">If random variables in <span class="math inline">\(Q(\cdot)\)</span> have a collective mean <span class="math inline">\(\mu(\theta)\)</span> close to the boundary of the convex hull of <span class="math inline">\(\mathcal{S}\)</span>.</td>
</tr>
</tbody>
</table>
</section><section id="instability" class="slide level2">
<h2>Instability</h2>
<p>Let <span class="math inline">\(R(\theta) = \max_{v} \max_{h}Q(x) - \min_{v}\max_{h}Q(x) - H\log 2\)</span>.</p>
<table>
<colgroup>
<col style="width: 53%" />
<col style="width: 46%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Characteristic</th>
<th style="text-align: left;">Detection</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Small changes in natural parameters result in large changes in probability masses, excessive sensitivity <span class="citation" data-cites="schweinberger2011instability">(Schweinberger 2011)</span>.</td>
<td style="text-align: left;">If <span class="math inline">\(R(\theta)/V\)</span> is large, then the the maximum log-likelihood ratio of two images that differ in only one pixel is large.</td>
</tr>
</tbody>
</table>
</section><section id="uninterpretability" class="slide level2">
<h2>Uninterpretability</h2>
<table>
<colgroup>
<col style="width: 53%" />
<col style="width: 46%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Characteristic</th>
<th style="text-align: left;">Detection</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Due to the existence of dependence, marginal mean-structure no longer maintained <span class="citation" data-cites="kaiser2007statistical">(Kaiser 2007)</span>.</td>
<td style="text-align: left;">If the magnitude of the difference between model expectations and expectations under independence (dependence parameters of zero), <span class="math inline">\(\left\vert E( X \vert \theta) -E( X \vert \emptyset ) \right\vert\)</span>, is large.</td>
</tr>
</tbody>
</table>
</section><section id="manageable-a.k.a.-small-examples" class="slide level2">
<h2>Manageable (a.k.a. small) examples</h2>
<p><img src="index_files/figure-revealjs/unnamed-chunk-1-1.png" width="33%" /><img src="index_files/figure-revealjs/unnamed-chunk-1-2.png" width="33%" /><img src="index_files/figure-revealjs/unnamed-chunk-1-3.png" width="33%" /> RBMs easily are <em>near-degenerate</em>, <em>unstable</em>, and <em>uninterpretable</em> for large portions of parameter space.</p>
</section><section id="data-coding-to-mitigate-degeneracy" class="slide level2">
<h2>Data coding to mitigate degeneracy</h2>
<div class="double">
<p class="double-flow">
<img src="images/toyhull_top.png" width="100%">
</p>
<p class="double-flow">
<img src="images/toyhull_top_2.png" width="70%">
</p>
</div>
<p>Convex hulls of the statistic space for a toy RBM with <span class="math inline">\(V = H = 1\)</span> for <span class="math inline">\(\{0,1\}\)</span> and <span class="math inline">\(\{-1,1\}\)</span>-encoding enclosed by an unrestricted hull of 3-space.</p>
</section><section id="data-coding-to-mitigate-degeneracy-contd" class="slide level2">
<h2>Data coding to mitigate degeneracy (cont’d)</h2>
<ul>
<li>For the <span class="math inline">\(\{-1, 1 \}\)</span> encoding of <span class="math inline">\(\mathcal{V}\)</span> and <span class="math inline">\(\mathcal{H}\)</span>, the origin is the center of the parameter space</li>
<li>At <span class="math inline">\(\theta = 0\)</span>, the RBM is equivalent to elements of <span class="math inline">\(X\)</span> being distributed as iid Bernoulli<span class="math inline">\(\left(\frac{1}{2}\right)\)</span> <span class="math inline">\(\Rightarrow\)</span> No <em>near-degeneracy</em>, <em>instability</em>, or <em>uninterpretability</em>!</li>
</ul>
<p><img src="index_files/figure-revealjs/encoding-volume-1.png" /><!-- --></p>
</section>
<section id="bayesian-model-fitting" class="titleslide slide level1"><h2 class="shout">Bayesian model fitting</h2></section><section id="a-tale-of-three-methods" class="slide level2">
<h2>A tale of three methods</h2>
<ol type="1">
<li class="fragment next"><em>Trick prior.</em> Cancel out the normalizing term, full conditionals normally dist’d. <span class="math display">\[
\pi(\theta) \propto \gamma(\theta)^n \exp\left(-\frac{1}{2C_{1}} \theta_{main}&#39; \theta_{main} -\frac{1}{2C_{2}} \theta_{int}&#39; \theta_{int}\right),
\]</span> where <span class="math inline">\(\gamma(\theta)\)</span> is the normalizing term <span class="citation" data-cites="li2014biclustering">(Li 2014)</span>.</li>
<li class="fragment next"><em>Truncated Normal prior.</em> Use two independent truncated spherical normal distributions as priors for <span class="math inline">\(\theta_{main}\)</span> and <span class="math inline">\(\theta_{int}\)</span> with <span class="math inline">\(\sigma_{int} &lt; \sigma_{main}\)</span>. Not conjugate <span class="math inline">\(\Rightarrow\)</span> geometric adaptive MH step <span class="citation" data-cites="zhou2014some">(Zhou 2014)</span> and calculation of <span class="math inline">\(\gamma(\theta)\)</span>.</li>
<li class="fragment next"><em>Marginalized likelihood</em> Marginalize out <span class="math inline">\(h\)</span> in <span class="math inline">\(f_{\theta}(x)\)</span>, use trunc. Normal prior. <span class="math display">\[
g_{ \theta}(v) = \sum\limits_{h \in \{-1,1\}^H} \exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)
\]</span></li>
</ol>
</section><section id="posterior-distributions-of-images" class="slide level2">
<h2>Posterior distributions of images</h2>
<p><img src="index_files/figure-revealjs/models-plot-1.png" /><!-- --> Each of these rigorous fitting methods seeks to merely replicate the empirical data distribution, which is the likelihood maximizer.</p>
</section><section id="wrapping-up" class="slide level2">
<h2>Wrapping up</h2>
<ul>
<li>While RMBs can be useful for classification, in the context of a statistical model as a representation of data, RBMs poor fit due to
<ol type="1">
<li><em>near-degeneracy</em>,</li>
<li><em>instability</em>, and</li>
<li><em>uninterpretability</em>.</li>
</ol></li>
<li>Rigorous fitting methodology is possible but slow, and ultimately results in the empirical data distribution.</li>
</ul>
</section>
<section id="thank-you" class="titleslide slide level1"><h2 class="shout">Thank you!</h2></section><section id="references" class="slide level2 unnumbered small">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-handcock2003assessing">
<p>Handcock, Mark S, Garry Robins, Tom AB Snijders, Jim Moody, and Julian Besag. 2003. “Assessing Degeneracy in Statistical Models of Social Networks.” Working paper.</p>
</div>
<div id="ref-hinton2006fast">
<p>Hinton, Geoffrey E, Simon Osindero, and Yee-Whye Teh. 2006. “A Fast Learning Algorithm for Deep Belief Nets.” <em>Neural Computation</em> 18 (7). MIT Press: 1527–54.</p>
</div>
<div id="ref-kaiser2007statistical">
<p>Kaiser, Mark S. 2007. “Statistical Dependence in Markov Random Field Models.” <em>Statistics Preprints</em> Paper 57. Digital Repository @ Iowa State University. <a href="http://lib.dr.iastate.edu/stat_las_preprints/57/" class="uri">http://lib.dr.iastate.edu/stat_las_preprints/57/</a>.</p>
</div>
<div id="ref-li2014biclustering">
<p>Li, Jing. 2014. “Biclustering Methods and a Bayesian Approach to Fitting Boltzmann Machines in Statistical Learning.” PhD thesis, Iowa State University; Graduate Theses; Dissertations. <a href="http://lib.dr.iastate.edu/etd/14173/" class="uri">http://lib.dr.iastate.edu/etd/14173/</a>.</p>
</div>
<div id="ref-salakhutdinov2009deep">
<p>Salakhutdinov, Ruslan, and Geoffrey E Hinton. 2009. “Deep Boltzmann Machines.” In <em>International Conference on Artificial Intelligence and Statistics</em>, 448–55.</p>
</div>
<div id="ref-schweinberger2011instability">
<p>Schweinberger, Michael. 2011. “Instability, Sensitivity, and Degeneracy of Discrete Exponential Families.” <em>Journal of the American Statistical Association</em> 106 (496). Taylor &amp; Francis: 1361–70.</p>
</div>
<div id="ref-zhou2014some">
<p>Zhou, Wen. 2014. “Some Bayesian and Multivariate Analysis Methods in Statistical Machine Learning and Applications.” PhD thesis, Iowa State University; Graduate Theses; Dissertations. <a href="http://lib.dr.iastate.edu/etd/13816/" class="uri">http://lib.dr.iastate.edu/etd/13816/</a>.</p>
</div>
</div>
</section>

  <!--
  To hide progress bar from entire presentation
  just remove “progress” element.
  -->
  <!-- <div class="progress"></div> -->
  <script src="index_files/rmdshower/node_modules/shower/node_modules/shower-core/shower.min.js"></script>
  <!-- Copyright © 2015 Yours Truly, Famous Inc. -->
  <!-- Photos by John Carey, fiftyfootshadows.net -->

    <script>renderMathInElement(document.body);</script>
  
  
</body>
</html>
