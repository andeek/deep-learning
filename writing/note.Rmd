---
title: "A note on the instability and degeneracy of deep learning models"
author: "Andee Kaplan, Daniel Nordman, Stephen Vardeman"
date: "October, 2016"
linestretch: 1.5
output: 
  pdf_document:
    fig_caption: yes
    includes:
      in_header: header.tex
    number_sections: yes
bibliography: references/refs.bib
abstract: |
---

# Discrete Expanding Finite space models 

Consider $(X_1, \dots X_N)$, a set of discrete random variables with a finite sample space, $\mathcal{X}^N$. Let $P_{\boldsymbol \theta_N}(x_1, \dots, x_N)$ denote the probability of the data outcome $(x_1, \dots, x_N) \in \mathcal{X}^N$, which is a sequence of expanding models that potentially grow size as the dimension of the data become larger (i.e. $\boldsymbol \theta_N \in \mathbb{R}^{q(N)}$, where $q(N)$ is a function of $N$). Further, the model support of $P_{\boldsymbol \theta_N}(x_1, \dots, x_N)$ is equal to the sample space $\mathcal{X}^N$. We will call this class of models *Discrete Expanding Finite space*, or DEF models.

Note many model families can fall under the umbrella of DEF models. In Sections \ref{discrete-exponential-family-models}-\ref{deep-learning}, we present three specific examples of DEF models, including models with deep architectures.

## Discrete exponential family models

Let $X_1, \dots, X_N$ be discrete random variables with sample space $\mathcal{X}^N$, where $|\mathcal{X}| < \infty$. Consider discrete exponential family model with probability mass function of the form
$$
p_{\boldsymbol \lambda}(\boldsymbol x) = \exp\left[\boldsymbol\eta^T(\boldsymbol \lambda) \boldsymbol g(\boldsymbol x) - \psi(\boldsymbol \lambda)\right], \qquad \boldsymbol x \in \mathcal{X}^N.
$$ 
where $\boldsymbol \lambda \in \Lambda \subset \mathbb{R}^{q(N)}$, $\boldsymbol \eta : \mathbb{R}^q \mapsto \mathbb{R}^L$ is a vector of natural parameters, $\boldsymbol g : \mathcal{X}^N \mapsto \mathbb{R}^L$ is a vector of sufficient statistics, 
$$
\psi(\boldsymbol \lambda) = \log \sum\limits_{\boldsymbol x \in \mathcal{X}^N}\exp\left[\boldsymbol \eta^T(\boldsymbol \lambda) \boldsymbol g(\boldsymbol x) \right], \qquad \boldsymbol \lambda \in \Lambda
$$
is the normalizing function, and $\Lambda = \{\boldsymbol \lambda \in \mathbb{R}^k : \psi(\boldsymbol \lambda) < \infty, k \le q(N) \}$ is the parameter space. 

By above definition, $X_1, \dots, X_N$ are discrete with a finite sample space. Let $\boldsymbol \theta = (\lambda_1, \dots, \lambda_k, 0, \dots, 0)$, where the first $k$ elements are equal to the $k$ elements of $\boldsymbol \lambda$ and for $i = k + 1, \dots, q(N)$, $\theta_i = 0$. Further, because $p_{\boldsymbol \lambda}(\boldsymbol x) > 0$ for all $\boldsymbol x \in \mathcal{X}^N$ by properties of the exponential function, the discrete exponential family models defined in this section are a special case of the DEF model.

## Restricted Boltzmann machines

A restricted Boltzmann machine (RBM) is an undirected graphical model specified for discrete or continuous random variables, binary variables being most common [@smolensky1986information]. Consider the binary case of an RBM only. A RBM architecture has two layers, hidden ($\mathcal{H}$) and visible ($\mathcal{V}$), with conditional independence within each layer. Let $\boldsymbol X = \{H_1, \dots, H_H, V_1,\dots, V_V\}$ be random variables corresponding to the hidden and visible variables in an RBM. We will consider the sample space $\mathcal{X} = \{-1,1\}$. The RBM is then has the joint probability mass function
$$
p_{\boldsymbol \theta} (\boldsymbol x) = \exp\left[ \boldsymbol \alpha^T \boldsymbol h + \boldsymbol \beta^T \boldsymbol v + \boldsymbol h^T \Gamma \boldsymbol v - \psi(\boldsymbol \theta)\right], \qquad \boldsymbol x = (\boldsymbol h, \boldsymbol v) \in \mathcal{X}^{H+V}
$$
where 
$$
\psi(\boldsymbol \theta) = \log \sum_{\boldsymbol x \in \mathcal{X}^{H + V}} \exp\left[ \boldsymbol \alpha^T \boldsymbol h + \boldsymbol \beta^T \boldsymbol v + \boldsymbol h^T \Gamma \boldsymbol v\right], \qquad \boldsymbol \theta \in \Theta
$$ 
is the normalizing function, $\boldsymbol \alpha \in \mathbb{R}^H$, $\boldsymbol \beta \in \mathbb{R}^V$, and $\Gamma$ is a matrix with dimension $H \times V$ are the parameters in the model. Let $\boldsymbol \theta = (\boldsymbol \alpha, \boldsymbol \beta,\Gamma) \in \Theta \subset \mathbb{R}^{H + V + H*V}$ denote the combined vector of parameters.

The probability mass function for the visible random variables $V_1, \dots, V_V$ follows from this joint specification:
$$
p_{\boldsymbol \theta} (\boldsymbol v) = \sum\limits_{\boldsymbol h \in \mathcal{X}^H} p_{\boldsymbol \theta} (\boldsymbol x), \qquad \boldsymbol v \in \mathcal{X}^V
$$

By definition, $V_1, \dots, V_V$ are discrete with a finite sample space. The vector of model parameters $\boldsymbol \theta$, of size $q(V) \equiv V + H + H*V$, is expanding in size as a function of the dimension of $V_1, \dots, V_V$. Additionally, $p_{\boldsymbol \theta}(\boldsymbol v) > 0$ for all $\boldsymbol v \in \mathcal{X}^V$ by properties of the exponential function. Thus, the visible RBM model specification is an example of a DEF model.

## Deep learning

# Instability and degeneracy results

To quantify "instability" in a model with discrete outcomes, it is useful to imagine how a data model might be expanded to incorporate more and more observations. A relevant quantity to this end is a (scaled) extremal log-probability ratio given by 
\begin{align}
\frac{1}{N} \log \left[\frac{\max\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(x_1, \dots, x_N)}{\min\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta}(x_1, \dots, x_N)}\right] \equiv \frac{1}{N} \text{ELPR}_N(\boldsymbol \theta_N) \label{eqn:elpr}
\end{align}


Essentially, in the process of developing a model to handle increasing numbers of variable (i.e., $N \rightarrow \infty$), we want (\ref{eqn:elpr}) to stay bounded, meaning that the largest probability under this model should maintain a magnitude within the smallest probability allowed under the model. This leads to the following definition:

\begin{defn}[unstable DEF]
A DEF model formulation is \emph{unstable} if, as the number of variables increase ($N \rightarrow \infty$), it holds that 
\begin{align*}
\lim\limits_{N \rightarrow \infty} \frac{1}{N} \log \left[\frac{\max\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(x_1, \dots, x_N)}{\min\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta}(x_1, \dots, x_N)}\right] = \infty.
\end{align*}
\end{defn}
In other words, given any $C > 0$, there exists an integer $N_C > 0$ so that $\frac{1}{N}\text{ELPR}_N(\boldsymbol \theta_N) > C$ for all $N \ge N_C$.

Note that this definition of *unstable* is a generalization of that in @schweinberger2011instability allowing nonexponential family models and an increasing number of parameters (while this definition appears different, it does match that in @schweinberger2011instability for the special exponential models considered there, see Section \ref{discrete-exponential-family-models} for details).

Unstable DEF models are undesirable for several reasons. One is that small changes in data can lead to overly-sensitive changes in probability. Consider, for example, 
$$
\Delta_N(\boldsymbol \theta_N) \equiv \max \left\{\log \frac{P_{\boldsymbol \theta_N}(x_1, \dots, x_N)}{P_{\boldsymbol \theta_N}(x_1^*, \dots, x_N^*)} : (x_1, \dots, x_N) \& (x_1^*, \dots, x_N^*) \in \mathcal{X}^N \text{ differ by exactly one component}\right\},
$$
the biggest log-probability ratio for a one component change in data outcomes. We then have the following result.

\begin{prop}
\label{prop:instab}
If $\frac{1}{N}\text{ELPR}_N(\boldsymbol \theta_N) > C$, then $\Delta_N(\boldsymbol \theta_N) > C$.
\end{prop}

In other words, if the quantity (\ref{eqn:elpr}) is too large, then the DEF model will exhibit large probability shifts for very small changes in the data configuration (i.e. instability mentioned earlier).

**Proof of Proposition \ref{prop:instab}.** We prove the contrapositive. Suppose that $\Delta_N(\boldsymbol \theta_N) \le C$ holds for some $C > 0$. Under the DEF model, $P_{\boldsymbol \theta_V}(\boldsymbol x) > 0$ holds for each outcome $\boldsymbol x \in \mathcal{X}^N$. Let $\boldsymbol x_{min} \equiv \argmin\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)$ and $\boldsymbol x_{max} \equiv \argmax\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)$. Note there exists a sequence $\boldsymbol x_{min} \equiv \boldsymbol x_0, \boldsymbol x_1, \dots, \boldsymbol x_k \equiv \boldsymbol x_{max}$ in $\mathcal{X}^N$ of component-wise switches to move from $\boldsymbol x_{min}$ to $\boldsymbol x_{max}$ in the sample space (i.e. $\boldsymbol x_i, \boldsymbol x_{i + 1} \in \mathcal{X}^N$ differ by exactly $1$ component for $i = 0, \dots, k$) for some integer $k \in \{0, 1, \dots, N\}$. Then
\begin{align*}
\log\left[\frac{P_{\boldsymbol \theta_N}(\boldsymbol x_{max})}{P_{\boldsymbol \theta_N}(\boldsymbol x_{min})}\right] &= \left|\sum\limits_{i = 1}^k\log\left(\frac{P_{\boldsymbol \theta_N}(\boldsymbol x_i)}{P_{\boldsymbol \theta_N}(\boldsymbol x_{i-1})}\right)\right| \\
& \le \sum\limits_{i = 1}^k\left|\log\left(\frac{P_{\boldsymbol \theta_N}(\boldsymbol x_i)}{P_{\boldsymbol \theta}(\boldsymbol x_{i-1})}\right)\right| \\
& \le k \Delta_N(\boldsymbol \theta_N) \le NC
\end{align*}
using $k \le N$ and $\Delta_N(\boldsymbol \theta_N) \le C$.$\Box$

Additionally, unstable RBM models are connected to degenerate models, where *degeneracy* implies placing all probability on a small pieace of the sample space. Define a model set 
$$
M_{\epsilon, \boldsymbol \theta_N} \equiv \left\{(x_1, \dots, x_N) \in \mathcal{X}^N: \log P_{\boldsymbol \theta_N}(x_1, \dots, x_N) > (1-\epsilon)\max\limits_{(x_1^*, \dots, x_N^*)}P_{\boldsymbol \theta_N}(x_1^*, \dots, x_N^*) + \epsilon\min\limits_{(x_1^*, \dots, x_N^*)}P_{\boldsymbol \theta_N}(x_1^*, \dots, x_N^*) \right\}
$$
of possible outcomes, for a given $0 < \epsilon < 1$.

\begin{prop}
\label{prop:degen}
For an unstable DEF model, and for any given $0 < \epsilon < 1$, $P_{\boldsymbol \theta_N}\left((x_1, \dots, x_N) \in M_{\epsilon, \boldsymbol \theta_N}\right) \rightarrow 1$ holds as $N \rightarrow \infty$.
\end{prop}

In other words, as a consequence of unstable models, all probability will stack up on mode sets or potentially those few outcomes with the highest probability.

**Proof of Proposition \ref{prop:degen}.** Define $\boldsymbol x_{max}$ and $\boldsymbol x_{min}$ as in the proof of Proposition \ref{prop:instab}. Fix $0 < \epsilon < 1$ Then, $\boldsymbol x_{max} \in M_{\epsilon, \boldsymbol \theta_N}$, so $P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N}) \ge P_{\boldsymbol \theta_N}(\boldsymbol x_{max})$. If $\boldsymbol x \in \mathcal{X}^N \setminus M_{\epsilon, \boldsymbol \theta_N}$, then by definition $P_{\boldsymbol \theta_N}(\boldsymbol x) \le [P_{\boldsymbol \theta_N}(\boldsymbol x_{max})]^{1-\epsilon}[P_{\boldsymbol \theta_N}(\boldsymbol x_{min})]^{\epsilon}$ holds so that
\begin{align*}
1-P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N}) &= P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N}^C) \\
& = \sum\limits_{\boldsymbol x \in \mathcal{X}^N \setminus M_{\epsilon, \boldsymbol \theta_N}}P_{\boldsymbol \theta_N}(\boldsymbol x) \\
& \le (2^N)[P_{\boldsymbol \theta_N}(\boldsymbol x_{max})]^{1-\epsilon}[P_{\boldsymbol \theta_N}(\boldsymbol x_{min})]^{\epsilon}
\end{align*}
Then,
\begin{align*}
\frac{1}{N}\log\left[\frac{P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N})}{1-P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N})}\right] & \ge \frac{1}{N} \log\left[\frac{P_{\boldsymbol\theta_N}(\boldsymbol x_{max})}{(2^V)[P_{\boldsymbol \theta_N}(\boldsymbol x_{max})]^{1-\epsilon}[P_{\boldsymbol \theta_N}(\boldsymbol x_{min})]^{\epsilon}}\right] \\
&= \frac{\epsilon}{N} \log\left[\frac{P_{\boldsymbol \theta_N}(\boldsymbol x_{max})}{P_{\boldsymbol \theta_N}(\boldsymbol x_{min})}\right] - \log 2 \rightarrow \infty
\end{align*}
as $N \rightarrow \infty$ by the definition of an unstable DEF model.$\Box$

# Implications

# References