---
title: "A note on the instability and degeneracy of deep learning models"
author: "Andee Kaplan, Daniel Nordman, Stephen Vardeman"
date: "October, 2016"
linestretch: 2
output: 
  pdf_document:
    fig_caption: yes
    includes:
      in_header: header.tex
    number_sections: yes
bibliography: references/refs.bib
abstract: |
  We introduce *Discrete Expanding Finite space*, or DEF models, a broad class of discrete space models that are defined in relation to how a probability model is built to incorporate more variables. These models are defined to be in discrete finite space with model support equal the sample space and with a sequence of expanding model parameter vectors that potentially grow size as the dimension of the data become larger. In this paper, we present results that define the notion of model instability as well as near-degeneracy within the framework of DEF models in addition to demonstrating example models of the DEF class.
---

# Introduction

In this paper, we introduce a broad class of discrete space models that are defined in relation to how a probability model is built to incorporate more variables. Consider $(X_1, \dots X_N)$, a set of discrete random variables with a finite sample space, $\mathcal{X}^N$. Let $P_{\boldsymbol \theta_N}(x_1, \dots, x_N)$ denote the probability of the data outcome $(x_1, \dots, x_N) \in \mathcal{X}^N$, which is a sequence of expanding models that potentially grow size as the dimension of the data become larger (i.e. $\boldsymbol \theta_N \in \mathbb{R}^{q(N)}$, where $q(N)$ is a function of $N$). Further, the model support of $P_{\boldsymbol \theta_N}(x_1, \dots, x_N)$ is equal to the sample space $\mathcal{X}^N$. We will call this class of models *Discrete Expanding Finite space*, or DEF models.

This paper is structured as follows. First, in Section \ref{examples}, we present specific examples of DEF models and discuss how they fit in this framework (including deep learning models). Section \ref{instability-and-degeneracy-results} establishes formal results regarding the model propriety of DEF models. Lastly, Section \ref{implications} emphasizes the implications of our model propriety results.

# Examples

Note many model families can fall under the umbrella of DEF models. In this section, we present three specific examples of DEF models, including models with deep architectures.

## Discrete exponential family models

Let $X_1, \dots, X_N$ be discrete random variables with sample space $\mathcal{X}^N$, where $|\mathcal{X}| < \infty$. Consider discrete exponential family model with probability mass function of the form
$$
p_{\boldsymbol \lambda}(\boldsymbol x) = \exp\left[\boldsymbol\eta^T(\boldsymbol \lambda) \boldsymbol g(\boldsymbol x) - \psi(\boldsymbol \lambda)\right], \qquad \boldsymbol x \in \mathcal{X}^N.
$$ 
where $\boldsymbol \lambda \in \Lambda \subset \mathbb{R}^{q(N)}$, $\boldsymbol \eta : \mathbb{R}^q \mapsto \mathbb{R}^L$ is a vector of natural parameters, $\boldsymbol g : \mathcal{X}^N \mapsto \mathbb{R}^L$ is a vector of sufficient statistics, 
$$
\psi(\boldsymbol \lambda) = \log \sum\limits_{\boldsymbol x \in \mathcal{X}^N}\exp\left[\boldsymbol \eta^T(\boldsymbol \lambda) \boldsymbol g(\boldsymbol x) \right], \qquad \boldsymbol \lambda \in \Lambda
$$
is the normalizing function, and $\Lambda = \{\boldsymbol \lambda \in \mathbb{R}^k : \psi(\boldsymbol \lambda) < \infty, k \le q(N) \}$ is the parameter space. 

By above definition, $X_1, \dots, X_N$ are discrete with a finite sample space. Let $\boldsymbol \theta = (\lambda_1, \dots, \lambda_k, 0, \dots, 0)$, where the first $k$ elements are equal to the $k$ elements of $\boldsymbol \lambda$ and for $i = k + 1, \dots, q(N)$, $\theta_i = 0$. Further, because $p_{\boldsymbol \lambda}(\boldsymbol x) > 0$ for all $\boldsymbol x \in \mathcal{X}^N$ by properties of the exponential function, the discrete exponential family models defined in this section are a special case of the DEF model.

## Restricted Boltzmann machines

A restricted Boltzmann machine (RBM) is an undirected graphical model specified for discrete or continuous random variables, binary variables being most common [@smolensky1986information]. Consider the binary case of an RBM only. A RBM architecture has two layers, hidden ($\mathcal{H}$) and visible ($\mathcal{V}$), with conditional independence within each layer. Let $\boldsymbol X = \{H_1, \dots, H_H, V_1,\dots, V_V\}$ be random variables corresponding to the hidden and visible variables in an RBM. We will consider the sample space $\mathcal{X} = \{-1,1\}$. The RBM is then has the joint probability mass function
$$
p_{\boldsymbol \theta} (\boldsymbol x) = \exp\left[ \boldsymbol \alpha^T \boldsymbol h + \boldsymbol \beta^T \boldsymbol v + \boldsymbol h^T \Gamma \boldsymbol v - \psi(\boldsymbol \theta)\right], \qquad \boldsymbol x = (\boldsymbol h, \boldsymbol v) \in \mathcal{X}^{H+V}
$$
where 
$$
\psi(\boldsymbol \theta) = \log \sum_{\boldsymbol x \in \mathcal{X}^{H + V}} \exp\left[ \boldsymbol \alpha^T \boldsymbol h + \boldsymbol \beta^T \boldsymbol v + \boldsymbol h^T \Gamma \boldsymbol v\right], \qquad \boldsymbol \theta \in \Theta
$$ 
is the normalizing function, $\boldsymbol \alpha \in \mathbb{R}^H$, $\boldsymbol \beta \in \mathbb{R}^V$, and $\Gamma$ is a matrix with dimension $H \times V$ are the parameters in the model. Let $\boldsymbol \theta = (\boldsymbol \alpha, \boldsymbol \beta,\Gamma) \in \Theta \subset \mathbb{R}^{H + V + H*V}$ denote the combined vector of parameters.

The probability mass function for the visible random variables $V_1, \dots, V_V$ follows from this joint specification:
$$
p_{\boldsymbol \theta} (\boldsymbol v) = \sum\limits_{\boldsymbol h \in \mathcal{X}^H} p_{\boldsymbol \theta} (\boldsymbol x), \qquad \boldsymbol v \in \mathcal{X}^V
$$

By definition, $V_1, \dots, V_V$ are discrete with a finite sample space. The vector of model parameters $\boldsymbol \theta$, of size $q(V) \equiv V + H + H*V$, is expanding in size as a function of the dimension of $V_1, \dots, V_V$. Additionally, $p_{\boldsymbol \theta}(\boldsymbol v) > 0$ for all $\boldsymbol v \in \mathcal{X}^V$ by properties of the exponential function. Thus, the visible RBM model specification is an example of a DEF model.

## Deep learning

Consider two models with "deep architechture" that contain multiple hidden (or latent) layers in addition to a visible layer of data, a deep Boltzmann machine [@salakhutdinov2009deep] and a deep belief network [@hinton2006fast]. Let $M$ denote the number of hidden layers included in the model and $H_1, \dots, H_M$ the number of hidden variables within each hidden layer. Then let $\boldsymbol X = \{H^{(1)}_1, \dots, H^{(1)}_{H_1}, \dots, H^{(M)}_1, \dots, H^{(M)}_{H_M}, V_1,\dots, V_V\}$ be random variables corresponding to the hidden and visible variables in a deep probabilitic model. We will consider the sample space $\mathcal{X} = \{-1,1\}$.

**Deep Boltzmann machine (DBM).** The DBM class of models maintains conditional independence within all layers in the model by stacking RBM models and maintaining the restriction of only allowing conditional dependence between layers. The joint probability mass function for a DBM is
$$
p_{\boldsymbol \theta} (\boldsymbol x) = \exp\left[ \sum\limits_{i = 1}^M\boldsymbol \alpha^{(i)T} \boldsymbol h^{(i)} + \boldsymbol \beta^T \boldsymbol v + \boldsymbol h^{(1)T} \Gamma^{(0)} \boldsymbol v + \sum\limits_{i = 1}^{M - 1} \boldsymbol h^{(i)T} \Gamma^{(i)} \boldsymbol h^{(i + 1)} - \psi(\boldsymbol \theta) \right], 
$$
where $\boldsymbol x = (\boldsymbol h^{(1)}, \dots, \boldsymbol h^{(M)}, \boldsymbol v) \in \mathcal{X}^{H_1 + \cdots + H_M +V}$ and
$$
\psi(\boldsymbol \theta) = \log \sum_{\boldsymbol x \in \mathcal{X}^{H_1 + \cdots + H_M + V}} \exp\left[ \sum\limits_{i = 1}^M\boldsymbol \alpha^{(i)T} \boldsymbol h^{(i)} + \boldsymbol \beta^T \boldsymbol v + \boldsymbol h^{(1)T} \Gamma^{(0)} \boldsymbol v + \sum\limits_{i = 1}^{M - 1} \boldsymbol h^{(i)T} \Gamma^{(i)} \boldsymbol h^{(i + 1)}\right], \qquad \boldsymbol \theta \in \Theta
$$ 
is the normalizing function, $\boldsymbol \beta \in \mathbb{R}^V$, $\boldsymbol \alpha^{(i)} \in \mathbb{R}^{H_i}$ for $i = 1, \dots, M$, $\Gamma^{(0)}$ is a matrix with dimension $H_1 \times V$, and $\Gamma^{(i)}$ is a matrix with dimension $H_i \times H_{i + 1}$ for $i = 1, \dots, M-1$ are the parameters in the model. Let $\boldsymbol \theta = (\boldsymbol \alpha^{(1)}, \dots, \boldsymbol \alpha^{(M)}, \boldsymbol \beta,\Gamma^{(0)}, \dots, \Gamma^{(M - 1)}) \in \Theta \subset \mathbb{R}^{H_1 + \cdots + H_M + V + H_1*V + H_1 *H_{2} + \cdots + H_{M-1}*H_M}$ denote the combined vector of parameters.

The probability mass function for the visible random variables $V_1, \dots, V_V$ follows from this joint specification:
$$
p_{\boldsymbol \theta} (\boldsymbol v) = \sum\limits_{(\boldsymbol h^{(1)}, \dots, \boldsymbol h^{(M)}) \in \mathcal{X}^{H_1 + \cdots + H_M}} p_{\boldsymbol \theta} (\boldsymbol x), \qquad \boldsymbol v \in \mathcal{X}^V
$$

By definition, $V_1, \dots, V_V$ are discrete with a finite sample space. The vector of model parameters $\boldsymbol \theta$, of size $q(V) \equiv H_1 + \cdots + H_M + V + H_1*V + H_1 *H_{2} + \cdots + H_{M-1}*H_M$, is expanding in size as a function of the dimension of $V_1, \dots, V_V$. Additionally, $p_{\boldsymbol \theta}(\boldsymbol v) > 0$ for all $\boldsymbol v \in \mathcal{X}^V$ by properties of the exponential function. Thus, the visible RBM model specification is an example of a DEF model.

**Deep belief network (DBN).** A DBN is similar to a DBM in that there are multiple layers of latent random variables stacked in a deep architecture with no conditional dependence between layers. The difference between the two models is that all but the last stacked model are Bayesian networks, rather than RBMs. Thus for $V_1, \dots, V_V$ discrete with finite sample space and support equal to the sample space $\mathcal{X}^V$, a DBN is also a DEF model if the number of components in the parameter vector is dependent on the dimension of the data, $V$. Commonly, as in a logistic belief net [@neal1992connectionist], a "weight" parameter is placed on each interaction between visibles, $V_1, \dots, V_V$ and the first layer of latent variables, $H^{(1)}_1, \dots, H^{(1)}_{H_1}$, satisfying the definition of a DEF model.

# Instability and degeneracy results

To quantify "instability" in a model with discrete outcomes, it is useful to imagine how a data model might be expanded to incorporate more and more observations. A relevant quantity to this end is a (scaled) extremal log-probability ratio given by 
\begin{align}
\frac{1}{N} \log \left[\frac{\max\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(x_1, \dots, x_N)}{\min\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta}(x_1, \dots, x_N)}\right] \equiv \frac{1}{N} \text{ELPR}_N(\boldsymbol \theta_N) \label{eqn:elpr}
\end{align}


Essentially, in formulating a model for potentially increasing numbers of variables (i.e., $N \rightarrow \infty$), the ratio (\ref{eqn:elpr}) should bounded, meaning that the largest probability under this model at parameter $\boldsymbol \theta_N$ should maintain a magnitude within the smallest probability allowed under the same model. This leads to the following definition:

\begin{defn}[S-unstable DEF]
Let $\boldsymbol \theta_N \in \mathbb{R}^{q(N)}$ be a sequence of DEF model parameters where the size of the model $q(N)$ is a function of the number of random variables $N$. A DEF model formulation is \emph{Schweinberger-unstable} or \emph{S-unstable} if, as the number of variables increase ($N \rightarrow \infty$), it holds that 
\begin{align*}
\lim\limits_{N \rightarrow \infty} \frac{1}{N} \text{ELPR}(\boldsymbol \theta_N) \equiv \lim\limits_{N \rightarrow \infty} \frac{1}{N} \log \left[\frac{\max\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(x_1, \dots, x_N)}{\min\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta}(x_1, \dots, x_N)}\right] = \infty.
\end{align*}
\end{defn}
In other words, given any $C > 0$, there exists an integer $N_C > 0$ so that $\frac{1}{N}\text{ELPR}_N(\boldsymbol \theta_N) > C$ for all $N \ge N_C$.

Note that this definition of *S-unstable* is a generalization or reinterpretation of "unstable" used in @schweinberger2011instability by allowing nonexponential family models and an increasing number of parameters. While this definition appears different, it does match that in @schweinberger2011instability for the special exponential models considered there, see Section \ref{discrete-exponential-family-models} for details.

S-unstable DEF models are undesirable for several reasons. One is that small changes in data can lead to overly-sensitive changes in probability. Consider, for example, 
$$
\Delta(\boldsymbol \theta_N) \equiv \max \left\{\log \frac{P_{\boldsymbol \theta_N}(x_1, \dots, x_N)}{P_{\boldsymbol \theta_N}(x_1^*, \dots, x_N^*)} : (x_1, \dots, x_N) \& (x_1^*, \dots, x_N^*) \in \mathcal{X}^N \text{ differ by exactly one component}\right\},
$$
the biggest log-probability ratio for a one component change in data outcomes at a DEF parameter $\boldsymbol \theta_N$. We then have the following result.

\begin{prop}
\label{prop:instab}
Let $C > 0$ and let $\text{ELPR}(\boldsymbol \theta_N)$ be as in (\ref{eqn:elpr}) for an integer $N \ge 1$. If $\frac{1}{N}\text{ELPR}_N(\boldsymbol \theta_N) > C$, then $\Delta_N(\boldsymbol \theta_N) > C$.
\end{prop}

In other words, if the probability ratio (\ref{eqn:elpr}) is too large, then the DEF model will exhibit large probability shifts for very small changes in the data configuration (i.e., instability).

**Proof of Proposition \ref{prop:instab}.** We prove the contrapositive. Suppose that $\Delta(\boldsymbol \theta_N) \le C$ holds for some $C > 0$. Under the DEF model, $P_{\boldsymbol \theta_N}(\boldsymbol x) > 0$ holds for each outcome $\boldsymbol x \in \mathcal{X}^N$. Let $\boldsymbol x_{min} \equiv \argmin\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)$ and $\boldsymbol x_{max} \equiv \argmax\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)$. Note there exists a sequence $\boldsymbol x_{min} \equiv \boldsymbol x_0, \boldsymbol x_1, \dots, \boldsymbol x_k \equiv \boldsymbol x_{max}$ in $\mathcal{X}^N$ of component-wise switches to move from $\boldsymbol x_{min}$ to $\boldsymbol x_{max}$ in the sample space (i.e. $\boldsymbol x_i, \boldsymbol x_{i + 1} \in \mathcal{X}^N$ differ by exactly $1$ component for $i = 0, \dots, k$) for some integer $k \in \{0, 1, \dots, N\}$. Then, if $k > 0$, it follows that
\begin{align*}
\log\left[\frac{P_{\boldsymbol \theta_N}(\boldsymbol x_{max})}{P_{\boldsymbol \theta_N}(\boldsymbol x_{min})}\right] &= \left|\sum\limits_{i = 1}^k\log\left(\frac{P_{\boldsymbol \theta_N}(\boldsymbol x_i)}{P_{\boldsymbol \theta_N}(\boldsymbol x_{i-1})}\right)\right| \\
& \le \sum\limits_{i = 1}^k\left|\log\left(\frac{P_{\boldsymbol \theta_N}(\boldsymbol x_i)}{P_{\boldsymbol \theta}(\boldsymbol x_{i-1})}\right)\right| \\
& \le k \Delta_N(\boldsymbol \theta_N) \le NC
\end{align*}
using $k \le N$ and $\Delta(\boldsymbol \theta_N) \le C$. If $k = 0$, then $\boldsymbol x_{max} = \boldsymbol x_{min}$ and the same bound above holds.$\Box$

Additionally, S-unstable DEF models are connected to degenerate models, where *degeneracy* implies placing all probability on a small portion of the sample space. Define a modal set 
$$
M_{\epsilon, \boldsymbol \theta_N} \equiv \left\{(x_1, \dots, x_N) \in \mathcal{X}^N: \log P_{\boldsymbol \theta_N}(x_1, \dots, x_N) > (1-\epsilon)\max\limits_{(x_1^*, \dots, x_N^*)}P_{\boldsymbol \theta_N}(x_1^*, \dots, x_N^*) + \epsilon\min\limits_{(x_1^*, \dots, x_N^*)}P_{\boldsymbol \theta_N}(x_1^*, \dots, x_N^*) \right\}
$$
of possible outcomes, for a given $0 < \epsilon < 1$ and along a sequence $\boldsymbol \theta_N$, $N \ge 1$, of DEF parameters.

\begin{prop}
\label{prop:degen}
For an unstable DEF model, and for any given $0 < \epsilon < 1$, $P_{\boldsymbol \theta_N}\left((x_1, \dots, x_N) \in M_{\epsilon, \boldsymbol \theta_N}\right) \rightarrow 1$ holds as $N \rightarrow \infty$.
\end{prop}

In other words, as a consequence of S-unstable models, all probability in the DEF model formulation with a large number of random variables will stack up on mode sets or simply those few outcomes with the highest probability.

**Proof of Proposition \ref{prop:degen}.** Define $\boldsymbol x_{max}$ and $\boldsymbol x_{min}$ as in the proof of Proposition \ref{prop:instab}. Fix $0 < \epsilon < 1$ Then, $\boldsymbol x_{max} \in M_{\epsilon, \boldsymbol \theta_N}$, so $P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N}) \ge P_{\boldsymbol \theta_N}(\boldsymbol x_{max})$. If $\boldsymbol x \in \mathcal{X}^N \setminus M_{\epsilon, \boldsymbol \theta_N}$, then by definition $P_{\boldsymbol \theta_N}(\boldsymbol x) \le [P_{\boldsymbol \theta_N}(\boldsymbol x_{max})]^{1-\epsilon}[P_{\boldsymbol \theta_N}(\boldsymbol x_{min})]^{\epsilon}$ holds so that
\begin{align*}
1-P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N})
& = \sum\limits_{\boldsymbol x \in \mathcal{X}^N \setminus M_{\epsilon, \boldsymbol \theta_N}}P_{\boldsymbol \theta_N}(\boldsymbol x) \\
& \le (2^N)[P_{\boldsymbol \theta_N}(\boldsymbol x_{max})]^{1-\epsilon}[P_{\boldsymbol \theta_N}(\boldsymbol x_{min})]^{\epsilon}
\end{align*}
Then, it follows that
\begin{align*}
\frac{1}{N}\log\left[\frac{P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N})}{1-P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N})}\right] & \ge \frac{1}{N} \log\left[\frac{P_{\boldsymbol\theta_N}(\boldsymbol x_{max})}{(2^V)[P_{\boldsymbol \theta_N}(\boldsymbol x_{max})]^{1-\epsilon}[P_{\boldsymbol \theta_N}(\boldsymbol x_{min})]^{\epsilon}}\right] \\
&= \frac{\epsilon}{N} \log\left[\frac{P_{\boldsymbol \theta_N}(\boldsymbol x_{max})}{P_{\boldsymbol \theta_N}(\boldsymbol x_{min})}\right] - \log 2 \rightarrow \infty
\end{align*}
as $N \rightarrow \infty$ by the definition of an unstable DEF model. Consequently, $P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N}) \rightarrow 1$ as $N \rightarrow \infty$ as claimed.$\Box$

\begin{remark}
There is a further generalization to the notion of "unstable" used in Schweinberger (2011) that deals with replication. One can conceivably imagine replications of the data $\boldsymbol X_1, \dots, \boldsymbol X_n$, where $X_i \in \mathbb{R}^N, i = 1, \dots, n$ such that there are now $n*N$ random variables in the model. However, the definition of S-unstable and propositions \ref{prop:instab}-\ref{prop:degen} still hold if the replications are independent and identically distributed. The reason for this is that for $n$ observations, $\left(\max\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)\right)^n$ is the largest probability possible under the model and similarly $\left(\min\limits_{\boldsymbol x \in  \mathcal{X}}P_{\boldsymbol \theta_N}(\boldsymbol x)\right)^n$ is the smallest. Thus, 
\begin{align*}
\frac{\text{extremal log-probability ratio}}{\text{\# random variables in the model}} &\equiv \frac{1}{n*N}\log \left[\frac{\left\{\max\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)\right\}^n}{\left\{\min\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta}(\boldsymbol x)\right\}^n}\right] \\
&= \frac{1}{N} \log \left[\frac{\max\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)}{\min\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta}(\boldsymbol x)}\right]\\
&= \frac{1}{N} \text{ELPR}(\boldsymbol \theta_N).
\end{align*}
Meaning, the definition of an S-unstable DEF model is invariant to replication.
\end{remark}

# Implications

For a large class of models that cover a broad range of applications (including "deep learning"), we have developed a formal definition and elucidated multiple consequences of instability. We have shown for DEF models that instability manifests through small changes in data leading to overly-sensitive changes in probability as well as the potential to place all probability on a small pieace of the sample space. Models that fall within the definition of a DEF model should be used with caution to ensure that the effects of instability are not experienced.

# References