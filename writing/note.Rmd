---
title: "A note on the instability and degeneracy of deep learning models"
author: "Andee Kaplan, Daniel Nordman, and Stephen Vardeman"
date: "October, 2016"
linestretch: 2
output: 
  pdf_document:
    fig_caption: yes
    includes:
      in_header: latex/header.tex
    number_sections: yes
bibliography: references/refs.bib
abstract: |
  We introduce *Finitely Supported Finite Sequence*, or FSFS models, a broad class of discrete space models that are defined in relation to how a probability model is built to incorporate more variables. These models are defined to be in discrete finite space with model support equal the sample space and with a sequence of expanding model parameter vectors that potentially grow size as the dimension of the data become larger. In this paper, we present results that define the notion of model instability as well as near-degeneracy within the framework of FSFS models in addition to demonstrating example models of the FSFS class.
---

# Introduction

A probability model exhibits instability if small changes in a data outcome result in large, perhaps unanticipated changes in probability. As data structures become larger and more complex, there is increasing interest in identifying instability in potential model specifications for such data (e.g., models for
random graphs or networks). We consider the problem of  quantifying instability for general probability models defined on sequences of observations, where each sequence of a given length $N$ has a finite number of possible outcomes. As the number of available observations increases, a sequence of probability models results to accommodate the data of expanding dimension.

Model instability is formally shown to occur when a certain log-probability ratio under such models grows faster than the length $n$ of the data sequence. In this case, a one component change in the data sequence can provably shift probability
by orders of magnitude. Also, as the measure of instability becomes more extreme, the resulting probability models are shown  to slip into degeneracy, thereby placing all probability
on arbitrarily small portions of the sample space. These results on instability apply to large classes of exponential models commonly used in random graphs, network analysis, and machine learning contexts.

In this paper, we consider the behavior of probability models built to incorporate a sequence of discrete observations with an arbitrarily specified length $N$. Let $(X_1, \dots X_N)$ denote a set of sequence of discrete random variables with a finite sample space, $\mathcal{X}^N$. That is, $\mathcal{X}$ represents a finite set of potential outcomes for the variable $X_i$, and the data sequence $(X_1,\ldots,X_N)$ assumes values in the $N$-fold product space $\mathcal{X}^N$. For any $N$, let $P_{\theta_N}$ denote a probability model on $\mathcal{X}^N$, under which $P_{\boldsymbol \theta_N}(x_1, \dots, x_N) > 0$ is the probability of the data outcome $(x_1, \dots, x_N) \in \mathcal{X}^N$. We assume that the model support of $P_{\boldsymbol \theta_N}$ is equal to the sample space $\mathcal{X}^N$. This framework produces a series $P_{\theta_N}$ of probability models, indexed by a generic sequence of parameters $\theta_N$, to describe data of any given length $N \geq 1$. The size and structure of such parameters may be generally specified, and one may intuitvely consider Euclidean $\theta_N \in \mathbb{R}^{q(N)}$ for some arbitrary integer-valued function $q(\cdot) \geq 1$. We will call this class *Finitely Supported Finite Sequence (FSFS) models*.

Section \ref{examples} provides several examples of FSFS models commonly used in graph/network analysis and machine learning (i.e., deep learning models) among other application areas. Section \ref{instability-results} establishes formal results regarding the model propriety of FSFS models. These results help to measure, and qualify the effects of, model instability that can arise when certain maximal probabilities in such models are too extreme relevant to the length $N$ of the data sample. **TODO: Somewhere here or in Section 3 we need a brief literature review connecting instablity here to previous work (e.g., Schweinberger) and related notions of degeneracy (e.g., Handcock)**. Lastly, Section \ref{implications} emphasizes the implications of our model propriety results.

# Examples

Note many model families can fall under the umbrella of FSFS models. In this section, we present three specific examples of FSFS models, including models with deep architectures.

## Discrete exponential family models

Let $X_1, \dots, X_N$ be discrete random variables with sample space $\mathcal{X}^N$, where $|\mathcal{X}| < \infty$. Consider discrete exponential family model with probability mass function of the form
$$
p_{\boldsymbol \lambda}(\boldsymbol x) = \exp\left[\boldsymbol\eta^T(\boldsymbol \lambda) \boldsymbol g(\boldsymbol x) - \psi(\boldsymbol \lambda)\right], \qquad \boldsymbol x \in \mathcal{X}^N.
$$ 
where $\boldsymbol \lambda \in \Lambda \subset \mathbb{R}^{q(N)}$, $\boldsymbol \eta : \mathbb{R}^q \mapsto \mathbb{R}^L$ is a vector of natural parameters, $\boldsymbol g : \mathcal{X}^N \mapsto \mathbb{R}^L$ is a vector of sufficient statistics, 
$$
\psi(\boldsymbol \lambda) = \log \sum\limits_{\boldsymbol x \in \mathcal{X}^N}\exp\left[\boldsymbol \eta^T(\boldsymbol \lambda) \boldsymbol g(\boldsymbol x) \right], \qquad \boldsymbol \lambda \in \Lambda
$$
is the normalizing function, and $\Lambda = \{\boldsymbol \lambda \in \mathbb{R}^k : \psi(\boldsymbol \lambda) < \infty, k \le q(N) \}$ is the parameter space. 

By above definition, $X_1, \dots, X_N$ are discrete with a finite sample space. Let $\boldsymbol \theta = (\lambda_1, \dots, \lambda_k, 0, \dots, 0)$, where the first $k$ elements are equal to the $k$ elements of $\boldsymbol \lambda$ and for $i = k + 1, \dots, q(N)$, $\theta_i = 0$. Further, because $p_{\boldsymbol \lambda}(\boldsymbol x) > 0$ for all $\boldsymbol x \in \mathcal{X}^N$ by properties of the exponential function, the discrete exponential family models defined in this section are a special case of the FSFS model. **TODO: need to briefly add where such exponential models arise (e.g., spatial data, random graphs, network data) to indicate wide-spread applications**

## Restricted Boltzmann machines

A restricted Boltzmann machine (RBM) is an undirected graphical model specified for discrete or continuous random variables, binary variables being most common [@smolensky1986information]. Consider the binary case of an RBM only. A RBM architecture has two layers, hidden ($\mathcal{H}$) and visible ($\mathcal{V}$), with conditional independence within each layer. Let $\boldsymbol X = \{H_1, \dots, H_H, V_1,\dots, V_V\}$ be random variables corresponding to the hidden and visible variables in an RBM. We will consider the sample space $\mathcal{X} = \{-1,1\}$. The RBM is then has the joint probability mass function
$$
p_{\boldsymbol \theta} (\boldsymbol x) = \exp\left[ \boldsymbol \alpha^T \boldsymbol h + \boldsymbol \beta^T \boldsymbol v + \boldsymbol h^T \Gamma \boldsymbol v - \psi(\boldsymbol \theta)\right], \qquad \boldsymbol x = (\boldsymbol h, \boldsymbol v) \in \mathcal{X}^{H+V}
$$
where 
$$
\psi(\boldsymbol \theta) = \log \sum_{\boldsymbol x \in \mathcal{X}^{H + V}} \exp\left[ \boldsymbol \alpha^T \boldsymbol h + \boldsymbol \beta^T \boldsymbol v + \boldsymbol h^T \Gamma \boldsymbol v\right], \qquad \boldsymbol \theta \in \Theta
$$ 
is the normalizing function, $\boldsymbol \alpha \in \mathbb{R}^H$, $\boldsymbol \beta \in \mathbb{R}^V$, and $\Gamma$ is a matrix with dimension $H \times V$ are the parameters in the model. Let $\boldsymbol \theta = (\boldsymbol \alpha, \boldsymbol \beta,\Gamma) \in \Theta \subset \mathbb{R}^{H + V + H*V}$ denote the combined vector of parameters.

The probability mass function for the visible random variables $V_1, \dots, V_V$ follows from this joint specification:
$$
p_{\boldsymbol \theta} (\boldsymbol v) = \sum\limits_{\boldsymbol h \in \mathcal{X}^H} p_{\boldsymbol \theta} (\boldsymbol x), \qquad \boldsymbol v \in \mathcal{X}^V
$$

By definition, $V_1, \dots, V_V$ are discrete with a finite sample space. The vector of model parameters $\boldsymbol \theta$, of size $q(V) \equiv V + H + H*V$, is expanding in size as a function of the dimension of $V_1, \dots, V_V$. Additionally, $p_{\boldsymbol \theta}(\boldsymbol v) > 0$ for all $\boldsymbol v \in \mathcal{X}^V$ by properties of the exponential function. Thus, the visible RBM model specification is an example of a FSFS model. This example also indicates that models formed by marginalizing a base FSFS model (e.g., exponential family) is again a FSFS model class.

## Deep learning

Consider two models with "deep architechture" that contain multiple hidden (or latent) layers in addition to a visible layer of data, a deep Boltzmann machine [@salakhutdinov2009deep] and a deep belief network [@hinton2006fast]. Let $M$ denote the number of hidden layers included in the model and $H_1, \dots, H_M$ the number of hidden variables within each hidden layer. Then let $\boldsymbol X = \{H^{(1)}_1, \dots, H^{(1)}_{H_1}, \dots, H^{(M)}_1, \dots, H^{(M)}_{H_M}, V_1,\dots, V_V\}$ be random variables corresponding to the hidden and visible variables in a deep probabilitic model. We will consider the sample space $\mathcal{X} = \{-1,1\}$.

**Deep Boltzmann machine (DBM).** The DBM class of models maintains conditional independence within all layers in the model by stacking RBM models and maintaining the restriction of only allowing conditional dependence between layers. The joint probability mass function for a DBM is
$$
p_{\boldsymbol \theta} (\boldsymbol x) = \exp\left[ \sum\limits_{i = 1}^M\boldsymbol \alpha^{(i)T} \boldsymbol h^{(i)} + \boldsymbol \beta^T \boldsymbol v + \boldsymbol h^{(1)T} \Gamma^{(0)} \boldsymbol v + \sum\limits_{i = 1}^{M - 1} \boldsymbol h^{(i)T} \Gamma^{(i)} \boldsymbol h^{(i + 1)} - \psi(\boldsymbol \theta) \right], 
$$
where $\boldsymbol x = (\boldsymbol h^{(1)}, \dots, \boldsymbol h^{(M)}, \boldsymbol v) \in \mathcal{X}^{H_1 + \cdots + H_M +V}$ and
$$
\psi(\boldsymbol \theta) = \log \sum_{\boldsymbol x \in \mathcal{X}^{H_1 + \cdots + H_M + V}} \exp\left[ \sum\limits_{i = 1}^M\boldsymbol \alpha^{(i)T} \boldsymbol h^{(i)} + \boldsymbol \beta^T \boldsymbol v + \boldsymbol h^{(1)T} \Gamma^{(0)} \boldsymbol v + \sum\limits_{i = 1}^{M - 1} \boldsymbol h^{(i)T} \Gamma^{(i)} \boldsymbol h^{(i + 1)}\right], \qquad \boldsymbol \theta \in \Theta
$$ 
is the normalizing function, $\boldsymbol \beta \in \mathbb{R}^V$, $\boldsymbol \alpha^{(i)} \in \mathbb{R}^{H_i}$ for $i = 1, \dots, M$, $\Gamma^{(0)}$ is a matrix with dimension $H_1 \times V$, and $\Gamma^{(i)}$ is a matrix with dimension $H_i \times H_{i + 1}$ for $i = 1, \dots, M-1$ are the parameters in the model. Let $\boldsymbol \theta = (\boldsymbol \alpha^{(1)}, \dots, \boldsymbol \alpha^{(M)}, \boldsymbol \beta,\Gamma^{(0)}, \dots, \Gamma^{(M - 1)}) \in \Theta \subset \mathbb{R}^{H_1 + \cdots + H_M + V + H_1*V + H_1 *H_{2} + \cdots + H_{M-1}*H_M}$ denote the combined vector of parameters.

The probability mass function for the visible random variables $V_1, \dots, V_V$ follows from this joint specification:
$$
p_{\boldsymbol \theta} (\boldsymbol v) = \sum\limits_{(\boldsymbol h^{(1)}, \dots, \boldsymbol h^{(M)}) \in \mathcal{X}^{H_1 + \cdots + H_M}} p_{\boldsymbol \theta} (\boldsymbol x), \qquad \boldsymbol v \in \mathcal{X}^V
$$

By definition, $V_1, \dots, V_V$ are discrete with a finite sample space. The vector of model parameters $\boldsymbol \theta$, of size $q(V) \equiv H_1 + \cdots + H_M + V + H_1*V + H_1 *H_{2} + \cdots + H_{M-1}*H_M$, is expanding in size as a function of the dimension of $V_1, \dots, V_V$. Additionally, $p_{\boldsymbol \theta}(\boldsymbol v) > 0$ for all $\boldsymbol v \in \mathcal{X}^V$ by properties of the exponential function. Thus, the visible RBM model specification is an example of a FSFS model.

**Deep belief network (DBN).** A DBN is similar to a DBM in that there are multiple layers of latent random variables stacked in a deep architecture with no conditional dependence between layers. The difference between the two models is that all but the last stacked model are Bayesian networks, rather than RBMs. Thus for $V_1, \dots, V_V$ discrete with finite sample space and support equal to the sample space $\mathcal{X}^V$, a DBN is also a FSFS model if the number of components in the parameter vector is dependent on the dimension of the data, $V$. Commonly, as in a logistic belief net [@neal1992connectionist], a "weight" parameter is placed on each interaction between visibles, $V_1, \dots, V_V$ and the first layer of latent variables, $H^{(1)}_1, \dots, H^{(1)}_{H_1}$, satisfying the definition of a FSFS model.

# Instability results

To measure "instability" in a FSFS model, it is useful to imagine the behavior of a data model sequence $P_{\theta_N}$ specified to incorporate more and more observations. A relevant quantity to this end is a (scaled) extremal log-probability ratio given by 
\begin{align}
\frac{1}{N} \log \left[\frac{\max\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(x_1, \dots, x_N)}{\min\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta}(x_1, \dots, x_N)}\right] \equiv \frac{1}{N} \text{ELPR}_N(\boldsymbol \theta_N). \label{eqn:elpr}
\end{align}


The main idea is that, in formulating a FSFS model for potentially increasing numbers of variables (i.e., $N \rightarrow \infty$), the ratio (\ref{eqn:elpr}) should remain bounded, entailing that the largest probability possible under $P_{\theta_N}$ should maintain a magnitude within the smallest probability allowed under the same model. Specifically, the log of the fold change should grow at mostly linearly with the sample size $N$. This leads to the following definition:

\begin{defn}[S-unstable FSFS]
Let $\boldsymbol \theta_N \in \mathbb{R}^{q(N)}$ be a sequence of FSFS model parameters where the size of the model $q(N)$ is a function of the number of random variables $N$. A FSFS model formulation is \emph{Schweinberger-unstable} or \emph{S-unstable} if, as the number of variables increase ($N \rightarrow \infty$), it holds that 
\begin{align*}
\lim\limits_{N \rightarrow \infty} \frac{1}{N} \text{ELPR}(\boldsymbol \theta_N) \equiv \lim\limits_{N \rightarrow \infty} \frac{1}{N} \log \left[\frac{\max\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(x_1, \dots, x_N)}{\min\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta}(x_1, \dots, x_N)}\right] = \infty.
\end{align*}
\end{defn}
In other words, given any $C > 0$, there exists an integer $N_C > 0$ so that $\frac{1}{N}\text{ELPR}_N(\boldsymbol \theta_N) > C$ for all $N \ge N_C$.

Note that this definition of *S-unstable* is a generalization or reinterpretation of "unstable" used in @schweinberger2011instability by allowing nonexponential family models and an increasing number of parameters. While this definition appears different, it does match that in @schweinberger2011instability for the special exponential models considered there, see Section \ref{discrete-exponential-family-models} for details.

S-unstable FSFS models are undesirable for several reasons. One is that small changes in data can lead to overly-sensitive changes in probability. Consider, for example, 
$$
\Delta(\boldsymbol \theta_N) \equiv \max \left\{\log \frac{P_{\boldsymbol \theta_N}(x_1, \dots, x_N)}{P_{\boldsymbol \theta_N}(x_1^*, \dots, x_N^*)} : (x_1, \dots, x_N) \& (x_1^*, \dots, x_N^*) \in \mathcal{X}^N \text{ differ by exactly one component}\right\},
$$
the biggest log-probability ratio for a one component change in data outcomes at a FSFS parameter $\boldsymbol \theta_N$. We then have the following result.

\begin{prop}
\label{prop:instab}
Let $C > 0$ and let $\text{ELPR}(\boldsymbol \theta_N)$ be as in (\ref{eqn:elpr}) for an integer $N \ge 1$. If $$\frac{1}{N}\text{ELPR}_N(\boldsymbol \theta_N) > C,$$ 
then 
$$\Delta_N(\boldsymbol \theta_N) > C.$$
\end{prop}

Again, if the probability ratio (\ref{eqn:elpr}) is too large, then the FSFS model will exhibit large probability shifts for very small changes in the data configuration, which exemplifies the intuitive notation of instability..

Additionally, S-unstable FSFS models are connected to degenerate models, where *degeneracy* implies placing all probability on a small portion of the sample space. That is, instability in FSFS models, as measured by (\ref{eqn:elpr}), is a spectrum and Proposition \ref{prop:instab} indicates increasing levels of one undesirable type of model behavior as (\ref{eqn:elpr}) increases. Furthermore, as (\ref{eqn:elpr}) increases, FSFS models will slide into full degeneracy as Proposition \ref{prop:degen} next shows. Define a modal set 
$$
M_{\epsilon, \boldsymbol \theta_N} \equiv \left\{\boldsymbol x \in \mathcal{X}^N: \log P_{\boldsymbol \theta_N}(\boldsymbol x) > (1-\epsilon)\max\limits_{\boldsymbol x^* \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x^*) + \epsilon\min\limits_{\boldsymbol x^* \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x^*) \right\}
$$
of possible outcomes, for a given $0 < \epsilon < 1$ and along a sequence $\boldsymbol \theta_N$, $N \ge 1$, of FSFS parameters.

\begin{prop}
\label{prop:degen}
For an unstable FSFS model, and for any given $0 < \epsilon < 1$, $P_{\boldsymbol \theta_N}\left((x_1, \dots, x_N) \in M_{\epsilon, \boldsymbol \theta_N}\right) \rightarrow 1$ holds as $N \rightarrow \infty$.
\end{prop}

In other words, as a consequence of S-unstable models, all probability in the FSFS model formulation with a large number of random variables will stack up on mode sets or simply those few outcomes with the highest probability.

**Proof of Proposition \ref{prop:instab}.** We prove the contrapositive. Suppose that $\Delta(\boldsymbol \theta_N) \le C$ holds for some $C > 0$. Under the FSFS model, $P_{\boldsymbol \theta_N}(\boldsymbol x) > 0$ holds for each outcome $\boldsymbol x \in \mathcal{X}^N$. Let $\boldsymbol x_{min} \equiv \argmin\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)$ and $\boldsymbol x_{max} \equiv \argmax\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)$. Note there exists a sequence $\boldsymbol x_{min} \equiv \boldsymbol x_0, \boldsymbol x_1, \dots, \boldsymbol x_k \equiv \boldsymbol x_{max}$ in $\mathcal{X}^N$ of component-wise switches to move from $\boldsymbol x_{min}$ to $\boldsymbol x_{max}$ in the sample space (i.e. $\boldsymbol x_i, \boldsymbol x_{i + 1} \in \mathcal{X}^N$ differ by exactly $1$ component for $i = 0, \dots, k$) for some integer $k \in \{0, 1, \dots, N\}$. Then, if $k > 0$, it follows that
\begin{align*}
\log\left[\frac{P_{\boldsymbol \theta_N}(\boldsymbol x_{max})}{P_{\boldsymbol \theta_N}(\boldsymbol x_{min})}\right] &= \left|\sum\limits_{i = 1}^k\log\left(\frac{P_{\boldsymbol \theta_N}(\boldsymbol x_i)}{P_{\boldsymbol \theta_N}(\boldsymbol x_{i-1})}\right)\right| \\
& \le \sum\limits_{i = 1}^k\left|\log\left(\frac{P_{\boldsymbol \theta_N}(\boldsymbol x_i)}{P_{\boldsymbol \theta}(\boldsymbol x_{i-1})}\right)\right| \\
& \le k \Delta_N(\boldsymbol \theta_N) \le NC
\end{align*}
using $k \le N$ and $\Delta(\boldsymbol \theta_N) \le C$. If $k = 0$, then $\boldsymbol x_{max} = \boldsymbol x_{min}$ and the same bound above holds.$\Box$

**Proof of Proposition \ref{prop:degen}.** Define $\boldsymbol x_{max}$ and $\boldsymbol x_{min}$ as in the proof of Proposition \ref{prop:instab}. Fix $0 < \epsilon < 1$ Then, $\boldsymbol x_{max} \in M_{\epsilon, \boldsymbol \theta_N}$, so $P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N}) \ge P_{\boldsymbol \theta_N}(\boldsymbol x_{max})$. If $\boldsymbol x \in \mathcal{X}^N \setminus M_{\epsilon, \boldsymbol \theta_N}$, then by definition $P_{\boldsymbol \theta_N}(\boldsymbol x) \le [P_{\boldsymbol \theta_N}(\boldsymbol x_{max})]^{1-\epsilon}[P_{\boldsymbol \theta_N}(\boldsymbol x_{min})]^{\epsilon}$ holds so that
\begin{align*}
1-P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N})
& = \sum\limits_{\boldsymbol x \in \mathcal{X}^N \setminus M_{\epsilon, \boldsymbol \theta_N}}P_{\boldsymbol \theta_N}(\boldsymbol x) \\
& \le (2^N)[P_{\boldsymbol \theta_N}(\boldsymbol x_{max})]^{1-\epsilon}[P_{\boldsymbol \theta_N}(\boldsymbol x_{min})]^{\epsilon}
\end{align*}
Then, it follows that
\begin{align*}
\frac{1}{N}\log\left[\frac{P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N})}{1-P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N})}\right] & \ge \frac{1}{N} \log\left[\frac{P_{\boldsymbol\theta_N}(\boldsymbol x_{max})}{(2^V)[P_{\boldsymbol \theta_N}(\boldsymbol x_{max})]^{1-\epsilon}[P_{\boldsymbol \theta_N}(\boldsymbol x_{min})]^{\epsilon}}\right] \\
&= \frac{\epsilon}{N} \log\left[\frac{P_{\boldsymbol \theta_N}(\boldsymbol x_{max})}{P_{\boldsymbol \theta_N}(\boldsymbol x_{min})}\right] - \log 2 \rightarrow \infty
\end{align*}
as $N \rightarrow \infty$ by the definition of an unstable FSFS model. Consequently, $P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N}) \rightarrow 1$ as $N \rightarrow \infty$ as claimed.$\Box$

\begin{remark}
There is a further generalization to the notion of "unstable" used in Schweinberger (2011) that deals with replication. One can conceivably imagine replications of the data $\boldsymbol X_1, \dots, \boldsymbol X_n$, where $X_i \in \mathbb{R}^N, i = 1, \dots, n$ such that there are now $n*N$ random variables in the model. However, the definition of S-unstable and propositions \ref{prop:instab}-\ref{prop:degen} still hold if the replications are independent and identically distributed. The reason for this is that for $n$ observations, $\left(\max\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)\right)^n$ is the largest probability possible under the model and similarly $\left(\min\limits_{\boldsymbol x \in \mathcal{X}}P_{\boldsymbol \theta_N}(\boldsymbol x)\right)^n$ is the smallest. Thus, 
\begin{align*}
\frac{\text{extremal log-probability ratio}}{\text{\# random variables in the model}} &\equiv \frac{1}{n*N}\log \left[\frac{\left\{\max\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)\right\}^n}{\left\{\min\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta}(\boldsymbol x)\right\}^n}\right] \\
&= \frac{1}{N} \log \left[\frac{\max\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)}{\min\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta}(\boldsymbol x)}\right]\\
&= \frac{1}{N} \text{ELPR}(\boldsymbol \theta_N).
\end{align*}
Meaning, the definition of an S-unstable FSFS model is invariant to replication.
\end{remark}

# Implications

For a large class of models that cover a broad range of applications (including "deep learning"), we have developed a formal definition and elucidated multiple consequences of instability. We have shown for FSFS models that instability manifests through small changes in data leading to overly-sensitive changes in probability as well as the potential to place all probability on a small pieace of the sample space. Models that fall within the definition of a FSFS model should be used with caution to ensure that the effects of instability are not experienced.

# References