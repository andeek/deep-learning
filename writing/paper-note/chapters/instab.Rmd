
# Introduction

We consider the behavior, and the potential impropriety, sequences of discrete probability models built to incorporate observations of increasing sample size $N$.  Interest is in identifying  instability in such models, which is roughly characterized by probabilities with extreme sensitivity to small changes in data configuration.  The concept of instability was introduced in the field of statistical physics (i.e., point processes) by @ruelle1999statistical and then further extended by Schweinberger @schweinberger2011instability for a family of exponential models. At issue, models exhibiting instability are typically undesirable as these tend to provide poor representations of data or data-generation. As an example, such models can include near-degenerate distributions that assign all essential probability mass to only a subset of an overall sample space. The latter issue in connection to degeneracy has been recognized as concern in that dominant model outcomes  may not resemble observed data [cf. @handcock2003assessing]. As a compounding issue, model instability often has direct negative impacts for statistical inference and computations based on likelihood functions. Namely, volatilities in probability structure can potentially hamper the numerical evaluations required for maximum likelihood estimation as well as other model-based simulations via Markov Chain Monte Carlo (MCMC). These reasons motivate our general study of instability for a broad class of probability models, described next.

In the model framework, let $\boldsymbol X_N = (X_1, \dots, X_N)$ denote a collection of discrete random variables with a finite sample space, $\mathcal{X}^N$, represented as some $N$-fold Cartesian  product. That is, $\mathcal{X}$ with $|\mathcal{X}| < \infty$ denotes the set of potential outcomes for each single variable $X_i$, so that the product space $\mathcal{X}^N$ corresponds to values for the variables $\boldsymbol X_N=(X_1,\ldots,X_N)$. For each $N$, let $P_{\boldsymbol \theta_N}$ denote a probability model on $\mathcal{X}^N$, under which $P_{\boldsymbol \theta_N}(x_1, \dots, x_N) > 0$ is the probability of the data outcome $(x_1, \dots, x_N) \in \mathcal{X}^N$. In this, we assume that the model support of $P_{\boldsymbol \theta_N}$ is the sample space $\mathcal{X}^N$. This framework produces probability models $P_{\boldsymbol \theta_N}$, indexed by a generic sequence of parameters $\boldsymbol \theta_N$, to describe data $\boldsymbol X_N$ of any given sample size $N \geq 1$. For simplicity, we will refer to this distributional class as *Finite Outcome Everywhere Supported (FOES)* models in the following. The dimension and structure of such parameters are generic, without restriction, though natural cases will be seen to include those where $\boldsymbol \theta_N \in \mathbb{R}^{q(N)}$ for some arbitrary integer-valued function $q(\cdot) \geq 1$.

Section \@ref(examples) provides some examples of FOES models encountered in graph/network analysis and machine learning (i.e., deep learning models). These are used as references for later illustrations. Section \@ref(instability-results) then  establishes several formal results for  FOES models with regard to instability.  @schweinberger2011instability originally developed instability results framed for, or specific to, a certain class of discrete exponential models. For similar exponential models with random networks, @handcock2003assessing studied model degeneracy, where a probability model places near complete mass on  modes and may thereby narrow the feasible model outcomes. As findings here and from @schweinberger2011instability suggest, model instability and degeneracy may also be related by viewing degeneracy as an extreme, or limiting form, of instability. Our main results establish a broad characterization of model instability, appropriate across the whole FOES model class, that incorporates results of @schweinberger2011instability as a special case. We prescribe a general and simple condition  for identifying instability in a FOES model sequence, which  quantifies whether certain maximal probabilities in a FOES model  are too extreme relative to the sample size $N$. When fulfilled, the probability structure of a FOES model is shown to exhibit extreme sensitivity, with probability assignments possessing extreme peaks and troughs across nearly identical outcomes. As the measure of model instability increases, probabilities from an unstable FOES model additionally increase in volatility and provably slide into degeneracy. Section \@ref(implications) then emphasizes the implications of such model instability, showing that such impropriety can be expected to numerically hinder maximum likelihood estimation and   MCMC-based simulations. As one potential remedy, suggestions are given or constraining model parameterizations and avoid the most problematic regions of the parameter space. Proofs of the main results appear in Appendix \@ref(appendix-instab).

# Examples

Many model families fall under the umbrella of FOES models. For illustration, this section presents three specific examples of FOES models, including models with deep architectures.

## Discrete exponential family models

For random variables $\boldsymbol X \equiv\boldsymbol X_N= (X_1, \dots, X_N)$ with sample space $\mathcal{X}^N$, $|\mathcal{X}| < \infty$, consider an exponential family model for $\boldsymbol X$ with probability mass function given by
\begin{equation}
(\#eq:expo)
p_{N, \boldsymbol \theta}(\boldsymbol x) = \exp\left[\boldsymbol\eta^T(\boldsymbol \theta) \boldsymbol g_N(\boldsymbol x) - \psi(\boldsymbol \theta)\right], \qquad \boldsymbol x \in \mathcal{X}^N,
\end{equation}
depending on parameter vector $\boldsymbol \theta \in \Theta_N \subset \mathbb{R}^{k}$ and natural parameter function $\boldsymbol \eta : \mathbb{R}^k \mapsto \mathbb{R}^L$ with fixed positive integers $k$ and $L$ denoting their dimensions. Above, $\boldsymbol g_N : \mathcal{X}^N \mapsto \mathbb{R}^L$ is a vector of sufficient statistics, while
$$
\psi(\boldsymbol \theta) = \log \sum\limits_{\boldsymbol x \in \mathcal{X}^N}\exp\left[\boldsymbol \eta^T(\boldsymbol \theta) \boldsymbol g_N(\boldsymbol x) \right], \qquad \boldsymbol \theta \in \Theta_N\equiv \{\boldsymbol \theta \in \mathbb{R}^k : \psi(\boldsymbol \theta) < \infty \},
$$
denotes the normalizing function with parameter space $\Theta_N$. The natural parameter function $\eta (\boldsymbol \theta)$ has a linear form (i.e., $\eta (\boldsymbol \theta)= \bm{A} \boldsymbol \theta$ for a given $L \times k$ matrix $\bm{A}$) in many common model formulaitons, though may also be nonlinear (e.g., curved exponential families). In the linear case, $\eta (\boldsymbol \theta) = \boldsymbol \theta$ may be generally assumed in the exponential parameterization with a minor modification to the definition of sufficient statistics $\boldsymbol g_N(\boldsymbol x)$.

Such discrete exponential family models are special cases of the FOES models, as seen by  defining $P_{\boldsymbol \theta_N}(\boldsymbol x)\equiv p_{N,\boldsymbol \theta_N}(\boldsymbol x)> 0$,  $\boldsymbol x \in \mathcal{X}^N$, based on \@ref(eq:expo) and a parameter sequence $\boldsymbol \theta_N \in \Theta_N \subset \mathbb{R}^k$. For example, if observations $\boldsymbol X = (X_1,\ldots,X_N)$ correspond to $N$ independent and identically distributed Bernoulli random variables, each indicating a binary $0$-$1$ outcome,  the resulting probabilities have exponential form \@ref(eq:expo)  given by
\begin{equation}
(\#eq:mod1)
P_{\boldsymbol \theta_N}(\boldsymbol x) \propto
 \exp\left[\boldsymbol \theta_N \sum_{i=1}^N x_i\right], \qquad \boldsymbol x=(x_1,\ldots,x_N) \in\{0,1\}^N, 
 \end{equation} 
 with sufficient statistic $\boldsymbol g_N(\boldsymbol x)\equiv \sum_{i=1}^N  x_i$ and "log odds ratio" parameter $\boldsymbol \theta_N \equiv  \log[ P_{\boldsymbol \theta_N}(X_i=1)/P_{\boldsymbol \theta_N}(X_i=0) ] \in \mathbb{R}$. More generally, supposing $\boldsymbol X =(X_1,\ldots,X_N)$ represent $N$ independent trials, each assuming an outcome $\{1,\ldots,k\}$ among $k$ possibilities (e.g., a die roll), a multinomial distribution is given by
\begin{equation}
(\#eq:mod11)
P_{\boldsymbol \theta_N}(\boldsymbol x) \propto  \exp\left[  \boldsymbol \theta_{N}^T g_N(\boldsymbol x)   \right] =
\exp\left[ \sum_{j=1}^k {\theta_{j,N}} \sum_{i=1}^N \mathbb{I}(x_i=j) \right], \qquad \boldsymbol x  \in\{1,\ldots,k\}^N, 
\end{equation}
with sufficient statistic $\boldsymbol g_N(\boldsymbol x)$ involving a count $\sum_{i=1}^N \mathbb{I}(x_i=j)$ for each outcome $j \in \{1,\ldots,k\}$, where $\mathbb{I}(\cdot)$ denotes the indicator function, and parameters $\boldsymbol \theta_N=(\theta_{1,N},\ldots,\theta_{k,N})\in\mathbb{R}^k$ defining log-probability ratios $\theta_{i,N}-\theta_{j,N} =\log [P_{\boldsymbol \theta_N}(X_1=i)/P_{\boldsymbol \theta_N}(X_1=j)]$. In addition to such standard models for discrete independent data,
exponential models of FOES type commonly arise with dependent spatial data [@besag1974spatial] and network/relational data [@wasserman1994social; @handcock2003assessing]. For a random graph  or network  with, say, $n$ nodes,  consider $N={n \choose 2}$ random edges where the $i$th edge is associated with a pair of nodes $s_i \equiv \{v_i,u_i\}$  and a binary variable $X_i\in\{0,1\}$ indicating   presence/absence of an edge among the node pair $s_i$, $i=1,\ldots,N$. Here the length $N$ of the edge variable sequence $\boldsymbol X = (X_1,\ldots,X_N)$ increases as a function of node number $n$ and corresponding exponential models often incorporate graph topographical features derived from $\boldsymbol X$. As an example, consider a   graph model of exponential/FOES form prescribed by
\begin{equation}
(\#eq:mod2)
P_{\boldsymbol \theta_N}(\boldsymbol x) \propto
 \exp\left[\sum_{j=1}^3 \theta_{j,N} g_{j,N}(\boldsymbol x)\right], \quad\qquad \boldsymbol x=(x_1,\ldots,x_N)  \in\{0,1\}^N,
\end{equation}
$$
  g_{1,N}(\boldsymbol x) \equiv \sum_{i=1}^N  x_i, \qquad\quad g_{2,N}(\boldsymbol x) \equiv \sum_{1 \leq i<j \leq N,\atop s_i \cap s_j \neq \emptyset}\!\!\!   x_i x_j, \qquad
  g_{3,N}(\boldsymbol x) \equiv \sum_{1 \leq i<j<\ell \leq N, \atop s_i \cap s_j \neq \emptyset,s_i \cap s_\ell \neq \emptyset, \\ s_j \cap s_\ell \neq \emptyset } \!\!\!\!\!\!\!\!  \!\!\!\!\!\!\!\!   x_i x_j x_\ell,
$$
involving the numbers of edges, 2-stars and triangles among an outcome $\boldsymbol x$ given by $g_{1,N}(\boldsymbol x)$, $g_{2,N}(\boldsymbol x)$ and $g_{3,N}(\boldsymbol x)$, respectively, along with $k=3$ real parameters $\boldsymbol \theta_N \equiv (\theta_{1,N},\theta_{2,N},\theta_{3,N})$. For this network model \@ref(eq:mod2) in particular, as well as for more general models of form \@ref(eq:expo),  @schweinberger2011instability considered instability in such exponential models with sequences of fixed parameters $\boldsymbol \theta_N = (\theta_1,\ldots,\theta_k)\in\mathbb{R}^k$, $N \geq 1$, of fixed dimension $k$.


For model sequences $P_{\boldsymbol \theta_N}(\boldsymbol x)\equiv p_{N,\boldsymbol \theta_N}(\boldsymbol x)$ of the exponential type \@ref(eq:expo), such as those in \@ref(eq:mod1)-\@ref(eq:mod2), note that the dimension $k$ of the parameter $\boldsymbol \theta_N\in\Theta \subset \mathbb{R}^k$ necessarily remains the same for all sample sizes $N \geq 1$ as the form of the natural parameter function $\eta(\cdot)$ in \@ref(eq:expo) and the number of sufficient statistics $\boldsymbol g_{N}(\boldsymbol x)$ do not depend on $N$. Consequently, $\boldsymbol \theta_N$ lies in a parameter space of fixed Euclidean dimension $k$. However, this aspect need not be true for other types of FOES models considered in Sections \@ref(restricted-boltzmann-machines) - \@ref(deep-learning), where instead the numbers of parameters and sufficient statistics commonly increase with the sample size $N$.

## Restricted Boltzmann machines

A restricted Boltzmann machine (RBM) is an undirected graphical model specified for discrete or continuous random variables, with binary variables being most common [cf. @smolensky1986information]. A RBM architecture has two layers, hidden ($\mathcal{H}$) and visible ($\mathcal{V}$), with conditional independence within each layer. Let $\boldsymbol X = (X_1,\ldots,X_N)$ denote the $N$ random variables for visibles with support $\mathcal{X}^N$ and $\boldsymbol H = (H_1,\ldots,H_{N_\mathcal{H}})$ denote the $N_\mathcal{H}$ random variables for hiddens with support $\mathcal{X}^{N_\mathcal{H}}$ where $\mathcal{X} = \{-1,1\}$. For parameters $\boldsymbol \theta_N^{\mathcal{H}} \in \mathbb{R}^{N_\mathcal{H}}$, $\boldsymbol \theta_N^{\mathcal{V}}\in \mathbb{R}^N$, and $\boldsymbol \theta_N^{\mathcal{HV}}$ as a real matrix with dimension $N_\mathcal{H} \times N$, the RBM model for $\tilde{\boldsymbol X}=(\boldsymbol X,\boldsymbol H)$  has the joint probability mass function
\begin{equation}
(\#eq:RBM1)
\tilde{P}_{\boldsymbol \theta_N} (\tilde{\boldsymbol x}) = \exp\left[ (\boldsymbol \theta_N^{\mathcal{H}})^T \boldsymbol h + \boldsymbol (\boldsymbol \theta_N^{\mathcal{V}})^T \boldsymbol x + \boldsymbol h^T  \boldsymbol\theta_N^{\mathcal{HV}} \boldsymbol x - \psi(\boldsymbol \theta_N)\right], \quad \tilde{\boldsymbol x} = (\boldsymbol x, \boldsymbol h) \in \{\pm 1\}^{N+N_\mathcal{H}}
\end{equation}
with normalizing function
$$
\psi(\boldsymbol \theta_N) = \log \sum_{\tilde{\boldsymbol x} \in \{\pm 1\}^{N+N_H} } \exp\left[ (\boldsymbol \theta_N^{\mathcal{H}})^T \boldsymbol h + \boldsymbol (\boldsymbol \theta_N^{\mathcal{V}})^T \boldsymbol x + \boldsymbol h^T  \boldsymbol\theta_N^{\mathcal{HV}} \boldsymbol x\right].
$$
Let $\boldsymbol \theta_N = (\boldsymbol \theta_N^{\mathcal{H}}, \boldsymbol \theta_N^{\mathcal{V}}, \boldsymbol\theta_N^{\mathcal{HV}} ) \in \Theta_N \equiv \mathbb{R}^{q(N)}$, with $q(N) = N + N_\mathcal{H} + N*N_\mathcal{H}$, denote the parameter vector for the RBM, as indexed by the number $N$ of visible random variables (which may differ from the actual lengths of these parameter vectors). The probability mass function for the visible variables $\boldsymbol X = (X_1, \dots, X_N)$ follows from marginalizing the joint specification to yield
\begin{equation}
(\#eq:RBM2)
P_{\boldsymbol \theta_N} (\boldsymbol x) = \sum\limits_{\boldsymbol h \in \{\pm 1\}^{N_{\mathcal{H}}}} \tilde{P}_{\boldsymbol \theta_N} (\boldsymbol x, \boldsymbol h), \qquad \boldsymbol x \in \{\pm 1\}^{N}\equiv \mathcal{X}^N.
\end{equation}
Here the baseline model \@ref(eq:RBM1) for hidden/visible variables is a linear exponential one in sufficient statistics $(\tilde{\boldsymbol X}, \boldsymbol X^T\boldsymbol H)$ using $\tilde{\boldsymbol X}=(\boldsymbol X,\boldsymbol H)$ from   \@ref(eq:RBM1), but the form differs from the previous exponential models in \@ref(eq:expo) in that the lengths of parameters $\boldsymbol \theta_N$ and statistics  $(\tilde{\boldsymbol X}, \boldsymbol X^T\boldsymbol H)$ increase to incorporate more visible variables. That is, in contrast to \@ref(eq:expo), the natural parameter function involved in the RBM model \@ref(eq:RBM1), as the identity mapping of the parameters $\boldsymbol \theta_N\in\mathbb{R}^{q(N)}$, naturally grows in  dimension  $q(N)\to \infty$ to accommodate visible variables $X_1, \dots, X_N$ of increasing sample size $N\to \infty$. Additionally, one may further arbitrarily choose the number $N_\mathcal{H}$ of hidden variables $\boldsymbol H$ in the joint RMB model \@ref(eq:RBM1) to define a marginal model \@ref(eq:RBM2) for the $N$ visible variables $\boldsymbol X$, and the number $N_\mathcal{H}$ of hiddens may also potentially   increase with $N$. Because $|\mathcal{X}| = 2$ and $P_{\boldsymbol \theta_N}(\boldsymbol x) > 0$ for all $\boldsymbol x \in \mathcal{X}^N$, the RBM  specification \@ref(eq:RBM2) for visibles $\boldsymbol$ corresponds to a FOES model, while the joint distribution \@ref(eq:RBM1) for $(\boldsymbol X, \boldsymbol H)$ is also a FOES model. As this example also indicates, any model formed by marginalizing a base FOES model class, such as the RBM joint specification \@ref(eq:RBM1), is again a FOES model.

## Deep learning

Consider two models with "deep architecture" that contain multiple hidden (or latent) layers in addition to a visible layer of data, namely a deep Boltzmann machine [@salakhutdinov2009deep] and a deep belief network @hinton2006fast]. Let $M$ denote the
number of hidden layers included in the model and let $N_{(H,1)}, \dots, N_{(H,M)}$ denote the numbers of hidden variables within each hidden layer. Then the random vector
$\tilde{\boldsymbol X} = \{H^{(1)}_1, \dots, H^{(1)}_{N_{(H,1)}}, \dots, H^{(M)}_1, \dots, H^{(M)}_{N_{(H,M)}}, \boldsymbol X\}$ collects both the hidden variables
$\{ H_{i}^{(j)} : i=1,\ldots, N_{(H,j)}, j=1,\ldots,M\}$ and visible variables $\boldsymbol X =(X_1,\ldots,X_N)$ in a deep probabilistic model. Each variable outcome will again lie in $\mathcal{X} = \{-1,1\}$.

**Deep Boltzmann machine (DBM).** The DBM class of models maintains conditional independence within all layers in the model by stacking RBM models and only allowing conditional dependence between neighboring layers. The joint probability mass function for a DBM is 
$$
\tilde{P}_{\boldsymbol \theta_N} ( \tilde{\boldsymbol x} ) = \exp\left[ \sum\limits_{i = 1}^M\boldsymbol \alpha^{(i)T} \boldsymbol h^{(i)} + \boldsymbol \beta^T \boldsymbol x + \boldsymbol h^{(1)T} \Gamma^{(0)} \boldsymbol x + \sum\limits_{i = 1}^{M - 1} \boldsymbol h^{(i)T} \Gamma^{(i)} \boldsymbol h^{(i + 1)} - \psi(\boldsymbol \theta_N) \right],
$$ 
for $\tilde{\boldsymbol x} = (\boldsymbol h^{(1)}, \dots, \boldsymbol h^{(M)}, \boldsymbol x) \in \mathcal{X}^{N_{(H,1)} + \cdots + N_{(H,M)} +N}$ where 
$$
\psi(\boldsymbol \theta_N) = \log \sum\limits_{\tilde{\boldsymbol x} \in \mathcal{X}^{N_{(H,1)} + \cdots + N_{(H,M)} +N}} \exp\left[ \sum\limits_{i = 1}^M\boldsymbol \alpha^{(i)T} \boldsymbol h^{(i)} + \boldsymbol \beta^T \boldsymbol x + \boldsymbol h^{(1)T} \Gamma^{(0)} \boldsymbol x + \sum\limits_{i = 1}^{M - 1} \boldsymbol h^{(i)T} \Gamma^{(i)} \boldsymbol h^{(i + 1)}\right],
$$ 
is the normalizing function for $\boldsymbol \theta_N = (\boldsymbol \alpha^{(1)}, \dots, \boldsymbol \alpha^{(M)}, \boldsymbol \beta,\Gamma^{(0)}, \dots, \Gamma^{(M - 1)}) \in \Theta_N \subset \mathbb{R}^{q(N)}$, consisting of model parameters $\boldsymbol \beta \in \mathbb{R}^N$, $\boldsymbol \alpha^{(i)} \in \mathbb{R}^{N_{(H,i)}}$, $i = 1, \dots, M$, along with a matrix $\Gamma^{(0)}$ of dimension $N_{(H,1)} \times N$, and matrices $\Gamma^{(i)}$ of dimension
$N_{(H,i)} \times N_{(H,i+1)}$ for $i = 1, \dots, M-1$. The combined parameter vector $\boldsymbol \theta_N$ has total length $q(N)= N_{(H,1)}+\cdots N_{(H,M)} + N + N_{(H,1)}*N+N_{H,2}*H_{(H,1)}+\cdots +N_{(H,M)}*H_{(H,M)-1}$. The probability mass function for the visible random variables $X_1, \dots, X_N$ follows from this joint specification as 
$$
P_{\boldsymbol \theta_N} (\boldsymbol x) = \sum\limits_{(\boldsymbol h^{(1)}, \dots, \boldsymbol h^{(M)}) \in \mathcal{X}^{N_{(H,1)} + \cdots + N_{(H,M)}}} \tilde{P}_{\boldsymbol \theta_N} (\boldsymbol h^{(1)}, \dots, \boldsymbol h^{(M)}, \boldsymbol x) , \qquad \boldsymbol x \in \mathcal{X}^N.
$$ 
Again like the RBM case, the DBM model specification is an example of a FOES model.

**Deep belief network (DBN).** A DBN resembles a DBM in that there are multiple layers of latent random variables stacked in a deep architecture with no conditional dependence between layers. The difference between the DBM and DBN models is that all but the last stacked layer in a DBN are Bayesian networks [see @pearl985bayesian], rather than RBMs. Thus for visibles $X_1, \dots, X_N$ with support $\mathcal{X}^N$, a DBN is also a FOES model if the number $q(N)$ of components in the parameter vector is dependent on the dimension of the visibles. Commonly, as in logistic belief nets [@neal1992connectionist], a "weight" parameter is placed on each interaction between visibles, $X_1, \dots, X_N$, and the first layer of latent variables, $H^{(1)}_1, \dots, H^{(1)}_{N_{(H,1)}}$, in the definition of a FOES model.

# Main results on model instability {#instability-results}

## A criterion for instability {#criterion}

To define a measure of instability in FOES models, it is useful to consider the behavior of  data  models $P_{\theta_N}$, again supported on a set $\mathcal{X}^N$ of outcomes for $\boldsymbol X\equiv \boldsymbol X_N =(X_1,\ldots,X_N)$,  in connection to the sample size $N$.  A relevant quantity to this end is a log-ratio of extremal probabilities (LREP), defined as
\begin{align}
(\#eq:elpr)
 \REP (\boldsymbol \theta_N)  =  \log \left[\frac{\max\limits_{  \boldsymbol x\in \mathcal{X}^N}P_{\boldsymbol \theta_N}( \boldsymbol x)}{\min\limits_{ \boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}( \boldsymbol x)}\right],
\end{align}
based on maximum and minimal model probabilities. In what follows, the main idea is that instability, and other negative model features, can be associated with a FOES model formulation for $N$ random variables where the log-ratio (\ref{eq:elpr}) is overly large relative to the sample size $N$. That is, a sequence of FOES probability models $P_{\boldsymbol \theta_N}$ results in specifying the distribution of observations $\boldsymbol X=(X_1,\ldots,X_N)$ for each sample size $N \geq 1$ and instability will generally occur among these models whenever the corresponding LREP \@ref(eq:elpr) grows faster than $N$. This leads to the following definition.

```{definition, name="S-unstable FOES model", label="instabFSFS", echo=TRUE}
A FOES model formulation for $\boldsymbol X_N=(X_1,\ldots,X_N)$ is *Schweinberger-unstable* or *S-unstable* if
\begin{equation}
(\#eq:Sun)
\lim \limits_{N \rightarrow \infty} \frac{1}{N} \REP(\boldsymbol \theta_N) \equiv \lim \limits_{N \rightarrow \infty} \frac{1}{N}\log \left[\frac{\max\limits_{  \boldsymbol x\in \mathcal{X}^N}P_{\boldsymbol \theta_N}( \boldsymbol x)}{\min\limits_{ \boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}( \boldsymbol x)}\right] = \infty
\end{equation}
 as the number of variables increases ($N \rightarrow \infty$).
```
In other words, a model is S-unstable if $\REP(\boldsymbol \theta_N)/N$ is an unbounded sequence of sample size  $N$; namely, given any $C > 0$, there exists an integer $N_C > 0$ so that $\REP(\boldsymbol \theta_N)/N > C$ holds for all $N \ge N_C$. A FOES model formulation may be termed S-stable if it fails to be S-unstable, i.e., if $\sup_{N \geq 1}\REP(\boldsymbol \theta_N)/N $ is bounded.

This definition of S-unstable is a generalization or reinterpretation of "unstable" used in @schweinberger2011instability by allowing possibility non-exponential family models (e.g., RBM and DBM models in Sections \@ref(restricted-boltzmann-machines) - \@ref(deep-learning) as well as a potentially increasing number $d(N)$ of parameters through the parameter sequence $\boldsymbol \theta_N\in \mathbb{R}^{d(N)}$. While this definition differs in form and scope from the original, it does match that in @schweinberger2011instability for the special case of exponential models (cf. Section \@ref(discrete-exponential-family-models) considered there. Section \ref@(illustrations) provides several examples  of unstable models as well as causes for model instability, where the latter may often be traced to issues in model form (i.e., data functions) and/or parameterization. We next describe several potentially undesirable features associated with unstable models.

```{remark, echo=TRUE}
In the definition \@ref(eq:Sun) of S-instability, we note that the numerical measure $\REP(\boldsymbol \theta_N)/N$ of model instability is invariant to *independent  replications* of data. That is, let $M \geq 1$ denote a possible number of replications and consider data $\boldsymbol Y_{N,M} \equiv (\boldsymbol X^{(1)}_N, \dots, \boldsymbol X^{(M)}_N)$ formed by $\{ \boldsymbol X^{(j)}_N\}_{j=1}^M$ as $M$  iid replications of a random vector $\boldsymbol X_N=(X_1,\ldots,X_N)$, where the latter follows a FOES model with probabilities \(P_{ \boldsymbol \theta_N}(\boldsymbol x)>0\), $\boldsymbol x\in\mathcal{X}^N$. This leads to a joint model, say $P_{ \boldsymbol \theta_N}(\boldsymbol y)$, $\boldsymbol y\in\mathcal{X}^{NM}$, for $\boldsymbol Y_{N,M}$ consisting of $N*M$ random variables in total. Then, the LREP for $\boldsymbol Y_{N,M}$, scaled by associated size, is given by
\begin{align*}
\frac{1}{NM}\REP_{\boldsymbol Y_{N,M}}( \boldsymbol\theta_N ) &\equiv \frac{1}{NM}\log\left[\frac{\max_{\boldsymbol y \in \mathcal{X}^{NM}}P_{ \boldsymbol \theta_N}(\boldsymbol y)  }{\min_{\boldsymbol y \in \mathcal{X}^{NM}}P_{ \boldsymbol \theta_N}(\boldsymbol y)} \right] \\
&= \frac{1}{NM}\log\left[\frac{\max_{\boldsymbol x \in \mathcal{X}^{N}}P_{ \boldsymbol \theta_N}(\boldsymbol x)  }{\min_{\boldsymbol x \in \mathcal{X}^{N}}P_{ \boldsymbol \theta_N}(\boldsymbol x)} \right]^M \equiv \frac{1}{N}\REP_{\boldsymbol X_{N}}( \boldsymbol\theta_N ),
\end{align*}
where $\REP_{\boldsymbol X_{N}}( \boldsymbol\theta_N )  \equiv \REP( \boldsymbol\theta_N )$ denotes the log-ratio of extremal probabilities for $\boldsymbol X_N$ defined from \@ref(eq:elpr). That is, due to iid properties, the sample-size corrected  LREP for $\boldsymbol Y_{N,M}$ equals the analog, $\REP(\boldsymbol\theta_N )/N$, from the underlying common data model for $\boldsymbol X_N$ alone, regardless of the level $M \geq 1$ of independent replication. Consequently, the definition of an S-unstable  model  is  unaffected by independent replication and all instability properties  may be characterized by those of one observation  from the common FOES model. For computational purposes, this aspect also implies that if the original data $\boldsymbol X_N=(X_1,\ldots,X_N)$ in a FOES model consist of $N$ iid random variables, then the size-scaled log-ratio \@ref(eq:elpr)  may be calculated as
$$
\frac{1}{N}\REP( \boldsymbol\theta_N ) \equiv \frac{1}{N}\REP_{\boldsymbol X_{N}}( \boldsymbol\theta_N ) = \log\left[ \frac{\max_{ x \in \mathcal{X}}P_{\boldsymbol\theta_N}(X_1=x)} {\min_{ x \in \mathcal{X}}P_{\boldsymbol\theta_N}(X_1=x)} \right]
$$
based on the extremal probabilities of just one random variable $X_1$.
```

## Characterizations and consequences of instability

As a basic characteristic, S-unstable FOES model sequences  have extremely sensitive probability structures. One aspect is that small changes in data configuration can   lead to very large changes in probability. Consider, for example, the quantity given by
$$
\DN(\boldsymbol \theta_N) \equiv \max \left\{\log \frac{P_{\boldsymbol \theta_N}(\boldsymbol x)}{P_{\boldsymbol \theta_N}(\boldsymbol x^*)} : \boldsymbol x \text{ }\& \text{ } \boldsymbol x^* \in \mathcal{X}^N \text{ differ in exactly one component}\right\},
$$
which represents the biggest log-probability ratio for a one component change in data outcomes in a FOES model with parameter $\boldsymbol \theta_N$.  We then have the following result prescribing the behavior of $\DN(\boldsymbol \theta_N)$ for unstable models.

```{theorem, label="instab-elpr", echo=TRUE}
Let $P_{\boldsymbol \theta_N}$, with support $\mathcal{X}^N$, $N\geq 1$, be a sequence of FOES models.

(i) For any integer $N \geq 1$ and any given $C>0$, if $\REP(\boldsymbol \theta_N)/N > C$ in (\ref{eq:elpr}), then
    $$ \DN(\boldsymbol \theta_N) > C,$$
    or probabilities from a one component change in some outcome have log-ratio exceeding $C$.
(ii) Suppose the FOES model sequence is S-unstable. Then, for all large $N$ and given any arbitrary $C>0$, there exist outcomes $\boldsymbol x,\boldsymbol x^*\in\mathcal{X}^N$, differing by one component, such that
    $$ \frac{P_{\boldsymbol \theta_N}(\boldsymbol x)}{P_{\boldsymbol \theta_N}(\boldsymbol x^*)} > \exp[N C]. $$
```

Theorem \@ref(instab-elpr)(i) is a  non-asymptotic result, which connects to the definition of instability in a FOES model through a log-ratio of extreme model probabilities \@ref(eq:elpr) being too large relative to the associated sample size $N$. If so, Theorem \ref@(instab-elpr)(i) guarantees the FOES model must also exhibit correspondingly large changes in probability for very small differences among some data configurations, a property that intuitively captures a notion of instability.  Furthermore, and perhaps more seriously under Theorem \@ref(instab-elpr)(ii),
S-unstable models can never have universally bounded changes in probability among single component variations in data configurations. While not all one component changes in data may produce massive changes in probability, unstable models must have some such data outcomes with this property.  As a consequence, unstable probability structure may exhibit extreme sensitivity through large peaks and troughs over the sample space.

Additionally, S-unstable FOES model sequences are also connected to degenerate models, where {\it degeneracy} involves assigning essentially all probability to modes within the sample space, which could potentially represent a  small subset among the totality of outcomes. For perspective, note that differing sizes of the scaled log-ratio $\REP(\boldsymbol \theta_N)/N$ from \@ref(eq:elpr) induce a spectrum of levels of instability/stability and Theorem \@ref(instab-elpr) indicates increasing sensitivity of model probabilities as \@ref(eq:elpr) increases. Furthermore, as the instability measure grows and the log-ratio $\REP(\boldsymbol \theta_N)/N$ diverges, as in the definition \@ref(eq:Sun) of S-unstable models, then a FOES model sequence  will become degenerate. Theorem \@ref(degenFOES) provides a formal statement of such degeneracy due to instability. For a given $0 < \epsilon < 1$, define a $\epsilon$-modal set of   outcomes as
\begin{equation}
(\#eq:mode)
\mathcal{M}_{\epsilon, \boldsymbol \theta_N} \equiv \left\{\boldsymbol x \in \mathcal{X}^N: \log P_{\boldsymbol \theta_N}(\boldsymbol x) > (1-\epsilon)\max\limits_{\boldsymbol y \in \mathcal{X}^N} \log  P_{\boldsymbol \theta_N}(\boldsymbol y) + \epsilon\min\limits_{\boldsymbol y \in \mathcal{X}^N} \log P_{\boldsymbol \theta_N}(\boldsymbol y) \right\}.
\end{equation}

```{theorem, label="degenFOES", echo=TRUE}
For any arbitrarily small $0 < \epsilon < 1$, an S-unstable FOES model sequence $P_{\boldsymbol \theta_N}$, $N \geq 1$, for $\boldsymbol X_N=(X_1, \dots, X_N)$ satisfies
$$
P_{\boldsymbol \theta_N}\left( \boldsymbol X_N\in \mathcal{M}_{\epsilon, \boldsymbol \theta_N}\right) \rightarrow 1 \text{ as } N \rightarrow \infty.
$$
```

In other words, as the sample size grows in S-unstable FOES models, all probability   tends to concentrate mass on an $\epsilon$-modal set, where $\epsilon$ can be made arbitrarily small. Intuitively, the occurrence of such degeneracy can be explained
by a type of "reverse" pigeonhole principle for unstable FOES models: if all outcomes should receive positive probability but the maximal probability far exceeds the minimal one in the model, then little probability remains for distribution among remaining model outcomes (i.e., if nearly all available pigeons are stuffed into one hole, the remaining pigeonholes must have few occupants). Degeneracy in unstable models can pose dangers in data modeling as well, particularly when a mode set represents a narrow collection of outcomes among those realistically possible for adequately describing data. In which case, model outcomes may fail to look like data of interest.

Connected to degeneracy, unstable models may also exhibit additional kinds of  extreme and undesirable sensitivity in probabilities if model parameters $\boldsymbol \theta_N$ can further be "dialed" between positive and negative values. That is, some FOES models naturally involve parameter spaces covering a positive-negative spectrum of parameter possibilities, where the signs of parameters provide a standard device for increasing or decreasing probabilities of outcomes in the model formulation.  In fact, for many models, the switch of a parameter sign serves to produce reciprocal probabilities, as outlined in the following model assumption about parameter sign reversal (PSR).

```{mcpsr, name="Reciprocal Probabilities from Parameter Sign Reversal", echo=TRUE}
Let $P_{\boldsymbol \theta_N}$, with support $\mathcal{X}^N$, $N\geq 1$, represent a sequence of FOES models. For each $N \geq 1$ and any outcome $\boldsymbol x \in \mathcal{X}^N$, suppose it holds that
$$
P_{\boldsymbol \theta_N}(\boldsymbol x)  \cdot P_{-\boldsymbol \theta_N}(\boldsymbol x) \;=\;   \max\limits_{\boldsymbol y \in \mathcal{X}^N}P_{ \boldsymbol \theta_N}(\boldsymbol y)\cdot \min\limits_{\boldsymbol y \in \mathcal{X}^N}P_{-\boldsymbol \theta_N}(\boldsymbol y),
$$
where $\max_{\boldsymbol y \in \mathcal{X}^N}P_{ \boldsymbol \theta_N}(\boldsymbol y)$ and $\min_{\boldsymbol y \in \mathcal{X}^N}P_{-\boldsymbol \theta_N}(\boldsymbol y)$ denote the maximum and minimum probabilities under parameters $\boldsymbol \theta_N$   and $-\boldsymbol \theta_N$, respectively.
```

The above model condition incorporates many standard parameterizations and follows, for instance, whenever $P_{\boldsymbol \theta_N}(\boldsymbol x)/P_{\boldsymbol \theta_N}(\boldsymbol y)  = [P_{-\boldsymbol \theta_N}(\boldsymbol x)/P_{-\boldsymbol \theta_N}(\boldsymbol y)]^{-1}$ holds for outcomes $\boldsymbol x, \boldsymbol y \in\mathcal{X}^N$ in a FOES model. For instance, this latter condition is fulfilled for all linear exponential families from Section \@ref(discrete-exponential-family-models) (e.g., \@ref(eq:mod1)-\@ref(eq:mod2)) as well as all network models from Sections \@ref(restricted-boltzmann-machines)-\@ref(deep-learning) (e.g., \@ref(eq:RBM1)-\@ref(eq:RBM2)). When parameters can be tuned in sign with effects prescribed in the model condition PSR, unstable FOES models will exhibit further probability sensitivities, as outlined in the following extension of Theorem \@ref(degenFOES}).

```{corollary, label="sign", echo=TRUE}
Let $P_{\boldsymbol \theta_N}$, with support $\mathcal{X}^N$, $N\geq 1$, be a sequence of FOES models satisfying model condition PSR. If the models $P_{\boldsymbol \theta_N}$  are additionally S-unstable, then

(i) the models  $P_{-\boldsymbol \theta_N}$  defined by $-\boldsymbol \theta_N$  are also S-unstable;
(ii) and for the complement $\mathcal{M}_{\epsilon, \boldsymbol \theta_N}^c \equiv \mathcal{X}^N \setminus \mathcal{M}_{\epsilon, \boldsymbol \theta_N}$ of any mode-set $\mathcal{M}_{\epsilon, \boldsymbol \theta_N} $ under $\boldsymbol \theta_N$ from (\ref{eqn:mode}), with $0<\epsilon<1$, it holds under $- \boldsymbol \theta_N$ that
    $$ P_{-\boldsymbol \theta_N}( \boldsymbol X_N \in \mathcal{M}_{\epsilon, \boldsymbol \theta_N}^c) \rightarrow 1\qquad \mbox{as $N\to \infty$},$$
    while, by Theorem \@ref(degenFOES), $P_{\boldsymbol \theta_N}( \boldsymbol X_N \in \mathcal{M}_{\epsilon, \boldsymbol \theta_N} ) \rightarrow 1$ holds for $ \boldsymbol X_N = (X_1,\ldots,X_N)$ under $  \boldsymbol \theta_N$.
```

For unstable models, Corollary \@ref(sign) shows that shifts in parameters around zero (i.e., from $\boldsymbol \theta_N$ to $-\boldsymbol \theta_N$) can induce extreme changes in probability among subsets of the sample space, as another manifestation  of instability and hyper-sensitivity in probability structure. For one-parameter exponential families, involving a fixed real-valued linear parameter  $\boldsymbol \theta_N = \theta \in \mathbb{R}$ and sufficient statistic $\boldsymbol g_N(\boldsymbol x)\in \mathbb{R}$ in \@ref(eq:expo), @schweinberger2011instability [Theorem 3] proved a result similar in spirit, though based on a characterization there in terms of maximum $U_N \equiv \max_{\boldsymbol x\in\mathcal{X}^N}g_N(\boldsymbol x)$ and minimal $L_N \equiv \min_{\boldsymbol x\in\mathcal{X}^N}g_N(\boldsymbol x)$ values of the sufficient statistic. For this case in particular,  mode sets have specific, and essentially complementary, forms over positive and negative parameters, namely, $\mathcal{M}_{\epsilon, \boldsymbol \theta_N} = \{\boldsymbol x \in\mathcal{X}^N: g_N(\boldsymbol x)  >  (1-\epsilon) U_N + \epsilon L_N \}$ and $\mathcal{M}_{\epsilon, -\boldsymbol \theta_N} = \{\boldsymbol x \in\mathcal{X}^N: g_N(\boldsymbol x)  <  \epsilon U_N + (1-\epsilon) L_N \}$ for any $\boldsymbol \theta_N>0$, and
 @schweinberger2011instability [Theorem 3] showed each mode set collects all mass, under positive and negative parameters, respectively, with unstable models of this exponential type. However, for all unstable FOES models, Corollary \@ref(sign) generalizes the same principle that unstable models can push all probability to different, and in fact disjoint, parts of the sample space, depending on how parameters fall with respect to zero. This feature can numerically complicate likelihood manipulations, such as maximization or MCMC-based Bayes posterior sampling, as further discussed in Section \@ref(statistical-consequences).

# Implications

For a large class of models that covers a broad range of applications (including "deep learning"), we have developed a formal definition of instability in model probability structure and elucidated multiple consequences of instability. We have shown for FSFS models that instability manifests through small changes in data leading to potentially large changes in probability as well as the potential to place all probability on certain modal subsections of the sample space, which potentially could be small. The FSFS model class is quite broad and, particularly in developing FSFS models for large data sets, some caution should be used in parameter specification to control effects of model instability.  
