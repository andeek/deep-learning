\documentclass[12pt]{article}
\usepackage{amsmath,amssymb}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

\usepackage{longtable,booktabs}
\usepackage[unicode=true]{hyperref}
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
  
\usepackage{tikz, subfig, amsthm, multirow, float}
\usepackage{tikz-3dplot}
\usetikzlibrary{arrows, shapes, positioning}
\usepackage{bm}

%%INCLUDED-AK
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\nv}{{n_{\scriptscriptstyle V}}}
\newcommand{\nh}{{n_{\scriptscriptstyle H}}}
\newcommand{\E}{E}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%
\addtolength{\topmargin}{-.8in}%



\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}


\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

\title{\bf A note on the instability and degeneracy of deep learning models}
\author{Andee Kaplan \\ Department of Statistics, Iowa State University \\  and \\ Daniel Nordman \\ Department of Statistics, Iowa State University \\  and \\ Stephen Vardeman \\ Department of Statistics and Department of Industrial and\\
Manufacturing Systems Engineering, Iowa State University \\ }

\date{}
\maketitle
\begin{abstract}
A probability model exhibits instability if small changes in a data
outcome result in large, and often unanticipated, changes in
probability. For correlated data structures found in several application
areas, there is increasing interest in predicting/identifying such
sensitivity in model probability structure. We consider the problem of
quantifying instability for general probability models defined on
sequences of observations, where each sequence of length \(N\) has a
finite number of possible outcomes. A sequence of probability models
results, indexed by \(N\), that accommodates data of expanding
dimension. Model instability is formally shown to occur when a certain
log-probability ratio under such models grows faster than \(N\). In this
case, a one component change in the data sequence can shift probability
by orders of magnitude. Also, as instability becomes more extreme, the
resulting probability models are shown to tend to degeneracy, placing
all their probability on potentially small portions of the sample space.
These results on instability apply to large classes of models commonly
used in random graphs, network analysis, and machine learning contexts.
\end{abstract}
\noindent%
{\it Keywords:}  Degeneracy, Instability, Classification, Deep Learning, Graphical Models
\vfill

\newpage
\spacingset{1.45}


\section{Introduction}\label{introduction}

We consider the behavior, and the potential impropriety, of probability
models built to incorporate a sequence of discrete observations with
length \(N\). Let \((X_1, \dots, X_N)\) denote a set of discrete random
variables with a finite sample space, \(\mathcal{X}^N\). That is,
\(\mathcal{X}\) with \(|\mathcal{X}| < \infty\) represents a finite set
of potential outcomes for each single variable \(X_i\), and the data
sequence \((X_1,\ldots,X_N)\) takes values in the \(N\)-fold product
space \(\mathcal{X}^N\). For each \(N\), let
\(P_{\boldsymbol \theta_N}\) denote a probability model on
\(\mathcal{X}^N\), under which
\(P_{\boldsymbol \theta_N}(x_1, \dots, x_N) > 0\) is the probability of
the data outcome \((x_1, \dots, x_N) \in \mathcal{X}^N\). In this, we
assume that the model support of \(P_{\boldsymbol \theta_N}\) is the
sample space \(\mathcal{X}^N\). This framework produces a series
\(P_{\boldsymbol \theta_N}\) of probability models, indexed by a generic
sequence of parameters \(\boldsymbol \theta_N\), to describe data of
each length \(N \geq 1\). The size and structure of such parameters are
without restriction, and natural cases include those where
\(\boldsymbol \theta_N \in \mathbb{R}^{q(N)}\) for some arbitrary
integer-valued function \(q(\cdot) \geq 1\). We will refer to this model
class as \emph{Finitely Supported Finite Sequence (FSFS) models}.

Section \ref{examples} provides several examples of FSFS models commonly
used in graph/network analysis and machine learning (i.e., deep learning
models). Section \ref{instability-results} establishes formal results
regarding the propriety of FSFS models with regard to stability. A FSFS
probability model sequence exhibits instability if small changes in the
components of a data outcome \((x_1,\ldots,x_N)\) can result in large
changes in probability \(P_{\theta_N}(x_1,\ldots,x_N)\). The concept of
instability, introduced in the field of statistical physics by Ruelle
(\protect\hyperlink{ref-ruelle1999statistical}{1999}), was extended to
include a notion of detection and quantification for certain exponential
family models by Schweinberger
(\protect\hyperlink{ref-schweinberger2011instability}{2011}). For
similar exponential models, particularly in connection to random
graphs/networks, Handcock
(\protect\hyperlink{ref-handcock2003assessing}{2003}) considered
(mean-based) characterizations for so-called model degeneracy, whereby a
probability model places all mass on a small subset of the sample space
and produces undesirably low variability in model outcomes. As described
by Schweinberger
(\protect\hyperlink{ref-schweinberger2011instability}{2011}), model
instability and model degeneracy are related by viewing degeneracy as an
extreme or limiting form of instability. The instability results of
Schweinberger
(\protect\hyperlink{ref-schweinberger2011instability}{2011}) were
developed for the case of discrete exponential family models. The main
results here concern a general measure of model instability, appropriate
across the whole FSFS model class. This can be used to identify when
certain maximal probabilities in FSFS models are too extreme relative to
the length \(N\) and may thereby induce a potentially undesirable
probability structure. In this case, a one component change in the data
sequence may shift probability by orders of magnitude, and FSFS models
are rigorously shown to become degenerate as the measure of instability
increases. Lastly, Section \ref{implications} emphasizes the
implications of our model propriety results and proofs of the main
results appear in Appendix \ref{appendix-instab}.

\section{Examples}\label{examples}

Many model families fall under the umbrella of FSFS models. For
illustration, this section presents three specific examples of FSFS
models, including models with deep architectures.

\subsection{Discrete exponential family
models}\label{discrete-exponential-family-models}

For discrete random variables \(\boldsymbol X = (X_1, \dots, X_N)\) with
sample space \(\mathcal{X}^N\), \(|\mathcal{X}| < \infty\), consider an
exponential family model for \(\boldsymbol X\) with probability mass
function of the form \[
p_{N, \boldsymbol \lambda}(\boldsymbol x) = \exp\left[\boldsymbol\eta^T(\boldsymbol \lambda) \boldsymbol g_N(\boldsymbol x) - \psi(\boldsymbol \lambda)\right], \qquad \boldsymbol x \in \mathcal{X}^N,
\] depending on parameter vector
\(\boldsymbol \lambda \in \Lambda \subset \mathbb{R}^{k}\) and natural
parameter function
\(\boldsymbol \eta : \mathbb{R}^k \mapsto \mathbb{R}^L\) with fixed
positive integers \(k\) and \(L\) denoting their dimensions. Above,
\(\boldsymbol g_N : \mathcal{X}^N \mapsto \mathbb{R}^L\) is a vector of
sufficient statistics, while \[
\psi(\boldsymbol \lambda) = \log \sum\limits_{\boldsymbol x \in \mathcal{X}^N}\exp\left[\boldsymbol \eta^T(\boldsymbol \lambda) \boldsymbol g_N(\boldsymbol x) \right], \qquad \boldsymbol \lambda \in \Lambda,
\] denotes the normalizing function, and
\(\Lambda = \{\boldsymbol \lambda \in \mathbb{R}^k : \psi(\boldsymbol \lambda) < \infty, k \le q(N) \}\)
is the parameter space.

Defining
\(P_{\boldsymbol \theta_N}(\boldsymbol x)\equiv p_{N,\boldsymbol \lambda_N}(\boldsymbol x)\)
with \(\boldsymbol \theta_N=\boldsymbol \lambda_N\) to be a sequence of
elements of \(\Lambda \subset \mathbb{R}^k\) and noting that
\(P_{\boldsymbol \theta_N}(\boldsymbol x) > 0\) for all
\(\boldsymbol x \in \mathcal{X}^N\), these discrete exponential family
models are special cases of the FSFS models. Such exponential models
arise with spatial data on a lattice (Besag
\protect\hyperlink{ref-besag1974spatial}{1974}), network data (Wasserman
and Faust \protect\hyperlink{ref-wasserman1994social}{1994}; Handcock
\protect\hyperlink{ref-handcock2003assessing}{2003}), and even standard
independence models for discrete data, such as with \(N\) iid Bernoulli
random variables. (Note that for random graphs or networks with, say,
\(m\) nodes, one may wish to consider \(N={m \choose 2}\) edges as
binary (presence/absence) variables \(X_i\). In this case, the length
\(N\) of data sequence may naturally increase as a function of \(m\).)
For these exponential models, the dimension of the parameter
\(\boldsymbol \theta_N\) remains constant over each \(N\), as
\(\boldsymbol \theta_N\) lies in a parameter space of fixed Euclidean
dimension \(k\). This need not be true for other types of FSFS models
considered in Sections
\ref{restricted-boltzmann-machines}-\ref{deep-learning}. Schweinberger
(\protect\hyperlink{ref-schweinberger2011instability}{2011}) considered
instability in such exponential models (e.g., for random graphs) for
sequences of fixed parameters
\(\boldsymbol \theta_N=\boldsymbol \lambda\in\mathbb{R}^k\),
\(N \geq 1\).

\subsection{Restricted Boltzmann
machines}\label{restricted-boltzmann-machines}

A restricted Boltzmann machine (RBM) is an undirected graphical model
specified for discrete or continuous random variables, with binary
variables being most common (cf. Smolensky
\protect\hyperlink{ref-smolensky1986information}{1986}). A RBM
architecture has two layers, hidden (\(\mathcal{H}\)) and visible
(\(\mathcal{V}\)), with conditional independence within each layer. Let
\(\boldsymbol X=(X_1,\ldots,X_N)\) denote the \(N\) random variables for
visibles with support \(\mathcal{X}^N\) and
\(\boldsymbol H=(H_1,\ldots,H_{N_H})\) denote the \(N_H\) random
variables for hiddens with support \(\mathcal{X}^{N_H}\) where
\(\mathcal{X} = \{-1,1\}\). For parameters
\(\boldsymbol \alpha \in \mathbb{R}^{N_H}\),
\(\boldsymbol \beta \in \mathbb{R}^N\), and \(\Gamma\) as a matrix with
dimension \(N_H \times N\), the RBM model for
\(\tilde{\boldsymbol X}=(\boldsymbol X,\boldsymbol H)\) then has the
joint probability mass function \[
P_{\boldsymbol \theta_N} (\tilde{\boldsymbol x}) = \exp\left[ \boldsymbol \alpha^T \boldsymbol h + \boldsymbol \beta^T \boldsymbol x + \boldsymbol h^T \Gamma \boldsymbol x - \psi(\boldsymbol \theta_N)\right], \qquad \tilde{\boldsymbol x} = (\boldsymbol h, \boldsymbol x) \in \mathcal{X}^{N+N_H},
\] where \[
\psi(\boldsymbol \theta_N) = \log \sum_{\tilde{\boldsymbol x} \in \mathcal{X}^{N + N_H}} \exp\left[ \boldsymbol \alpha^T \boldsymbol h + \boldsymbol \beta^T \boldsymbol x + \boldsymbol h^T \Gamma \boldsymbol x\right], \qquad \boldsymbol \theta_N \in \Theta_N,
\] is the normalizing function. Let
\(\boldsymbol \theta_N = (\boldsymbol \alpha, \boldsymbol \beta,\Gamma) \in \Theta_N \subset \mathbb{R}^{q(N)}\)
with \(q(N) = N + N_H + N*N_H\) denote the vector of parameters for the
RBM. The probability mass function for the visible variables
\(X_1, \dots, X_N\) follows from marginalizing this joint specification:
\[
P_{\boldsymbol \theta_N} (\boldsymbol x) = \sum\limits_{\boldsymbol h \in \mathcal{X}^{N_H}} P_{\boldsymbol \theta_N} (\boldsymbol x, \boldsymbol h), \qquad \boldsymbol x \in \mathcal{X}^N.
\] Note that the vector of model parameters \(\boldsymbol \theta_N\), of
size \(q(N)\), grows in size as a function of sample dimension \(N\) to
accommodate the dimension of visible variables \(X_1, \dots, X_N\), and
one may further choose the number \(N_H\) of hidden variables to change
with \(N\) as well. In particular, the number \(N_H\) of hiddens may
also potentially and arbitrarily increase with \(N\). Additionally, as
\(|\mathcal{X}| = 2\) and
\(P_{\boldsymbol \theta_N}(\boldsymbol x) > 0\) for all
\(\boldsymbol x \in \mathcal{X}^N\), the RBM model specification for
visibles is a FSFS model. This example also indicates that models formed
by marginalizing a base FSFS model (e.g., a type of exponential family
model) is again a FSFS model class.

\subsection{Deep learning}\label{deep-learning}

Consider two models with ``deep architecture'' that contain multiple
hidden (or latent) layers in addition to a visible layer of data, namely
a deep Boltzmann machine (Salakhutdinov and Hinton
\protect\hyperlink{ref-salakhutdinov2009deep}{2009}) and a deep belief
network (Hinton, Osindero, and Teh
\protect\hyperlink{ref-hinton2006fast}{2006}). Let \(M\) denote the
number of hidden layers included in the model and let
\(N_{(H,1)}, \dots, N_{(H,M)}\) denote the numbers of hidden variables
within each hidden layer. Then the random vector
\(\tilde{\boldsymbol X} = \{H^{(1)}_1, \dots, H^{(1)}_{N_{(H,1)}}, \dots, H^{(M)}_1, \dots, H^{(M)}_{N_{(H,M)}}, \boldsymbol X\}\)
collects both the hidden variables
\(\{ H_{i}^{(j)} : i=1,\ldots, N_{(H,j)}, j=1,\ldots,M\}\) and visible
variables \(\boldsymbol X =(X_1,\ldots,X_N)\) in a deep probabilistic
model. Each variable outcome will again lie in
\(\mathcal{X} = \{-1,1\}\).

\textbf{Deep Boltzmann machine (DBM).} The DBM class of models maintains
conditional independence within all layers in the model by stacking RBM
models and only allowing conditional dependence between neighboring
layers. The joint probability mass function for a DBM is \[
P_{\boldsymbol \theta_N} (\tilde{\boldsymbol x}) = \exp\left[ \sum\limits_{i = 1}^M\boldsymbol \alpha^{(i)T} \boldsymbol h^{(i)} + \boldsymbol \beta^T \boldsymbol x + \boldsymbol h^{(1)T} \Gamma^{(0)} \boldsymbol x + \sum\limits_{i = 1}^{M - 1} \boldsymbol h^{(i)T} \Gamma^{(i)} \boldsymbol h^{(i + 1)} - \psi(\boldsymbol \theta_N) \right], 
\] for
\(\tilde{\boldsymbol x} = (\boldsymbol h^{(1)}, \dots, \boldsymbol h^{(M)}, \boldsymbol x) \in \mathcal{X}^{N_{(H,1)} + \cdots + N_{(H,M)} +N}\)
where \[
\psi(\boldsymbol \theta_N) = \log \sum\limits_{\tilde{\boldsymbol x} \in \mathcal{X}^{N_{(H,1)} + \cdots + N_{(H,M)} +N}} \exp\left[ \sum\limits_{i = 1}^M\boldsymbol \alpha^{(i)T} \boldsymbol h^{(i)} + \boldsymbol \beta^T \boldsymbol x + \boldsymbol h^{(1)T} \Gamma^{(0)} \boldsymbol x + \sum\limits_{i = 1}^{M - 1} \boldsymbol h^{(i)T} \Gamma^{(i)} \boldsymbol h^{(i + 1)}\right],
\] for \(\boldsymbol \theta_N \in \Theta_N\) is the normalizing function
and parameters in the model are \(\boldsymbol \beta \in \mathbb{R}^V\),
\(\boldsymbol \alpha^{(i)} \in \mathbb{R}^{N_{(H,i)}}\) for
\(i = 1, \dots, M\), along with a matrix \(\Gamma^{(0)}\) of dimension
\(N_{(H,1)} \times N\), and matrices \(\Gamma^{(i)}\) of dimension
\(N_{(H,i)} \times N_{(H,i+1)}\) for \(i = 1, \dots, M-1\). Let
\(\boldsymbol \theta_N = (\boldsymbol \alpha^{(1)}, \dots, \boldsymbol \alpha^{(M)}, \boldsymbol \beta,\Gamma^{(0)}, \dots, \Gamma^{(M - 1)}) \in \Theta_N \subset \mathbb{R}^{q(N)}\)
denote the combined vector of parameters with total length
\(q(N)= N_{(H,1)}+\cdots N_{(H,M)} + N + N_{(H,1)}*N+N_{H,2}*H_{(H,1)}+\cdots +N_{(H,M)}*H_{(H,M)-1}\).

The probability mass function for the visible random variables
\(X_1, \dots, X_N\) follows from this joint specification as \[
P_{\boldsymbol \theta_N} (\boldsymbol x) = \sum\limits_{(\boldsymbol h^{(1)}, \dots, \boldsymbol h^{(M)}) \in \mathcal{X}^{N_{(H,1)} + \cdots + N_{(H,M)}}} P_{\boldsymbol \theta_N} (\tilde{\boldsymbol x}), \qquad \boldsymbol x \in \mathcal{X}^N
\] Again like the RBM case, the DBM model specification examples a FSFS
model.

\textbf{Deep belief network (DBN).} A DBN resembles a DBM in that there
are multiple layers of latent random variables stacked in a deep
architecture with no conditional dependence between layers. The
difference between the DBM and DBN models is that all but the last
stacked layer in a DBN are Bayesian networks (see Pearl
\protect\hyperlink{ref-pearl985bayesian}{1985}), rather than RBMs. Thus
for visibles \(X_1, \dots, X_N\) with support \(\mathcal{X}^N\), a DBN
is also a FSFS model if the number \(q(N)\) of components in the
parameter vector is dependent on the dimension of the visibles.
Commonly, as in logistic belief nets (Neal
\protect\hyperlink{ref-neal1992connectionist}{1992}), a ``weight''
parameter is placed on each interaction between visibles,
\(X_1, \dots, X_N\), and the first layer of latent variables,
\(H^{(1)}_1, \dots, H^{(1)}_{N_{(H,1)}}\), in the definition of a FSFS
model.

\section{Instability results}\label{instability-results}

To define or measure instability in FSFS models, it is useful to
consider the behavior of a data model sequence \(P_{\theta_N}\). A
relevant quantity to this end is a (scaled) extremal log-probability
ratio (ELPR)
\begin{align}
\frac{1}{N} \log \left[\frac{\max\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(x_1, \dots, x_N)}{\min\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(x_1, \dots, x_N)}\right] \equiv \frac{1}{N} \text{ELPR}_N(\boldsymbol \theta_N). \label{eq:elpr}
\end{align}
The main idea is that, in formulating FSFS models for potentially
increasing numbers of variables (i.e., for \(N \rightarrow \infty\)),
the ratio (\eqref{eq:elpr}) should remain bounded to better ensure model
stability, requiring that the largest probability possible under
\(P_{\theta_N}\) maintain a fixed order of magnitude relative to the
smallest probability allowed under the same model. Specifically, the log
of the ratio should grow at mostly linearly with the sample size \(N\).
This leads to the following definition.
\BeginKnitrBlock{definition}[S-unstable FSFS]
\protect\hypertarget{def:instabFSFS}{}{\label{def:instabFSFS}
\iffalse (S-unstable FSFS) \fi{} }Let
\(\boldsymbol \theta_N \in \mathbb{R}^{q(N)}\) be a sequence of FSFS
model parameters where the size of the model \(q(N)\) is a function of
the number of random variables \(N\). A FSFS model formulation is
\emph{Schweinberger-unstable} or \emph{S-unstable} if, as the number of
variables increase (\(N \rightarrow \infty\)),
\begin{align*}
\lim\limits_{N \rightarrow \infty} \frac{1}{N} \text{ELPR}(\boldsymbol \theta_N) \equiv \lim\limits_{N \rightarrow \infty} \frac{1}{N} \log \left[\frac{\max\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(x_1, \dots, x_N)}{\min\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(x_1, \dots, x_N)}\right] = \infty.
\end{align*}
\EndKnitrBlock{definition} In other words, a model is S-unstable if,
given any \(C > 0\), there exists an integer \(N_C > 0\) so that
\(\frac{1}{N}\text{ELPR}_N(\boldsymbol \theta_N) > C\) for all
\(N \ge N_C\). A FSFS model formulation may be termed S-stable if it
fails to be S-unstable.

This definition of S-unstable is a generalization or reinterpretation of
``unstable'' used in Schweinberger
(\protect\hyperlink{ref-schweinberger2011instability}{2011}) by allowing
non-exponential family models (e.g.~RBM and DBM models in Sections
\ref{restricted-boltzmann-machines}-\ref{deep-learning}) and an
increasing number of parameters. While this definition differs in form
and scope from the original, it does match that in Schweinberger
(\protect\hyperlink{ref-schweinberger2011instability}{2011}) for the
special case of exponential models (cf.~Section
\ref{discrete-exponential-family-models}) considered there.

S-unstable FSFS model sequences may be undesirable for several reasons.
One is that small changes in data can lead to overly-sensitive changes
in probability. Consider, for example, the quantity given by \[
\Delta(\boldsymbol \theta_N) \equiv \max \left\{\log \frac{P_{\boldsymbol \theta_N}(\boldsymbol x)}{P_{\boldsymbol \theta_N}(\boldsymbol x^*)} : \boldsymbol x \text{ }\& \text{ } \boldsymbol x^* \in \mathcal{X}^N \text{ differ in exactly one component}\right\},
\] which represents the biggest log-probability ratio for a one
component change in data outcomes at a FSFS parameter
\(\boldsymbol \theta_N\). We then have the following (non-asymptotic)
result.

\BeginKnitrBlock{proposition}
\protect\hypertarget{prp:instab-elpr}{}{\label{prp:instab-elpr}}Let
\(\text{ELPR}(\boldsymbol \theta_N)\) be as in (\eqref{eq:elpr}) for an
integer \(N \ge 1\). For a given \(C>0\), if
\[\frac{1}{N}\text{ELPR}_N(\boldsymbol \theta_N) > C,\] then
\[\Delta_N(\boldsymbol \theta_N) > C.\]
\EndKnitrBlock{proposition}

Again, if the probability ratio (\eqref{eq:elpr}) is too large, then the
FSFS model will exhibit large changes in probability for very small
differences in the data configuration, which exemplifies the intuitive
notation of instability.

Additionally, S-unstable FSFS model sequences are connected to
degenerate models, where model \emph{degeneracy} typically entails
placing all probability on a small portion of the sample space. For
perspective, note that differing sizes of
\(1/N\cdot\text{ELPR}(\boldsymbol \theta_N)\) in (\eqref{eq:elpr}) may
induce a spectrum of levels of ``stability'' and Proposition
\ref{prp:instab-elpr} indicates increasing sensitivity of model
probabilities (i.e., for one component changes in outcomes) as
(\eqref{eq:elpr}) increases. Furthermore, as the instability measure
(\eqref{eq:elpr}) grows, FSFS model sequences are guaranteed to slide into
full degeneracy as Proposition \ref{prp:degenFSFS} shows. Define a
\(\epsilon\)-modal set \[
M_{\epsilon, \boldsymbol \theta_N} \equiv \left\{\boldsymbol x \in \mathcal{X}^N: \log P_{\boldsymbol \theta_N}(\boldsymbol x) > (1-\epsilon)\max\limits_{\boldsymbol x^* \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x^*) + \epsilon\min\limits_{\boldsymbol x^* \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x^*) \right\}
\] of possible outcomes, for a given \(0 < \epsilon < 1\).

\BeginKnitrBlock{proposition}
\protect\hypertarget{prp:degenFSFS}{}{\label{prp:degenFSFS}}For an unstable
FSFS model in Definition \ref{def:instabFSFS}, and for any given
\(0 < \epsilon < 1\), \[
P_{\boldsymbol \theta_N}\left((x_1, \dots, x_N) \in M_{\epsilon, \boldsymbol \theta_N}\right) \rightarrow 1 \text{ as } N \rightarrow \infty.
\]
\EndKnitrBlock{proposition}

In other words, in S-unstable FSFS models, all probability in the model
formulation with a large number of random variables will concentrate
mass on an \(\epsilon\)-modal set, where \(\epsilon\) can be made
arbitrarily small. The associated mode set could potentially be quite
small, in which case Proposition \ref{prp:degenFSFS} would suggest that
the unstable model asymptotically stacks all probability on a few
outcomes.

\BeginKnitrBlock{remark}
\iffalse <span class="remark"><em>Remark. \fi{} There is a further
generalization the notion of instability in Definition
\ref{def:instabFSFS} meant to address independent replications of data
sequences. That is, one might consider data as \(n\) independent and
identically distributed replications
\(\boldsymbol X_1, \dots, \boldsymbol X_n\), where each
\(\boldsymbol X_i=(X_{i,1},\ldots,X_{i,N}) \in \mathcal{X}^N\) follows a
common FSFS model with probabilities \(P_{\theta_N}(\boldsymbol x)>0\),
\(\boldsymbol x\in\mathcal{X}^N\) and \(|\mathcal{X}|<\infty\), for
\(i=1,\ldots,n\). This leads to a total of \(n*N\) random variables in
the joint model. However, the definition of S-unstable and Propositions
\ref{prp:instab-elpr}-\ref{prp:degenFSFS} still hold for such iid
replications. This is because
\(\left(\max\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)\right)^n\)
is the largest probability possible under the joint model for the \(n\)
replications while
\(\left(\min\limits_{\boldsymbol x \in \mathcal{X}}P_{\boldsymbol \theta_N}(\boldsymbol x)\right)^n\)
is the smallest probability. Thus, for the combined replications
\(\boldsymbol X_1, \ldots, \boldsymbol X_n\), the analog definition of
the extremal log-probability becomes
\begin{align*}
\frac{\text{extremal log-probability ratio}}{\text{\# random variables in the model}} &\equiv \frac{1}{n*N}\log \left[\frac{\left(\max\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)\right)^n}{\left(\min\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)\right)^n}\right] = \frac{1}{N} \log \left[\frac{\max\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)}{\min\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)}\right] \\
&= \frac{1}{N} \text{ELPR}(\boldsymbol \theta_N),
\end{align*}
implying that the definition of an S-unstable FSFS model sequence is
invariant to the level (\(n\)) of independent replication. Consequently,
overall model instabilities may be characterized by those of one
observation from the common FSFS model.
\EndKnitrBlock{remark}

\section{Implications}\label{implications}

For a large class of models that covers a broad range of applications
(including ``deep learning''), we have developed a formal definition of
instability in model probability structure and elucidated multiple
consequences of instability. We have shown for FSFS models that
instability manifests through small changes in data leading to
potentially large changes in probability as well as the potential to
place all probability on certain modal subsections of the sample space,
which could be potentially small. The FSFS model class is quite broad
and, particularly in developing FSFS models for large data sets, some
caution should be used in parameter specification to control effects of
model instability.

\clearpage

\appendix


\section{Proofs of instability results}\label{appendix-instab}

\textbf{Proof of Proposition \ref{prp:instab-elpr}.} We prove the
contrapositive, supposing that \(\Delta(\boldsymbol \theta_N) \le C\)
holds for some \(C > 0\) and show
\(\text{ELPR}(\boldsymbol \theta_N) \leq NC\). Let
\(\boldsymbol x_{min} \equiv \argmin\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)\)
and
\(\boldsymbol x_{max} \equiv \argmax\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x)\).
Note there exists a sequence
\(\boldsymbol x_{min} \equiv \boldsymbol x_0, \boldsymbol x_1, \dots, \boldsymbol x_k \equiv \boldsymbol x_{max}\)
in \(\mathcal{X}^N\) of component-wise switches to move from
\(\boldsymbol x_{min}\) to \(\boldsymbol x_{max}\) in the sample space
(i.e. \(\boldsymbol x_i, \boldsymbol x_{i + 1} \in \mathcal{X}^N\)
differ in exactly \(1\) component for \(i = 0, \dots, k\)) for some
integer \(k \in \{0, 1, \dots, N\}\). Under the FSFS model, recall
\(P_{\boldsymbol \theta_N}(\boldsymbol x) > 0\) holds so that
\(\log P_{\boldsymbol \theta_N}(\boldsymbol x)\) is well-defined for
each outcome \(\boldsymbol x \in \mathcal{X}^N\). Then, if \(k > 0\), it
follows that
\begin{align*}
\text{ELPR}(\boldsymbol \theta_N) = \log\left[\frac{P_{\boldsymbol \theta_N}(\boldsymbol x_{max})}{P_{\boldsymbol \theta_N}(\boldsymbol x_{min})}\right] &= \left|\sum\limits_{i = 1}^k\log\left(\frac{P_{\boldsymbol \theta_N}(\boldsymbol x_i)}{P_{\boldsymbol \theta_N}(\boldsymbol x_{i-1})}\right)\right| \\
&\le \sum\limits_{i = 1}^k\left|\log\left(\frac{P_{\boldsymbol \theta_N}(\boldsymbol x_i)}{P_{\boldsymbol \theta}(\boldsymbol x_{i-1})}\right)\right| \le k \Delta_N(\boldsymbol \theta_N) \le NC,
\end{align*}
using \(k \le N\) and \(\Delta(\boldsymbol \theta_N) \le C\). If
\(k = 0\), then \(\boldsymbol x_{max} = \boldsymbol x_{min}\) and the
same bound above holds.\hfill \(\Box\)

\textbf{Proof of Proposition \ref{prp:degenFSFS}.} where
\(|\mathcal{X}|<\infty\) holds in the FSFS model. We may suppose
\(|\mathcal{X}|>1\) (i.e., \(\mathcal{X}^N\) has more than one outcome)
because otherwise the model is trivially degenerate for all
\(N \geq 1\). Fix \(0 < \epsilon < 1\). Then,
\(\boldsymbol x_{max} \in M_{\epsilon, \boldsymbol \theta_N}\), so
\(P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N}) \ge P_{\boldsymbol \theta_N}(\boldsymbol x_{max}) > 0\).
If
\(\boldsymbol x \in \mathcal{X}^N \setminus M_{\epsilon, \boldsymbol \theta_N}\),
then by definition
\(P_{\boldsymbol \theta_N}(\boldsymbol x) \le [P_{\boldsymbol \theta_N}(\boldsymbol x_{max})]^{1-\epsilon}[P_{\boldsymbol \theta_N}(\boldsymbol x_{min})]^{\epsilon}\)
holds so that
\begin{align*}
1-P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N})
& = \sum\limits_{\boldsymbol x \in \mathcal{X}^N \setminus M_{\epsilon, \boldsymbol \theta_N}}P_{\boldsymbol \theta_N}(\boldsymbol x) \\
& \le (|\mathcal{X}|^N)[P_{\boldsymbol \theta_N}(\boldsymbol x_{max})]^{1-\epsilon}[P_{\boldsymbol \theta_N}(\boldsymbol x_{min})]^{\epsilon}.
\end{align*}
From the lower bound on
\(P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N})\) and the
upper bound on
\(1-P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N})\), it
follows that
\begin{align*}
\frac{1}{N}\log\left[\frac{P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N})}{1-P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N})}\right] & \ge \frac{1}{N} \log\left[\frac{P_{\boldsymbol\theta_N}(\boldsymbol x_{max})}{(|\mathcal{X}|^N)[P_{\boldsymbol \theta_N}(\boldsymbol x_{max})]^{1-\epsilon}[P_{\boldsymbol \theta_N}(\boldsymbol x_{min})]^{\epsilon}}\right] \\
&= \frac{\epsilon}{N} \log\left[\frac{P_{\boldsymbol \theta_N}(\boldsymbol x_{max})}{P_{\boldsymbol \theta_N}(\boldsymbol x_{min})}\right] - \log |\mathcal{X}| \rightarrow \infty
\end{align*}
as \(N \rightarrow \infty\) by the definition of an unstable FSFS model
(cf.~Definition \ref{def:instabFSFS}). Consequently,
\(P_{\boldsymbol \theta_N}(M_{\epsilon, \boldsymbol \theta_N}) \rightarrow 1\)
as \(N \rightarrow \infty\) as claimed. \hfill \(\Box\)

\clearpage

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\hypertarget{ref-besag1974spatial}{}
Besag, Julian. 1974. ``Spatial Interaction and the Statistical Analysis
of Lattice Systems.'' \emph{Journal of the Royal Statistical Society.
Series B (Methodological)}. JSTOR, 192--236.

\hypertarget{ref-handcock2003assessing}{}
Handcock, Mark S. 2003. ``Assessing Degeneracy in Statistical Models of
Social Networks.'' Center for Statistics; the Social Sciences,
University of Washington. \url{http://www.csss.washington.edu/}.

\hypertarget{ref-hinton2006fast}{}
Hinton, Geoffrey E, Simon Osindero, and Yee-Whye Teh. 2006. ``A Fast
Learning Algorithm for Deep Belief Nets.'' \emph{Neural Computation} 18
(7). MIT Press: 1527--54.

\hypertarget{ref-neal1992connectionist}{}
Neal, Radford M. 1992. ``Connectionist Learning of Belief Networks.''
\emph{Artificial Intelligence} 56 (1). Elsevier: 71--113.

\hypertarget{ref-pearl985bayesian}{}
Pearl, Judea. 1985. ``Bayesian Networks: A Model of Self-Activated
Memory for Evidential Reasoning.'' UCLA Computer Science Department.

\hypertarget{ref-ruelle1999statistical}{}
Ruelle, D. 1999. \emph{Statistical Mechanics: Rigorous Results}. London:
Imperial College Press.

\hypertarget{ref-salakhutdinov2009deep}{}
Salakhutdinov, Ruslan, and Geoffrey E Hinton. 2009. ``Deep Boltzmann
Machines.'' In \emph{International Conference on Artificial Intelligence
and Statistics}, 448--55. AI \& Statistics.

\hypertarget{ref-schweinberger2011instability}{}
Schweinberger, Michael. 2011. ``Instability, Sensitivity, and Degeneracy
of Discrete Exponential Families.'' \emph{Journal of the American
Statistical Association} 106 (496). Taylor \& Francis: 1361--70.

\hypertarget{ref-smolensky1986information}{}
Smolensky, Paul. 1986. ``Information Processing in Dynamical Systems:
Foundations of Harmony Theory.'' DTIC Document.

\hypertarget{ref-wasserman1994social}{}
Wasserman, Stanley, and Katherine Faust. 1994. \emph{Social Network
Analysis: Methods and Applications}. Vol. 8. Cambridge: Cambridge
University Press.

\end{document}
