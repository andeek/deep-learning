\clearpage

# (APPENDIX) Appendix {-}


# Proofs of instability results {#appendix-instab}

**Proof of Theorem \@ref(thm:instab-elpr).** For part (i), we prove the contrapositive, supposing that $\DN(\ak{\thetaN}) \le C$ holds for some $C > 0$ and show $\REP(\ak{\thetaN}) \leq NC$. Let $\boldsymbol x_{min} \equiv \argmin\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\ak{\thetaN}}(\boldsymbol x)$
and $\boldsymbol x_{max} \equiv \argmax\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\ak{\thetaN}}(\boldsymbol x)$. Note there exists a sequence $\boldsymbol x_{min} \equiv \boldsymbol x_0, \boldsymbol x_1, \dots, \boldsymbol x_k \equiv \boldsymbol x_{max}$ in $\mathcal{X}^N$ of component-wise switches to move from $\boldsymbol x_{min}$ to $\boldsymbol x_{max}$ in the sample space (i.e. $\boldsymbol x_i, \boldsymbol x_{i + 1} \in \mathcal{X}^N$ differ in exactly $1$ component, $i = 0, \dots, k$) for some integer $k \in \{0, 1, \dots, N\}$. Under the FOES model, recall
$P_{\ak{\thetaN}}(\boldsymbol x) > 0$ holds so that $\log P_{\ak{\thetaN}}(\boldsymbol x)$ is well-defined for each outcome $\boldsymbol x \in \mathcal{X}^N$. Then, if $k > 0$, it follows that
\begin{align*}
\REP(\ak{\thetaN}) = \log\left[\frac{P_{\ak{\thetaN}}(\boldsymbol x_{max})}{P_{ \ak{\thetaN}}(\boldsymbol x_{min})}\right] &= \left|\sum\limits_{i = 1}^k\log\left(\frac{P_{\ak{\thetaN}}(\boldsymbol x_i)}{P_{\ak{\thetaN}}(\boldsymbol x_{i-1})}\right)\right| \\
&\le \sum\limits_{i = 1}^k\left|\log\left(\frac{P_{\ak{\thetaN}}(\boldsymbol x_i)}{P_{\boldsymbol \theta}(\boldsymbol x_{i-1})}\right)\right| \le k \Delta_N(\ak{\thetaN}) \le NC,
\end{align*}
using $k \le N$ and $\Delta(\ak{\thetaN}) \le C$. If $k = 0$, then $\boldsymbol x_{max} = \boldsymbol x_{min}$ and the same bound above holds. This establishes part (i). To show part (ii), note the definition of S-instability (i.e., $\lim_{N\to \infty}\REP(\ak{\thetaN})/N= \infty$) combined with part (i) implies that $\lim_{N\to \infty}\DN(\ak{\thetaN})=\infty$. \hfill $\Box$

**Proof of Theorem \@ref(thm:degenFOES).** As $|\mathcal{X}|<\infty$ holds in the FOES model, we may suppose
$|\mathcal{X}|>1$; otherwise, $\mathcal{X}^N$ has one outcome and the model is  trivially degenerate for all $N \geq 1$. Fix $0 < \epsilon < 1$ and write $\boldsymbol x_{min} \equiv \argmin\limits_{\boldsymbol x \in \mathcal{X}^N}P_{ \ak{\thetaN}}(\boldsymbol x)$ and $\boldsymbol x_{max} \equiv \argmax\limits_{\boldsymbol x \in \mathcal{X}^N}P_{\ak{\thetaN}}(\boldsymbol x)$. Then, $\boldsymbol x_{max} \in M_{\epsilon, \ak{\thetaN}}$, so $P_{\ak{\thetaN}}(M_{\epsilon, \ak{\thetaN}}) \ge P_{ \ak{\thetaN}}(\boldsymbol x_{max}) > 0$. If $\boldsymbol x \in \mathcal{X}^N \setminus M_{\epsilon, \ak{\thetaN}}$, then by definition
$P_{\ak{\thetaN}}(\boldsymbol x) \le [P_{\ak{\thetaN}}(\boldsymbol x_{max})]^{1-\epsilon}[P_{\ak{\thetaN}}(\boldsymbol x_{min})]^{\epsilon}$ holds so that
$$
1-P_{\ak{\thetaN}}(M_{\epsilon, \ak{\thetaN}})
 = \sum\limits_{\boldsymbol x \in \mathcal{X}^N \setminus M_{\epsilon, \ak{\thetaN}}}P_{\ak{\thetaN}}(\boldsymbol x)
  \le (|\mathcal{X}|^N)[P_{\ak{\thetaN}}(\boldsymbol x_{max})]^{1-\epsilon}[P_{ \ak{\thetaN}}(\boldsymbol x_{min})]^{\epsilon}.
$$
From the lower bound on $P_{\ak{\thetaN}}(M_{\epsilon, \ak{\thetaN}})$ and the
upper bound on $1-P_{\ak{\thetaN}}(M_{\epsilon, \ak{\thetaN}})$, it follows that
\begin{align*}
\frac{1}{N}\log\left[\frac{P_{\ak{\thetaN}}(M_{\epsilon, \ak{\thetaN}})}{1-P_{ \ak{\thetaN}}(M_{\epsilon, \ak{\thetaN}})}\right] & \ge \frac{1}{N} \log\left[\frac{P_{\ak{\thetaN}}(\boldsymbol x_{max})}{(|\mathcal{X}|^N)[P_{ \ak{\thetaN}}(\boldsymbol x_{max})]^{1-\epsilon}[P_{\ak{\thetaN}}(\boldsymbol x_{min})]^{\epsilon}}\right] \\
&= \frac{\epsilon}{N} \log\left[\frac{P_{\ak{\thetaN}}(\boldsymbol x_{max})}{P_{ \ak{\thetaN}}(\boldsymbol x_{min})}\right] - \log |\mathcal{X}| \rightarrow \infty
\end{align*}
as $N \rightarrow \infty$ by the definition of an S-unstable FOES model \@ref(eq:Sun). Consequently, $P_{\ak{\thetaN}}(M_{\epsilon, \ak{\thetaN}}) \rightarrow 1$ as $N \rightarrow \infty$ as claimed. \hfill $\Box$

**Proof of Corollary \@ref(cor:sign).** The model condition PSR implies that
\begin{equation}
(\#eq:R)
\frac{\max_{\boldsymbol y \in \mathcal{X}^N}P_{\ak{\thetaN}}(\boldsymbol y)  }{\min_{\boldsymbol y \in \mathcal{X}^N}P_{\ak{\thetaN}}(\boldsymbol y) } = \frac{\max_{\boldsymbol y \in \mathcal{X}^N}P_{-\ak{\thetaN}}(\boldsymbol y) }{\min_{\boldsymbol y \in \mathcal{X}^N}P_{-\ak{\thetaN}}(\boldsymbol y) }
\end{equation}
so that the log-ratio $\REP(\ak{\thetaN})=\REP(-\ak{\thetaN})$ is the same for both $\ak{\thetaN}$ and $-\ak{\thetaN}$ in \@ref(eq:elpr). Now part (i) of Corollary \@ref(cor:sign) follows from $\REP(\ak{\thetaN})/N=\REP(-\ak{\thetaN})/N\to \infty$ as $N\to \infty$ in \@ref(eq:Sun). To show part (ii), fix $0 < \epsilon < 1$ and consider a $\epsilon$-mode set $\mathcal{M}_{\epsilon, \ak{\thetaN}}$ under $\ak{\thetaN}$ from \@ref(eq:mode). If $\boldsymbol x \in \mathcal{M}_{\epsilon, \ak{\thetaN}}^c \equiv \mathcal{X}^N \setminus \mathcal{M}_{\epsilon, \ak{\thetaN}}$, then, by definition,
$$
\frac{P_{\ak{\thetaN}}(\boldsymbol x)}{\min_{\boldsymbol y \in \mathcal{X}^N}P_{ \ak{\thetaN}}(\boldsymbol y)} \leq \left[\frac{\max_{\boldsymbol y \in \mathcal{X}^N}P_{\ak{\thetaN}}(\boldsymbol y)  }{\min_{\boldsymbol y \in \mathcal{X}^N}P_{\ak{\thetaN}}(\boldsymbol y) } \right]^{1-\epsilon}
$$
holds, which is equivalent to
$$
\frac{\max_{\boldsymbol y \in \mathcal{X}^N}P_{-\ak{\thetaN}}(\boldsymbol y)}{P_{-\ak{\thetaN}}(\boldsymbol x)} \leq
\left[\frac{\max_{\boldsymbol y \in \mathcal{X}^N}P_{-\ak{\thetaN}}(\boldsymbol y)  }{\min_{\boldsymbol y \in \mathcal{X}^N}P_{-\ak{\thetaN}}(\boldsymbol y) } \right]^{1-\epsilon}
$$
by model condition PSR and \@ref(eq:R). The latter is in turn equivalent to
\begin{equation}
(\#eq:R2)
\log P_{-\ak{\thetaN}}(\boldsymbol x) \geq \epsilon \max\limits_{\boldsymbol y \in \mathcal{X}^N} \log  P_{-\ak{\thetaN}}(\boldsymbol y) + (1-\epsilon)\min\limits_{\boldsymbol y \in \mathcal{X}^N} \log P_{-\ak{\thetaN}}(\boldsymbol y),
\end{equation}
so that $\boldsymbol x \in \mathcal{M}_{\epsilon, \ak{\thetaN}}^c$ if and only if \@ref(eq:R2) holds. Next consider the $(1-\epsilon)$-mode set $\mathcal{M}_{1-\epsilon, -\ak{\thetaN}}$ under $-\ak{\thetaN}$ from \@ref(eq:mode). If $\boldsymbol x \in\mathcal{M}_{1-\epsilon, -\ak{\thetaN}}$, then by definition $\boldsymbol x$ fulfills \@ref(eq:R2) and so $\boldsymbol x \in \mathcal{M}_{\epsilon, \ak{\thetaN}}^c$, showing that $\mathcal{M}_{1-\epsilon, -\ak{\thetaN}} \subset \mathcal{M}_{\epsilon, \ak{\thetaN}}^c$. By this and the fact that that Theorem \@ref(thm:degenFOES) and Corollary \@ref(cor:sign)(i) entail that $P_{-\ak{\thetaN}}(\boldsymbol X_N \in \mathcal{M}_{1-\epsilon, -\ak{\thetaN}})\to 1$ as $N\to \infty$ (i.e., $P_{-\ak{\thetaN}}$ is S-unstable), we have
$$
1  = \lim_{N\to \infty} P_{-\ak{\thetaN}}(\boldsymbol X_N \in \mathcal{M}_{1-\epsilon, -\ak{\thetaN}}) \leq \lim_{N\to \infty} P_{-\ak{\thetaN}}(\boldsymbol X_N \in \mathcal{M}_{\epsilon, \ak{\thetaN}}^c ) \leq 1,
$$
proving Corollary \@ref(cor:sign)(ii) \hfill $\Box$

**Proof of Proposition \@ref(prp:prop1).** For any two outcomes $\boldsymbol x_1, \boldsymbol x_2\in\mathcal{X}^N$, the log-ratio of probabilities from the linear exponential model \@ref(eq:expo) with $k$ parameters/statistics satisfies
$$
\left|\log \left[ \frac{P_{\ak{\thetaN}}(\boldsymbol x_1)}{P_{\ak{\thetaN}}(\boldsymbol x_2)}  \right] \right| =
\left|  \sum_{i=1}^k \theta_{i,\ak{\thetaidx}} [g_{i,k}(\boldsymbol x_1) - g_{i,k}(\boldsymbol x_2) ] \right|  \leq  \sum_{i=1}^k | \theta_{i,\ak{\thetaidx}}| (U_{i,N}-L_{i,N});
$$
consequently, $\REP(\ak{\thetaN} )  \leq  \sum_{i=1}^k | \theta_{i,\ak{\thetaidx}}| (U_{i,N}-L_{i,N})$ holds in \@ref(eq:elpr) and model stability in Proposition \@ref(prp:prop1) follows from \@ref(eq:Sun). \hfill $\Box$

**Proof of Proposition \@ref(prp:prop2).**
Writing $\boldsymbol x=(x_1,\ldots,x_N)$ and $\boldsymbol h = (h_1,\ldots,h_{N_{\mathcal{H}}})$ with all components $x_i,h_j\in\{\pm 1\}$, probabilities in the joint RBM model \@ref(eq:RBM1) can be written as $\tilde{P}_{\ak{\thetaN}} (\boldsymbol x, \boldsymbol h) = c(\ak{\thetaN})\exp[ f_{\ak{\thetaN}} (\boldsymbol x, \boldsymbol h)]$ in terms of the function $f_{\ak{\thetaN}} (\boldsymbol x, \boldsymbol h)$ from \@ref(eq:f) and the normalizing constant $c(\ak{\thetaN})= \exp [-\psi(\ak{\thetaN})]$ from \@ref(eq:RBM1). Let $\boldsymbol x_M, \boldsymbol x_m\in\{\pm 1\}^N$ be such that $P_{\ak{\thetaN}} (\boldsymbol x_M) = \max_{\boldsymbol x}P_{\ak{\thetaN}} (\boldsymbol x)$ and $P_{\ak{\thetaN}} (\boldsymbol x_m) = \min_{\boldsymbol x}P_{\ak{\thetaN}} (\boldsymbol x)$ under the marginal RBM model $P_{\ak{\thetaN}} (\boldsymbol x) = c(\ak{\thetaN})\sum_{\boldsymbol h \in\{\pm 1\}^{\mathcal{N}_H}} \tilde{P}_{\ak{\thetaN}} (\boldsymbol x, \boldsymbol h)= c( \ak{\thetaN})\sum_{\boldsymbol h \in\{\pm 1\}^{\mathcal{N}_H}} \exp[ f_{\ak{\thetaN}} (\boldsymbol x, \boldsymbol h)]$ from \@ref(eq:RBM2). Also, $\boldsymbol x_0,x_1\in\{\pm 1\}^N$ be such that $\max_{\boldsymbol h}f_{\ak{\thetaN}} (\boldsymbol x_0, \boldsymbol h)=\max_{\boldsymbol x}\max_{\boldsymbol h}f_{\ak{\thetaN}} (\boldsymbol x , \boldsymbol h)$ and $\max_{\boldsymbol h}f_{\ak{\thetaN}} (\boldsymbol x_1, \boldsymbol h)=\min_{\boldsymbol x}\max_{\boldsymbol h}f_{\ak{\thetaN}} (\boldsymbol x , \boldsymbol h)$. Then, Proposition \@ref(prp:prop2)(i) follows from $\REP_{(\boldsymbol X)}(\ak{\thetaN}) = \log[P_{\ak{\thetaN}} (\boldsymbol x_M) /P_{\ak{\thetaN}} (\boldsymbol x_m) ]$ and the lower/upper bounds on $P_{\ak{\thetaN}} (\boldsymbol x_M)$ and $P_{\ak{\thetaN}} (\boldsymbol x_m)$ as
$$
c(\ak{\thetaN}) \exp[\max_{\boldsymbol h}f_{\ak{\thetaN}} (\boldsymbol x_0 , \boldsymbol h)]
\leq P_{\ak{\thetaN}} (\boldsymbol x_0) \leq  P_{\ak{\thetaN}} (\boldsymbol x_M) \leq  2^{N_{\mathcal{H}}} c(\ak{\thetaN}) \exp[\max_{\boldsymbol x}\max_{\boldsymbol h}f_{\ak{\thetaN}} (\boldsymbol x, \boldsymbol h)]
$$
and
\begin{align*}
c(\ak{\thetaN}) \exp[\min_{\boldsymbol x}\max_{\boldsymbol h}f_{\ak{\thetaN}} (\boldsymbol x , \boldsymbol h)] &\leq c(\ak{\thetaN}) \exp[ \max_{\boldsymbol h}f_{\ak{\thetaN}} (\boldsymbol x_m , \boldsymbol h)] \\
&\leq P_{\ak{\thetaN}} (\boldsymbol x_m) \\
&\leq P_{\ak{\thetaN}} (\boldsymbol x_1) \\ 
&\leq 2^{N_{\mathcal{H}}} c(\ak{\thetaN}) \exp[ \max_{\boldsymbol h}f_{\ak{\thetaN}} (\boldsymbol x_1 , \boldsymbol h)]\\
&=2^{N_{\mathcal{H}}} c(\ak{\thetaN}) \exp[\min_{\boldsymbol x}\max_{\boldsymbol h}f_{\ak{\thetaN}} (\boldsymbol x , \boldsymbol h)]
\end{align*}

To prove Proposition \@ref(prp:prop2), we next expand the function $f_{\ak{\thetaN}} (\boldsymbol x, \boldsymbol h)$ from \@ref(eq:f) as
$$
f_{\ak{\thetaN}} (\boldsymbol x, \boldsymbol h)= \sum_{j=1}^{N_\mathcal{H}} h_j \theta_{j,\ak{\thetaidx}}^{\mathcal{H}}  +   \sum_{i=1}^N \left( \theta_{i,\ak{\thetaidx}}^{\mathcal{V}} + \sum_{j=1}^{N_\mathcal{H}}  h_j  \theta_{ij,\ak{\thetaidx}}^{\mathcal{VH}}\right)x_i=\sum_{i=1}^N x_i  \theta_{i,\ak{\thetaidx}}^{\mathcal{V}} +  \sum_{j=1}^{N_\mathcal{H}} \left( \theta_{j,\ak{\thetaidx}}^{\mathcal{H}}  + \sum_{i=1}^N  x_i  \theta_{ij,\ak{\thetaidx}}^{\mathcal{VH}}\right)h_j.
$$
By this and the fact that $x_i,h_j\in\{\pm 1\}$, we then have
\begin{align}
\nonumber \max_{\boldsymbol x}  f_{\ak{\thetaN}} (\boldsymbol x, \boldsymbol h) =  \sum_{j=1}^{N_\mathcal{H}} h_j \theta_{j,\ak{\thetaidx}}^{\mathcal{H}}  +   a_{\ak{\thetaN}, \mathcal{H}} (\boldsymbol h), & \min_{\boldsymbol x}  f_{\ak{\thetaN}} (\boldsymbol x, \boldsymbol h) =  \sum_{j=1}^{N_\mathcal{H}} h_j \theta_{j,\ak{\thetaidx}}^{\mathcal{H}}  - a_{\ak{\thetaN}, \mathcal{H}} (\boldsymbol h),\\
\nonumber \max_{\boldsymbol h}  f_{\ak{\thetaN}} (\boldsymbol x, \boldsymbol h) = \sum_{i=1}^{N}x_i \theta_{i,\ak{\thetaidx}}^{\mathcal{V}} + b_{\ak{\thetaN}, \mathcal{V}} (\boldsymbol x), & \min_{\boldsymbol h}  f_{\ak{\thetaN}} (\boldsymbol x, \boldsymbol h) = \sum_{i=1}^{N}x_i \theta_{i,\ak{\thetaidx}}^{\mathcal{V}}  -   b_{\ak{\thetaN}, \mathcal{V}} (\boldsymbol x), \\ 
(\#eq:max)
a_{\ak{\thetaN}, \mathcal{H}} (\boldsymbol h) \equiv \sum_{i=1}^N \left| \theta_{i,\ak{\thetaidx}}^{\mathcal{V}} + \sum_{j=1}^{N_\mathcal{H}}  h_j  \theta_{ij,\ak{\thetaidx}}^{\mathcal{VH}}\right|, &  b_{\ak{\thetaN}, \mathcal{V}} (\boldsymbol x) \equiv \sum_{j=1}^{N_\mathcal{H}} \left| \theta_{j,\ak{\thetaidx}}^{\mathcal{H}}  + \sum_{i=1}^N  x_i  \theta_{ij,\ak{\thetaidx}}^{\mathcal{VH}}\right|,
\end{align}
where  $\boldsymbol h^T \ak{\thetaN}^{\mathcal{H}}=\sum_{j=1}^{N_\mathcal{H}} h_j \theta_{j,\ak{\thetaidx}}^{\mathcal{H}}$, $\boldsymbol x^T \ak{\thetaN}^{\mathcal{V}}=  \sum_{i=1}^{N}x_i \theta_{i,\ak{\thetaidx}}^{\mathcal{V}}$ and $\Gam\equiv \max_{\boldsymbol h} a_{\ak{\thetaN}, \mathcal{H}} (\boldsymbol h)$.
From this, it follows that
\begin{align*}
\REP_{(\boldsymbol X, \boldsymbol H)}(\ak{\thetaN}) &=  \max_{\boldsymbol h}\max_{\boldsymbol x}f_{\ak{\thetaN}} (\boldsymbol x , \boldsymbol h) -  \min_{\boldsymbol h}\min_{\boldsymbol x}f_{\ak{\thetaN}} (\boldsymbol x , \boldsymbol h)\\
&= \max_{\boldsymbol h_1 }  \max_{\boldsymbol h_2 }\left[ (\boldsymbol h_1 - \boldsymbol h_2)^T  \ak{\thetaN}^{\mathcal{H}}  +   a_{\ak{\thetaN}, \mathcal{H}} (\boldsymbol h_1)  + a_{\ak{\thetaN}, \mathcal{H}} (\boldsymbol h_2)\right],
\end{align*}
which leads to the upper bound $\REP_{(\boldsymbol X, \boldsymbol H)}(\ak{\thetaN}) \leq 2 \Gam + 2 | \ak{\thetaN}^{\mathcal{H}} |_1$. Then, taking $\boldsymbol h_1=\boldsymbol h_2$ (i.e., before maximization) gives $\REP_{(\boldsymbol X, \boldsymbol H)}(\ak{\thetaN}) \geq 2\Gam$ and taking $\boldsymbol h_1=-\boldsymbol h_2$, such that $\boldsymbol h_1^T \ak{\thetaN}^{\mathcal{H}} = |\ak{\thetaN}^{\mathcal{H}}|_1$, gives $\REP_{(\boldsymbol X, \boldsymbol H)}(\ak{\thetaN}) \geq 2|\ak{\thetaN}^{\mathcal{H}}|_1$; this yields the lower bound $\REP_{(\boldsymbol X, \boldsymbol H)}(\ak{\thetaN})\geq 2\max\{\Gam, |\ak{\thetaN}^{\mathcal{H}} |_1\}$.

We next consider $\elt$ and, by \@ref(eq:max), write
\begin{align*}
\elt &=  \max_{\boldsymbol h}\max_{\boldsymbol x}f_{\ak{\thetaN}} (\boldsymbol x , \boldsymbol h) -  \max_{\boldsymbol h}\min_{\boldsymbol x}f_{\ak{\thetaN}} (\boldsymbol x , \boldsymbol h)\\
&= \max_{\boldsymbol h_1} \min_{\boldsymbol h_2 }\left[ (\boldsymbol h_1 - \boldsymbol h_2)^T \ak{\thetaN}^{\mathcal{H}} + a_{\ak{\thetaN}, \mathcal{H}} (\boldsymbol h_1)  + a_{\ak{\thetaN}, \mathcal{H}} (\boldsymbol h_2)\right].
\end{align*}
Taking $\boldsymbol h_1=\boldsymbol h_2$ and maximizing over both $\boldsymbol h_1,\boldsymbol h_2$ produces the upper bound $\elt \leq 2\Gam$. Then, using $(\boldsymbol h_1 - \boldsymbol h_2)^T \ak{\thetaN}^{\mathcal{H}} + a_{\ak{\thetaN}, \mathcal{H}} (\boldsymbol h_2) \geq - 2|\ak{\thetaN}^{\mathcal{H}} |_1$ and maximizing over $\boldsymbol h_1$ gives $\elt \geq  \Gam- 2|\ak{\thetaN}^{\mathcal{H}} |_1$, while setting $\boldsymbol h_1=\boldsymbol h_2^*$ for $\boldsymbol h_2^*$ such that $-(\boldsymbol h_2^*) ^T  \ak{\thetaN}^{\mathcal{H}}  + a_{\ak{\thetaN}, \mathcal{H}} (\boldsymbol h_2^*) = \min_{\boldsymbol h_2} [-\boldsymbol h_2^T \ak{\thetaN}^{\mathcal{H}}  + a_{\ak{\thetaN}, \mathcal{H}} (\boldsymbol h_2)]$ gives $\elt \geq 2 a_{\ak{\thetaN}, \mathcal{H}} (\boldsymbol h_2^*) \geq \Gamc \equiv \min_{\boldsymbol h}  a_{ \ak{\thetaN}, \mathcal{H}} (\boldsymbol h)$. Finally, note that for any $\boldsymbol h$, the triangle inequality gives
\begin{align*}
\Gam  \equiv \max_{\boldsymbol h_1}  a_{\ak{\thetaN}, \mathcal{H}} (\boldsymbol h_1) &\geq
[a_{\ak{\thetaN}, \mathcal{H}} (\boldsymbol h)  + a_{\ak{\thetaN}, \mathcal{H}} (-\boldsymbol h)]/2 \\
&=
2^{-1}\sum_{i=1}^{N } \left(\left| \theta_{i,\ak{\thetaidx}}^{\mathcal{V}}   + \sum_{j=1}^{N_{\mathcal{H}}} h_j \theta_{ij,\ak{\thetaidx}}^{\mathcal{VH}} \right|
+ \left| \theta_{i,\ak{\thetaidx}}^{\mathcal{V}}   - \sum_{j=1}^{N_{\mathcal{H}}} h_j \theta_{ij,\ak{\thetaidx}}^{\mathcal{VH}} \right|\right)\\
& \geq \sum_{i=1}^{N }  \left| \theta_{i,\ak{\thetaidx}}^{\mathcal{V}} \right| \equiv | \ak{\thetaN}^{\mathcal{V}}|_1.
\end{align*}
\hfill $\Box$


**Proof of Theorem \@ref(thm:something).**  Let $L_{\ak{\thetaN}}(\boldsymbol X) = \log[ P_{ \ak{\thetaN}}(\boldsymbol X)/ \min_{\boldsymbol y \in \mathcal{X}^N} P_{\ak{\thetaN}}(\boldsymbol y) ]/\REP(\ak{\thetaN})$, where again $\boldsymbol X=(X_1, \dots, X_N)$ and $\REP(\ak{\thetaN})= \log[\max_{\boldsymbol y \in \mathcal{X}^N} P_{\ak{\thetaN}}(\boldsymbol y)/ \min_{\boldsymbol y \in \mathcal{X}^N} P_{ \ak{\thetaN}}(\boldsymbol y) ]$. As $L_{\ak{\thetaN}}(\boldsymbol X)\in[0,1]$, convergence of $L_{ \ak{\thetaN}}(\boldsymbol X)$ to 1 in probability under $P_{\ak{\thetaN}}$ is equivalent to convergence to $1$ in expectation under $P_{\ak{\thetaN}}$ (i.e., convergence in expectation implies probabilistic convergence by Markov's inequality while probabilistic convergence implies convergence in expectation by uniform integrability/boundedness).

For $\epsilon \in(0,1)$, let $\mathcal{M}_{\epsilon, \ak{\thetaN}}$ denote a modal set as in \@ref(eq:mode).  By Theorem \@ref(thm:degenFOES), $P_{\ak{\thetaN}}\left( \boldsymbol X\in \mathcal{M}_{\epsilon, \ak{\thetaN}}\right) \rightarrow 1$ holds as $N \rightarrow \infty$ and, by definition of \@ref(eq:mode), $\boldsymbol X\in \mathcal{M}_{\epsilon, \ak{\thetaN}}$ follows if and only if
$1-L_{\ak{\thetaN}}(\boldsymbol X)<\epsilon$. Hence, $L_{\ak{\thetaN}}(\boldsymbol X) \stackrel{p,\mathrm{E}}{\longrightarrow} 1$ holds under $\ak{\thetaN}$ in Theorem \@ref(thm:something). The convergence $L_{-\ak{\thetaN}}(\boldsymbol X) \stackrel{p,\mathrm{E}}{\longrightarrow} 1$  under $-\ak{\thetaN}$ likewise follows from Corollary \@ref(cor:sign). \hfill $\Box$

```{remark, echo = TRUE}
Consider the exponential graph model from \@ref(eq:mod2) involving counts of edges, 2-stars and triangles with fixed parameters $\thetaN = (\theta_{1},\theta_2,\theta_3)^\prime \in \mathbb{R}^3$ for $N\geq 1$. This model is unstable whenever $|\theta_2| + |\theta_3| \neq 0$. To see this, consider an even number $n>2$ of nodes and let $\boldsymbol x_0$ denote the data outcome in $\mathcal{X}^N \equiv \{0,1\}^N$ with all $N = {n \choose 2}$ edges being zero, let $\boldsymbol x_1$ denote the outcome with all edges being 1, and let $\boldsymbol x_2$ denote the edge configuration from dividing the nodes into two equal groups, with no edges within a group and all edges between the groups (so that no triangles exist in $\boldsymbol x_2$). Then, the $N$-scaled  log-ratio \@ref(eq:elpr) for the exponential graph model \@ref(eq:mod2) can, by definition, be bounded below by
\begin{align*}
\frac{1}{N}\REP_N(\ak{\thetaN}) &\geq \max_{i=1,2}\frac{1}{N}
\left| \log\left[ \frac{P_{\ak{\thetaN}}(\boldsymbol x_i)}{P_{\ak{\thetaN}}(\boldsymbol x_0)}\right] \right| \\
&= (n-2) \max\left\{ \left| \theta_2 + \frac{\theta_3}{3}+\frac{\theta_1}{n-2} \right|, \frac{n}{4(n-1)} \left| \theta_2 + \frac{8\theta_1}{n-2} \right| \right\};
\end{align*}
a similar expression also holds for an odd node number $n>2$. Consequently, for all fixed parameters excluding $\theta_2=\theta_3=0$, $\lim_{N\to \infty}\REP_N(\ak{\thetaN})/N=\infty$ then follows and the graph model with 2-stars and triangles is S-unstable, as suggested by the breach of Proposition \@ref(prp:prop1) for this model when $|\theta_2|+|\theta_3|\neq 0$.
```

```{remark, echo=TRUE}
Let $M \geq 1$ denote a possible number of replications and consider data $\boldsymbol Y_{N,M} \equiv (\boldsymbol X^{(1)}_N, \dots, \boldsymbol X^{(M)}_N)$ formed by $\{ \boldsymbol X^{(j)}_N\}_{j=1}^M$ as $M$  iid replications of a random vector $\boldsymbol X_N=(X_1,\ldots,X_N)$, where the latter follows a FOES model with probabilities $P_{\ak{\thetaN}}(\boldsymbol x)>0$, $\boldsymbol x\in\mathcal{X}^N$. This leads to a joint model, say $P_{\ak{\thetaN}}(\boldsymbol y)$, $\boldsymbol y\in\mathcal{X}^{NM}$, for $\boldsymbol Y_{N,M}$ consisting of $N*M$ random variables in total. Then, the LREP for $\boldsymbol Y_{N,M}$, scaled by associated size, is given by
\begin{align*}
\frac{1}{NM}\REP_{\boldsymbol Y_{N,M}}(\ak{\thetaN} ) &\equiv \frac{1}{NM}\log\left[\frac{\max_{\boldsymbol y \in \mathcal{X}^{NM}}P_{\ak{\thetaN}}(\boldsymbol y)  }{\min_{\boldsymbol y \in \mathcal{X}^{NM}}P_{\ak{\thetaN}}(\boldsymbol y)} \right] \\
&= \frac{1}{NM}\log\left[\frac{\max_{\boldsymbol x \in \mathcal{X}^{N}}P_{\ak{\thetaN}}(\boldsymbol x)  }{\min_{\boldsymbol x \in \mathcal{X}^{N}}P_{\ak{\thetaN}}(\boldsymbol x)} \right]^M \equiv \frac{1}{N}\REP_{\boldsymbol X_{N}}(\ak{\thetaN} ),
\end{align*}
where $\REP_{\boldsymbol X_{N}}(\ak{\thetaN} )  \equiv \REP(\ak{\thetaN} )$ denotes the log-ratio of extremal probabilities for $\boldsymbol X_N$ defined from \@ref(eq:elpr). That is, due to iid properties, the sample-size corrected  LREP for $\boldsymbol Y_{N,M}$ equals the analog, $\REP(\ak{\thetaN} )/N$, from the underlying common data model for $\boldsymbol X_N$ alone, regardless of the level $M \geq 1$ of independent replication. Hence, the definition of an S-unstable  model is unaffected by independent replication. For computational purposes, this aspect also implies that if the original data $\boldsymbol X_N=(X_1,\ldots,X_N)$ in a FOES model consist of $N$ iid random variables, then the size-scaled log-ratio \@ref(eq:elpr)  may be calculated as
$$
\frac{1}{N}\REP( \ak{\thetaN} ) \equiv \frac{1}{N}\REP_{\boldsymbol X_{N}}( \ak{\thetaN} ) = \log\left[ \frac{\max_{ x \in \mathcal{X}}P_{\ak{\thetaN}}(X_1=x)} {\min_{ x \in \mathcal{X}}P_{\ak{\thetaN}}(X_1=x)} \right]
$$
based on the extremal probabilities of just one random variable $X_1$.
```

# Details on Centered Graph Example
\ak{To illustrate centered model parameterizations and n examination of stability in these, consider the two-star model (\ref{eq:mod2}) for the $N = {n \choose 2}$ edges in simple graph with $n$ nodes and binary edge-variables $(X_1,\ldots,X_N)\in\{0,1\}^N$. Here a common or standard parameterization in (\ref{eq:mod2}) leads to a conditional probability of ``$1$" for an edge $i$ as
$$
\mathrm{logit}[  P_{\thetaN}(X_{i} = 1 | X_j=x_j, j\neq i)=  \theta_1 + \theta_2 \sum_{j \in \mathcal{N}_i } x_j,$$ 
based on summing other edge observations $x_j$ in a neighborhood $\mathcal{N}_i = \{{s}_j: {s}_i \cap {s}_j \neq \emptyset\}$ to edge $i$ (i.e., edges $j$,  marked by pairs of graph vertices ${s}_{j} = \{ v_{j_1}, v_{j_2} \}$, that share a common vertex with edge $i$ marked by the vertex pair ${s}_i =\{v_{i_1}, v_{i_2}\}$).  In contrast, a centered conditional would yield 
$$
\mathrm{logit}[  P_{\thetaN}(X_{i} = 1 | X_j=x_j, j\neq i)=  \theta_1 +  \frac{\theta_2}{2(n-1)}\sum_{j \in \mathcal{N}_i} (x_{j}-\kappa),
$$
involving a parameter $\theta_1 \equiv \mathrm{logit}(\kappa)$ for $\kappa \in (0,1)$ and $2(n-2)$ as the size of $\mathcal{N}_i$ (cf. \citep{kaiser2009exploring}); the corresponding joint model would involve parameters $\theta_{1,\thetaidx}= \mathrm{logit}(\kappa) - \kappa\theta_2$
and $\theta_{2,\thetaidx} = \theta_2/(2(n-2))$ in (\ref{eq:mod2}). The purpose of the centerization is to have $\kappa \in(0,1)$ represent a model mean parameter (note $E X_i =\kappa$ is the edge proportion/probability under independence $\theta_2=0$), while a separate parameter $\theta_2$ (for dependence) modifies the conditional probability of ``$X_i=1$" up/down from $\kappa$, depending on neighbors $x_j=1$ or 0.  A similar interpretation does not hold in the uncentered model; see \citep{kaiser2009exploring} and \citep{casleton2017local} for a discussion of the centered parameterization in spatial and network modeling, where the intent is to improve parameter interpretation and help detect degeneracy (e.g., intuitively given by large dependence parameters $\theta_2$, so that $\kappa$ fails to correspond to the mean or the marginal proportion of $1$'s in data generations from the model).  However, from a different perspective using the model measures here, centered parameterizations also lead to models which are more stable over wider regions of the parameter space. To illustrate, in the standard/uncentered parameterization for a graph model (\ref{eq:mod2}) with only two-stars $\thetaN = (0,\theta_2,0)$, our measure $\REP(\thetaN)/N$ of model instability becomes $\REP(\thetaN)/N = |\theta_2| (n-2)$, which is unbounded as $N\to \infty$ for all non-zero parameters $\theta_2$ (i.e., all models with $\theta_2\neq 0$ are S-unstable).  However, in the centered parameterization  with only two-stars, corresponding to $\kappa=1/2$ and $\thetaN= 0.5*(-\theta_2,\theta_2/(n-2),0)$, the measure becomes
\begin{align*}
   \REP(\thetaN)/N  &= 0.5*|\theta_2|  \max_{x_i, y_i \in\{0,1\}}\left|  \frac{1}{N}\sum_{i=1}^N x_i \left( 1 - \frac{1}{2(n-2)} \sum_{j \in \mathcal{N}_i} x_j\right) - \frac{1}{N}\sum_{i=1}^N y_i \left( 1 - \frac{1}{2(n-2)} \sum_{j \in \mathcal{N}_i} y_j\right)\right| \\ 
   &\leq |\theta_2|,   
\end{align*}
which is bounded, for any fixed $\theta_2 \in\mathbb{R}$, as $N$ increases (i.e., note $\sum_{j \in \mathcal{N}_j} x_i/(2(n-2)) \in [0,1]$ is a conditional/neighborhood sample proportion while $\sum_{i=1}^N x_i/N \in[0,1]$ is a marginal proportion). This aspect owes to adjusting parameters by neighborhood sizes in centered conditional distributions, but centering also induces an additional effect of alternating signs in parameters (e.g., $\thetaN= 0.5*(-\theta_2,\theta_2/(n-2),0)$). The latter has been suggested in other contexts with exponential graph models for regulating degeneracy \citep[cf.][]{snijders2006new}.}   


\clearpage

