---
title: "On the instability and degeneracy of deep learning models"
shorttitle: "Instability of deep learning models"
shortauthor: "Kaplan, Nordman, and Vardeman"
author: 
  - name: Andee Kaplan
    corresponding: true
    address: Department of Statistical Science, Duke University, P.O. Box 90251, Durham, NC 27708-0251, USA
    email: andrea.kaplan@duke.edu
  - name: Daniel Nordman
    address: Department of Statistics, Iowa State University, 2438 Osborn Dr., Ames, IA 50011-1090, USA
    email: dnordman@iastate.edu
  - name: Stephen Vardeman
    address: Departments of Statistics and Industrial and Manufacturing Systems Engineering, Iowa State University, 2438 Osborn Dr., Ames, IA 50011-1090, USA
    email: vardeman@iastate.edu
site: bookdown::bookdown_site
output: 
  bookdown::pdf_book:
    includes:
      in_header: ../resources/latex/header2.tex
    toc: no
bibliography: "../resources/latex/rbm.bib"
github-repo: andeek/rbm
abstract: |
  A probability model exhibits instability if small changes in a data outcome result in large, and often unanticipated, changes in probability. This instability is a property of the probability \ak{model, given by a distributional form and a given configuration of parameters}. For correlated data structures found in several application areas, there is increasing interest in \ak{identifying} such sensitivity in model probability structure. We consider the problem of quantifying instability for general probability models defined on sequences of observations, where each sequence of length $N$ has a finite number of possible values \ak{that can be taken at each point}. A sequence of probability models results, indexed by $N$, \ak{and an associated parameter sequence,} that accommodates data of expanding dimension. Model instability is formally shown to occur when a certain log-probability ratio under such models grows faster than $N$. In this case, a one component change in the data sequence can shift probability by orders of magnitude. Also, as instability becomes more extreme, the resulting probability models are shown to tend to degeneracy, placing all their probability on potentially small portions of the sample space. These results on instability apply to large classes of models commonly used in random graphs, network analysis, and machine learning contexts.
keywords: Degeneracy, Instability, Classification, Deep Learning, Graphical Models
---

```{r setup, include=FALSE}
# https://github.com/rstudio/bookdown/pull/374
fix_envs <- function(x) {
  beg_reg <- '^\\s*\\\\begin\\{.*\\}'
  end_reg <- '^\\s*\\\\end\\{.*\\}'
  i3 = if (length(i1 <- grep(beg_reg, x))) (i1 - 1)[grepl("^\\s*$", x[i1 - 1])]
  i3 = c(i3, if (length(i2 <- grep(end_reg, x))) (i2 + 1)[grepl("^\\s*$", x[i2 + 1])])
  if (length(i3)) x = x[-i3]
  x
}


options(
  dplyr.print_min = 6, dplyr.print_max = 6, width = 70,
  digits = 3,
  bookdown.post.latex = fix_envs
)
knitr::opts_chunk$set(
  comment = "#>",
  warning = FALSE,
  message = FALSE,
  cache = TRUE,
  echo = FALSE
)

ggplot2::theme_set(ggplot2::theme_bw(base_family="serif"))

library(knitr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(purrr)
library(scales)
library(xtable)
```
