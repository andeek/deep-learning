---
title: "Degeneracy in a restricted Boltzmann machine"
author: "Andee Kaplan, Stephen Vardeman, Daniel Nordman"
date: "January 8, 2016"
output: 
  pdf_document:
    fig_caption: yes
    includes:
      in_header: header.tex
    number_sections: yes
bibliography: ../references/refs.bib
---

```{r libraries_options, echo=FALSE, message=FALSE, warnings=FALSE}
library(knitr)
library(ggplot2)
library(dplyr)
library(tidyr)

opts_chunk$set(echo=FALSE, message=FALSE, warnings=FALSE)
theme_set(theme_bw(base_family="serif"))

source("../helpers/functions.R")
```

#Restricted Boltzmann Machines

A restricted Boltzmann machine (RBM) is a specific undirected graphical model specified for discrete random variables, binary being the most common. The architecture of an RBM is specified as having two layers, a hidden ($\mathcal{H}$) and a visible layer ($\mathcal{V}$), with no connections within a layer. An example of this structure is visualized in figure \ref{fig:rbm} with the hidden nodes indicated by gray circles and the visible nodes indicated by white circles. 

\begin{figure}[ht]
  \centering
  \resizebox{\linewidth}{!}{\input{tikz/rbm.tikz}}
  \caption{An example restricted Boltzmann machine (RBM), which consists of two layers, a hidden ($\mathcal{H}$) and a visible layer ($\mathcal{V}$), with no connections within a layer. Hidden nodes indicated by gray circles and the visible nodes indicated by white circles.}
  \label{fig:rbm}
\end{figure}

A common use for the RBM is to create features for use in classification. For example, a binary image could be classified where the pixel values would be considered the visible variables $v_i$.

##Joint Distribution
Let $\boldsymbol x = \{h_1, \dots, h_H, v_1,\dots,v_V\}$ represent the states of the visible and hidden nodes in an RBM. Then we posit a parametric form for probabilities corresponding to the state of each node taking the value of $1$.

\begin{align}
\label{eqn:pmf}
f_{\boldsymbol \theta} (\boldsymbol x) = \frac{\exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}{\sum\limits_{\boldsymbol x \in \mathcal{X}}\exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)} 
\end{align}

Let the normalizing constant in equation \ref{eqn:pmf} be denoted $\gamma(\boldsymbol \theta)$ [@Vardeman602, pp.163-167]. Additionally, we can refer to $Q(\boldsymbol x) = \sum\limits_{i = 1}^H\sum\limits_{j=1}^V \theta_{ij} h_i v_j + \sum\limits_{i = 1}^H\theta_{hi} h_i + \sum\limits_{j = 1}^V\theta_{vj} v_j$ as the neg-potential function in our model. The RBM model is parameterized by $\boldsymbol \theta$, which contains two types of parameters, main effects and interaction effects. The main effects parameters ($\theta_{v_i}, \theta_{h_j}$) weight to the values of the visible and hidden nodes and the interaction effects parameters ($\theta_{ij}$) weight the values of the connections, or dependencies, between hidden and visible layers.

Due to the potential size of the model the normalizing constant $\gamma(\boldsymbol \theta)$ can be infeasible to calculate, making it impossible to estimate the model exactly. An approximation can be achieved using Gibbs sampling, which is a natural fit due to the restricted architecture of the RBM. Specifically, the conditional independence of each layer given the other allows for simulation of the main and hidden layers.

##Connection to Deep Learning
RBMs have risen to prominence in recent years due to their connection to deep learning [see @hinton2006fast, @salakhutdinov2012efficient, or @srivastava2013modeling for examples]. By stacking multiple layers of RBMs in a deep architecture, the method's proponents claim the ability to learn "internal representations that become increasingly complex, which is considered to be a promising way of solving object and speech recognition problems" [@salakhutdinov2009deep, pp. 450]. The stacking is achieved by treating a hidden layer of one RBM as the visible layer in a second RBM, and so on until the desired architecture is created.

#Degeneracy in Random Graph Models
In Random Graph Model theory, \emph{degeneracy} occurs when there is a disproportionate amount of probability placed on only a few elements of the sample space, $\mathcal{X}$, by the model. We consider three related definitions of degeneracy as they relate to a graphical model. Section \ref{mean-parameterization} presents a definition from @handcock2003assessing based on the mean parameterization of the model parameters, section \ref{neg-potential} presents a definition from @schweinberger2011instability based on the neg-potential function, and section \ref{conditional-expectation} presents a definition from @kaiser2006markov that involves the difference between expectations of the model and expectations under independence.

##Mean Parameterization
@handcock2003assessing define a model to be near degenerate if $\mu(\boldsymbol \theta)$, the mean parametrization on the model parameters, is close to the boundary of the convex hull of $\{t_k(\boldsymbol x): \boldsymbol x \in \mathcal{X}, k = 1, \dots, K\}$, where $t_k(\boldsymbol x), k = 1, \dots, K\}$ is the set of statistics in the neg-potential function $Q(\boldsymbol x)$ from equation \ref{eqn:pmf}.

The mean parameterization on the model parameters, $\mu(\boldsymbol \theta)$, for the RBM is defined as
\begin{align*}
\mu(\boldsymbol \theta) &= \text{E}_{\boldsymbol \theta}\left[ t(\boldsymbol X) \right] \\
&= \sum\limits_{x \in \mathcal{X}} \left\{ t(\boldsymbol x) f_{\boldsymbol \theta} (\boldsymbol x) \right\} \\
&= \sum\limits_{x \in \mathcal{X}} \left\{ t(\boldsymbol x)\dfrac{\exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}{\sum\limits_{\boldsymbol x \in \mathcal{X}}\exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}\right\} \\
\end{align*}

##Neg-potential
@schweinberger2011instability introduces a related concept, instability, which  is characterized
by excessive sensitivity and near-degeneracy. He provides the following tool for detecting instability in the a discrete exponential family model, and thus near-degeneracy.

\begin{defn}
A discrete exponential family distribution is unstable if for any $C > 0$, however large, there exists $N_C > 0$ such that 
\begin{align}
\max\limits_{\boldsymbol x_N \in \mathcal{X}_N}[Q(\boldsymbol x_N)] > CN \text{ for all } N > N_C \label{eq:unstable}
\end{align}
\end{defn}

\begin{lemma}
If a discrete exponential family distribution is unstable, then it is degenerate in the sense that for any $0 < \epsilon < 1$, $P_\theta(\boldsymbol X_N \in \mathcal{M}_{\epsilon, N}) \rightarrow 1$ as $N \rightarrow \infty$ where $\mathcal{M}_{\epsilon, N}$ is the subset of $\epsilon$-modes of the probability mass function.
\end{lemma}

In the context of the RBM, $N$ is the number of nodes in the model, $H + V$. So, the distribution is unstable if for any $C > 0$, 

\begin{align*}
\max\limits_{\boldsymbol x_N \in \mathcal{X}_N}[Q(\boldsymbol x_N)] &= \max\limits_{\boldsymbol x \in \mathcal{X}}\left[ \sum\limits_{i = 1}^H\sum\limits_{j=1}^V \theta_{ij} h_i v_j + \sum\limits_{i = 1}^H\theta_{hi} h_i + \sum\limits_{j = 1}^V\theta_{vj} v_j \right] \\
&= \max\limits_{h_j, v_i \in \{0,1\}}\left[ \sum\limits_{i = 1}^H\sum\limits_{j=1}^V \theta_{ij} h_i v_j + \sum\limits_{i = 1}^H\theta_{hi} h_i + \sum\limits_{j = 1}^V\theta_{vj} v_j \right] \\
&> C(H + V)
\end{align*}

There are $H + V + H*V$ terms in the sum that comprises $Q(\boldsymbol x)$, which leads to a natural split of the quantity into main effects, those terms based solely on $h_i$ and $v_j$ and interaction effects, those terms based on the product of $h_i v_j$. There are $H+V$ main effects terms and $H*V$ interaction effects terms. For $H$ and $V$ large enough, the number of interaction terms is growing much faster than $H + V$, meaning no matter the $C$, we are able to find values of $\boldsymbol theta$ that can make the left side of equation \ref{eq:unstable} large compared to $C(H+V)$. This points to the interaction terms as potentially the more dangerous parameters for causing near-degeneracy in the RBM.

```{r interaction size, include=FALSE}
expand.grid(H = 1:10, V = 1:10) %>%
  mutate(interaction_terms = H*V) %>%
  ggplot() +
  geom_contour(aes(H, V, z = interaction_terms/(H + V), colour = ..level..), bins = 20) +
  geom_contour(aes(H, V, z = interaction_terms/(H + V)), colour = "black", breaks = 1) +
  scale_colour_gradient("H*V/(H + V)", low = "yellow", high = "red")

```

##Expectation Differences
The characterization put forth by @kaiser2006markov of degeneracy in a model concerns the difference between model expectations and expectations given independence. Using this concept of dependence in a model, it is possible to understand what conditions lead to near-degeneracy in a model versus those that guarantee non-degenerate models. In this method, a model is considered without near-degeneracy if its expected value is not very different from the same model with assumed independence. The quantities to compare in the RBM case are

\begin{align*}
\text{E}\left[\boldsymbol X\right] &= \sum\limits_{\boldsymbol x \in \mathcal{X}} \boldsymbol x f_{\boldsymbol \theta}(\boldsymbol x) \\
&= \sum\limits_{\boldsymbol x \in \mathcal{X}} \boldsymbol x \frac{\exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}{\sum\limits_{\boldsymbol x \in \mathcal{X}}\exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)} \\
&= \frac{\sum\limits_{\boldsymbol x \in \mathcal{X}} x_k \exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}{\sum\limits_{\boldsymbol x \in \mathcal{X}}\exp\left(\sum\limits_{i = 1}^V \sum\limits_{j=1}^H \theta_{ij} v_i h_j + \sum\limits_{i = 1}^V\theta_{v_i} v_i + \sum\limits_{j = 1}^H\theta_{h_j} h_j\right)}  \qquad k = 1, \dots, H + V
\end{align*}




#Tiny example
To illustrate the idea of graph degeneracy in a RBM, we will consider a the smallest possible toy example that consists of one visible node $v_1$ and one hidden node $h_1$, both binary. A schematic of this model can be found in figure \ref{fig:toymodel}. 

\begin{figure}[ht]
  \centering
  \resizebox{1cm}{!}{\input{tikz/toymodel.tikz}}
  \caption{A small example restricted Boltzmann machine (RBM), which consists of two nodes, one hidden and one visible.}
  \label{fig:toymodel}
\end{figure}

##Degeneracy three ways

For this small model, we are able to illustrate the boundary of the convex hull of $\{t_k(\boldsymbol x): \boldsymbol x \in \mathcal{X}, k = 1, \dots, K\}$ because we are in three dimensions. In figure \ref{fig:toyhull}, the convex hull of our statistic space in three dimensions for the toy RBM with one visible and one hidden node from multiple perspectives enclosed by an unrestricted hull of $\{0,1\}^3$ space is shown. In this small model, $\{t(\boldsymbol x)\}$ is defined as $\{t(\boldsymbol x)\} = \{v_1, h_1, v_1 h_1\}$ by definition and form of the neg-potential function $Q(\boldsymbol x)$. Note that the convex hull of $\{t(\boldsymbol x)\}$ does not fill the unrestricted hull because of the relationship between the elements of $\{t(\boldsymbol x)\}$.

```{r models}
m1_bin <- calc_hull(1, 1)
m1_neg <- calc_hull(1, 1, type = "negative")
```

\begin{figure}[ht]
  \begin{minipage}{0.45\textwidth}
  \resizebox{\linewidth}{!}{
    \tdplotsetmaincoords{60}{-60}
    \input{tikz/toyhull_top.tikz}
    
    ```{r, results='asis', echo=FALSE, include=TRUE, message=FALSE}
    cat(sapply(1:nrow(m1_bin$c_hull$hull), function(x) paste0("\\draw", paste(paste0("(", apply(m1_bin$possible_t[cbind(m1_bin$c_hull$hull, m1_bin$c_hull$hull[,1])[x,],], 1, paste, collapse=","), ")"), collapse=" -- "), "; \n")))
   ```
    \end{tikzpicture}
  }
  \end{minipage}
  \begin{minipage}{0.45\textwidth}
  \resizebox{\linewidth}{!}{
    \tdplotsetmaincoords{60}{120}
    \input{tikz/toyhull_top.tikz}
    
    ```{r, results='asis', echo=FALSE, include=TRUE, message=FALSE}
    cat(sapply(1:nrow(m1_bin$c_hull$hull), function(x) paste0("\\draw", paste(paste0("(", apply(m1_bin$possible_t[cbind(m1_bin$c_hull$hull, m1_bin$c_hull$hull[,1])[x,],], 1, paste, collapse=","), ")"), collapse=" -- "), "; \n")))
    ```
    \end{tikzpicture}
  }
  \end{minipage}
  \caption{The convex hull of our statistic space in three dimensions for the toy RBM with one visible and one hidden node from multiple perspectives enclosed by an unrestricted hull of $\{0,1\}^3$ space. Note that the convex hull of $\{t(\boldsymbol x)\}$ does not fill the unrestricted hull because of the relationship between the elements of $\{t(\boldsymbol x)\}$.}
\label{fig:toyhull}
\end{figure}

Additionally, we can compute the mean parametrization of the model parameters as 
\begin{align*}
\mu(\boldsymbol \theta) &= \text{E}_{\boldsymbol \theta}\left[ t(X) \right] \\
&= \sum\limits_{x \in \mathcal{X}} \left\{ t(x) \frac{\exp\left( \theta_{11} h_1 v_1 + \theta_{h1} h_1 + \theta_{v1} v_1\right)}{\sum\limits_{\boldsymbol x \in \mathcal{X}}\exp\left( \theta_{11} h_1 v_1 + \theta_{h1} h_1 + \theta_{v1} v_1\right)} \right\} \\
&= \sum\limits_{x \in \mathcal{X}} \left\{ t(x) \frac{\exp\left( \theta_{11} h_1 v_1 + \theta_{h1} h_1 + \theta_{v1} v_1\right)}{\gamma(\boldsymbol \theta)} \right\} \\
&= \left[
\begin{matrix}
\frac{\exp\left(\theta_{v_1}\right) + \exp\left(\theta_{11} + \theta_{v_1} + \theta_{h_1}\right)}{\gamma(\boldsymbol \theta)} \\
\frac{\exp\left(\theta_{h_1}\right) + \exp\left(\theta_{11} + \theta_{v_1} + \theta_{h_1}\right)}{\gamma(\boldsymbol \theta)} \\
\frac{\exp\left(\theta_{11} + \theta_{v_1} + \theta_{h_1}\right)}{\gamma(\boldsymbol \theta)} \\
\end{matrix} \right]
\end{align*}
These three functions can be visualized over multiple values of $\boldsymbol \theta$, as seen in figure \ref{fig:toymodel}. By examining these functions, we can see that as values of $\boldsymbol \theta$ grow larger in magnitude, at least one mean-parameterization functions is approaching the values 0 or 1, indicating a potential to be near to the boundary of the convex hull of $\{t_k(\boldsymbol x): \boldsymbol x \in \mathcal{X}, k = 1, \dots, K\}$. Thus, for a very small example we can see the relationship between values of $\boldsymbol \theta$ and model degeneracy.

```{r toymodel, fig.cap="\\label{fig:toymodel}The three mean-parameterization functions of the model parameters for a RBM with one visible and one hidden node.", out.height = "4in", fig.align='center'}
possibles <- stats(1, 1, type = "binary")

expand.grid(theta_h1 = seq(-2, 2, length.out = 5),
            theta_v1 = seq(-2, 2, length.out = 5),
            theta_11 = seq(-2, 2, length.out = 11)) %>%
  data.frame() %>%
  group_by(theta_v1, theta_h1, theta_11) %>%
  do(data.frame(gamma = sum(exp(possibles %*% t(data.matrix(.)))),
                mu_v1 = possibles[,"v1"] %*% exp(possibles %*% t(data.matrix(.))),
                mu_h1 = possibles[,"h1"] %*% exp(possibles %*% t(data.matrix(.))),
                mu_11 = possibles[,"theta11"] %*% exp(possibles %*% t(data.matrix(.))))) %>%
  mutate_each(funs(./gamma), starts_with("mu")) %>%
  select(-gamma) %>%
  gather(mu_parameter, value, mu_v1, mu_h1, mu_11) %>%
  ungroup() %>%
  mutate(theta_h1 = factor(theta_h1, labels = paste0("theta[h1]==", unique(theta_h1))), 
         theta_v1 = factor(theta_v1, labels = paste0("theta[v1]==", unique(theta_v1)))) %>%
  ggplot() +
  geom_line(aes(theta_11, value, colour = mu_parameter)) +
  facet_grid(theta_h1 ~ theta_v1, labeller = label_parsed) +
  xlab(expression(theta[11])) +
  scale_colour_discrete(expression(mu(theta)), labels = c(expression(theta[v1]), 
                                                          expression(theta[h1]), 
                                                          expression(theta[11])))
  
  
```

##Variable Encoding
Multiple encodings of the binary variables are possible. For example, we could allow $\mathcal{H} \subset \{0,1\}^H, \mathcal{V} \subset \{0,1\}^V$, as in the previous sections or we could encode the state of the variables as $\mathcal{H} \subset \{-1,1\}^H, \mathcal{V} \subset \{-1,1\}^V$. This will result in $t(\boldsymbol x) \in \{0,1\}^{H + V + HV}$ or $t(\boldsymbol x) \in \{-1,1\}^{H + V + HV}$ depending on how we encode on and off states in the nodes. To explore the effect of this encoding on the potential near degeneracy of an RBM, we can explore the behavior of the ratio of the volume of the unrestricted convex hull to the restructed convex hull of $\{t(\boldsymbol x)\}$ to determine how much the restriction in our statistics by including the cross product terms incurs.

For the small two node example, the binary encoding loses `r round((1-m1_bin$c_hull$vol)*100,2)`% of the volume compared to unrestricted space and the negative encoding loses `r round((1-m1_neg$c_hull$vol/(2^3))*100,2)`% of the volume compared to unrestricted space. This notion of lost volume can be helpful to conceptualize how difficult it will be for the mean parameterized vector $\mu(\boldsymbol \theta)$ to avoid the boundary, and thus avoid near degeneracy. In fact, if we look at the ratio of volume within a convex hull versus the volume of an unrestricted hull, we can see a relationship emerge as the number of nodes (and thus parameters) increases. In figure \ref{fig:volume_plot}, it is evident that as the number of parameters increses, this ratio is decreasing at an increasing rate, meaning it gets more and more difficult to avoid the boundary of the convex hull and thus near degeneracy. Additionally, it appears that the $\{-1,1\}$ encoding suffers slightly less from this problem.

```{r volume_plot, echo=FALSE, fig.cap="\\label{fig:volume_plot}The relationship between volume within the convex hull of our statistics and the convex hull of unrestricted space for different configurations of nodes. We compare the two encodings for on and off in a node.", out.height = "3in", fig.align='center'}
test_cases <- expand.grid(data.frame(1:4)[,rep(1,2)]) %>% 
  rename(H = X1.4, V = X1.4.1) %>%
  filter(H <= V) %>% #remove those cases with less visibles than hiddens. Is this necessary?
  mutate(n_param = H*V + H + V) %>%
  mutate(max_facets = (2^(H+V))^(floor(n_param/2))) %>%
  filter(n_param <= 11) #calc_hull can't handle any higher dimensions currently

bin_res <- plyr::dlply(test_cases, plyr::.(n_param), function(x) calc_hull(x$V, x$H, "binary"))
neg_res <- plyr::dlply(test_cases, plyr::.(n_param), function(x) calc_hull(x$V, x$H, "negative"))

plyr::ldply(bin_res, function(x) x$c_hull$vol) %>% 
  mutate(frac_vol = V1/(1^n_param)) %>%
  inner_join(plyr::ldply(neg_res, function(x) x$c_hull$vol) %>% 
               mutate(frac_vol = V1/(2^n_param)),
             by="n_param") %>%
   rename(vol.bin = V1.x, vol.neg = V1.y, frac_vol.bin = frac_vol.x, frac_vol.neg = frac_vol.y) %>%
  gather(vars, value, -n_param) %>%
  separate(vars, c("type", "encoding"), "\\.") %>%
  spread(type, value) %>%
  ggplot() +
  geom_point(aes(x=n_param, y=frac_vol, colour=encoding)) +
  geom_line(aes(x=n_param, y=frac_vol, colour=encoding, group=encoding)) +
  ylab("Fraction of unrestricted volume") +
  xlab("Number of parameters") +
  scale_colour_discrete("Encoding", labels=c("Binary (1,0)", "Negative (1,-1)"))
```

Thus, moving forward we will restrict our examinations to the $\{-1, 1\}$-encoding of the binary variables in the RBM.

#Exploring Manageable Examples
```{r split_sample, cache=TRUE}
#reshape data function
plot_data <- function(res, grid = FALSE) {
  
  plot.data <- data.frame()
  
  if(!grid) {
    for(i in 1:nrow(res)) {
      tmp <- res$g_theta[[i]] %>% data.frame()
      H <- res[i,]$H
      V <- res[i,]$V
      N <- res[i,]$N
      r1 <- res[i,]$r1
      r2 <- res[i,]$r2
      C <- res[i,]$C
      epsilon <- res[i,]$epsilon
      
      
      tmp %>% 
        rowwise() %>% 
        mutate_(ss_interaction = paste0("(", paste(paste0(names(.)[(H + V + 1):(H + V + H*V)], "^2"), collapse = " + "), ")"),
                ss_main = paste0("sum(", paste(paste0(names(.)[1:(H+V)],"^2"), collapse = " + "), ")")) %>%    
        ungroup() -> ratio
      
      
      inner_join(ratio, res$outside[[i]] %>% ungroup()) %>%
        select(ss_interaction, ss_main, near_hull) %>%
        mutate(H = H, V = V, n_param = H + V + H*V, N = H + V, N = N, r1 = r1, r2 = r2, C = C, epsilon = epsilon) %>%
        rbind(plot.data) -> plot.data
      }
    } else {
      for(i in 1:nrow(res)) {
        tmp <- res$g_theta[[i]] %>% data.frame()
        H <- res[i,]$H
        V <- res[i,]$V
        N <- res[i,]$N
        r1 <- res[i,]$r1
        r2 <- res[i,]$r2
        
        tmp %>% 
          rowwise() %>% 
          mutate_(ss_interaction = paste0("(", paste(paste0(names(.)[(H + V + 1):(H + V + H*V)], "^2"), collapse = " + "), ")"),
                ss_main = paste0("sum(", paste(paste0(names(.)[1:(H+V)],"^2"), collapse = " + "), ")")) %>%  
          ungroup() -> ratio
        
        
        inner_join(ratio, res$outside[[i]] %>% ungroup()) %>%
          select(ss_interaction, ss_main, near_hull) %>%
          mutate(H = H, V = V, n_param = H + V + H*V, N = H + V, N = N, r1 = r1, r2 = r2) %>%
          rbind(plot.data) -> plot.data
      }
    }
  
  return(plot.data)
}

load("../data/results_split.RData")
plot_dat_split <- res %>% plot_data

load("../data/results_grid.RData")
plot_dat_grid <- res %>% plot_data(grid = TRUE)
```

To explore the behavior of the RBM parameters $\boldsymbol \theta$ as it related to model degeneracy and near degeneracy, we example various models of small size. For $H, V \in \{1, \dots, 4\}$, we sample uniformly `r dim(res$samp[[1]])[1]` values of $\boldsymbol \theta$ with magnitude, $||\boldsymbol \theta || = \sqrt{\sum_{i = 1, \dots, H + V + H*V} \theta_i^2}$, between `r min(plot_dat_grid$r1)` and `r max(plot_dat_grid$r1)`. For each set of parameters we then calculate the mean parameterization, as detailed in section \ref{degeneracy-in-random-graph-models} and can classify each model as near-degenerate or viable and look at the fraction of models that were near-degenerate for each combination of magnitude of $\boldsymbol \theta$ and model size. 

In this numerical experiment, we split $\boldsymbol \theta$ into $\boldsymbol \theta_{interaction}$ and $\boldsymbol \theta_{main}$, in reference to which sufficient statistics the parameters correspond to, and allow the two types of terms to have varying magnitudes. The results of this numerical study can be seen in figure \ref{fig:split_plots}. From this figure, it is evident that the fraction of near-degenerate models simulated at specified magnitudes of $\boldsymbol \theta_{main}$ and $\boldsymbol \theta_{interaction}$ is more closely related to the combined distance from the origin of these two vectors, than any individual group of parameter values.

```{r split_plots, fig.cap="\\label{fig:split_plots}A numerical experiment looking at the fraction of models that were near-degenerate for each combination of magnitude of $\\boldsymbol \\theta$ and model size. The thick black line shows the level where the fraction of -near degenerate models is .05."}
plot_dat_grid %>%
  group_by(r1, r2, H, V) %>%
  summarise(frac_degen = sum(near_hull)/n(), count = n()) %>% 
  ungroup() %>% 
  mutate(Hiddens = paste0("Hiddens: ", H), Visibles = paste0("Visibles: ", V)) %>%
  ggplot() +
  geom_tile(aes(x = r1, y = r2, fill = frac_degen)) +
  geom_contour(aes(x = r1, y = r2, z = frac_degen), colour = "black", bins = 8) +
  geom_contour(aes(x = r1, y = r2, z = frac_degen), colour = "black", breaks = .05, size = 1.5) +
  geom_abline(aes(intercept = 0, slope = 1), alpha = .5, lty = 2) +
  scale_fill_gradient("Fraction Near-degenerate", low = "yellow", high = "red") +
#   geom_point(aes(r1, r2), alpha = .1, 
#              data = plot_dat_split %>%
#                mutate(ss_interaction = sqrt(ss_interaction), ss_main = sqrt(ss_main)) %>%
#                select(-r1, -r2) %>%
#                rename(r1 = ss_main, r2 = ss_interaction) %>%
#                group_by(r1, r2, n_param) %>%
#                summarise(frac_degen = sum(near_hull)/n(), count = n()) %>%
#                filter(frac_degen > 0)) +
  facet_grid(Visibles~Hiddens) +
  xlab(expression(group("||", theta[main], "||"))) +
  ylab(expression(group("||", theta[interaction], "||"))) +
  theme(aspect.ratio = 1)
  #theme(legend.position = "bottom")
```

Figure \ref{fig:split_plots_dist} shows the fraction near-degenerate for each magnitude of $\boldsymbol \theta$ for each model. For each number of visibles in the model, the fraction near-degenerate becomes greater than zero at larger values of $||\boldsymbol \theta||$ as $H$ increases and the slope becomes steeper as $H$ increases also. This shows that as model size gets larger, the risk of degeneracy starts at a slightly larger magnitude of parameters, but very quickly increases until reaching close to 1. 

```{r split_plots2, warning=FALSE, fig.cap="\\label{fig:split_plots_dist}The fraction near-degenerate for each magnitude of $\\boldsymbol \\theta$ for each model. For each number of visibles in the model, the fraction near-degenerate becomes greater than zero at larger values of $||\\boldsymbol \\theta||$ as $H$ increases and the slope becomes steeper as $H$ increases also.", fig.height=2}
plot_dat_grid %>% 
  mutate(dist = sqrt(r1^2 + r2^2)) %>%
  group_by(dist, H, V) %>%
  summarise(frac_degen = sum(near_hull)/n()) %>%
  mutate(Hiddens = as.character(H), Visibles = paste0("Visibles: ", V)) %>%
  ggplot() +
  geom_point(aes(dist, frac_degen, colour = Hiddens)) +
  geom_smooth(aes(dist, frac_degen, colour = Hiddens, group = Hiddens)) +
  facet_grid(~Visibles) +
  ylim(c(0,1)) +
  xlab(expression(paste(group("||", theta, "||"), ", Distance from the origin"))) +
  ylab("Fraction Near-degenerate")
```

```{r, frac-degen, include=FALSE}
plot_dat_grid %>%
  mutate(dist = sqrt(r1^2 + r2^2)) %>%
  group_by(dist, H, V) %>%
  summarise(frac_degen = sum(near_hull)/n()) %>%
  ungroup() %>%
  group_by(H, V) -> tmp

tmp %>%
  filter(frac_degen > 0) %>%
  mutate(min_degen_pos = min(dist)) %>%
  select(H, V, min_degen_pos) %>% unique() %>%
left_join(tmp %>%
  filter(frac_degen > .6) %>%
  mutate(min_degen_50 = min(dist)) %>%
  select(H, V, min_degen_50) %>% unique()) %>%
  mutate(min_degen_pos = min_degen_pos/(H + V + H*V),
         min_degen_50 = min_degen_50/(H + V + H*V)) %>%
  mutate(degen_ratio = min_degen_50/min_degen_pos) %>%
  ggplot() +
  geom_point(aes(V, degen_ratio, colour = factor(H))) +
  geom_line(aes(V, degen_ratio, colour = factor(H), group = factor(H))) +
  ylab("Ratio of Min Distance above 0% near-degenerate to 50% near-degenerate") +
  xlab("Visibles") +
  scale_colour_discrete("Hiddens")

  
```


#Discussion

#References


